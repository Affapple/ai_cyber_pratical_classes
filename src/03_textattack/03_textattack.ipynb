{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e84g1YoseoE"
      },
      "source": [
        "# TextAttack End-to-End\n",
        "\n",
        "This tutorial provides a broad end-to-end overview of training, evaluating, and attacking a model using [TextAttack](https://textattack.readthedocs.io/en/master/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONayD5EJseoG"
      },
      "source": [
        "## Training\n",
        "Text attack comes with its own fine tuned models on several datasets. You can list them with the command below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thVtyRTkGeFu",
        "outputId": "6ce6d772-02db-4698-b1cf-dd3ac379e2ec"
      },
      "outputs": [],
      "source": [
        "! textattack list models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhNIugCuGegY"
      },
      "source": [
        "You can use these models as it is by referring to their name. However for the purpose of this practical we are going to train our model from scratch.\n",
        "\n",
        "TextAttack integrates directly with [transformers](https://github.com/huggingface/transformers/) and [datasets](https://github.com/huggingface/datasets) to train any of the `transformers` pre-trained models on datasets from `datasets`.\n",
        "\n",
        "Let's use the Rotten Tomatoes Movie Review dataset: it's relatively short, and showcases the key features of `textattack train`. Let's take a look at the dataset using `textattack peek-dataset`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spS2eW5WseoG",
        "outputId": "52c15f6c-f056-4ba2-bf39-864cdf575883"
      },
      "outputs": [],
      "source": [
        "!textattack peek-dataset --dataset-from-huggingface rotten_tomatoes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uguqpjnLseoI"
      },
      "source": [
        "The dataset looks good! It's lowercased already, so we'll make sure our model is uncased. The longest input is 51 words, so we can cap our maximum sequence length (`--model-max-length`) at 64.\n",
        "\n",
        "We'll train [`distilbert-base-uncased`](https://huggingface.co/transformers/model_doc/distilbert.html), since it's a relatively small model, and a good example of how we integrate with `transformers`.\n",
        "\n",
        "So we have our command:\n",
        "\n",
        "```bash\n",
        "textattack train                      \\ # Train a model with TextAttack\n",
        "    --model distilbert-base-uncased   \\ # Using distilbert, uncased version, from `transformers`\n",
        "    --dataset rotten_tomatoes         \\ # On the Rotten Tomatoes dataset\n",
        "    --model-num-labels 2              \\ # That has 2 labels\n",
        "    --model-max-length 64             \\ # With a maximum sequence length of 64\n",
        "    --per-device-train-batch-size 128 \\ # And batch size of 128\n",
        "    --num-epochs 3                    \\ # For 3 epochs\n",
        "```\n",
        "\n",
        "Now let's run it (please remember to use GPU if you have access):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY33W9aWseoI",
        "outputId": "17b3bc8f-2d60-446a-bb3c-37d73d8125f4"
      },
      "outputs": [],
      "source": [
        "!textattack train --model-name-or-path distilbert-base-uncased --dataset rotten_tomatoes --model-num-labels 2 --model-max-length 64 --per-device-train-batch-size 128 --num-epochs 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xzv3BGLseoI"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We successfully fine-tuned `distilbert-base-cased` for 3 epochs. Now let's evaluate it using `textattack eval`. This is as simple as providing the path to the pretrained model (that you just obtain from running the above command!) to `--model`, along with the number of evaluation samples. `textattack eval` will automatically load the evaluation data from training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGYR_W6DseoJ",
        "outputId": "b72af7cc-f415-4769-f007-3f2668acb181"
      },
      "outputs": [],
      "source": [
        "!textattack eval --num-examples 1000 --model ./outputs/2024-09-30-08-37-16-508338/best_model/ --dataset-from-huggingface rotten_tomatoes --dataset-split test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFPkCZShseoJ"
      },
      "source": [
        "Awesome -- we were able to train a model up to 84.4% accuracy on the test dataset â€“ with only a single command!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWglEuvUseoK"
      },
      "source": [
        "## Attack\n",
        "\n",
        "Finally, let's attack our pre-trained model. We can do this the same way as before (by providing the path to the pretrained model to `--model`). For our attack, let's use the \"TextFooler\" attack recipe, from the paper [\"Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment\" (Jin et al, 2019)](https://arxiv.org/abs/1907.11932). We can do this by passing `--recipe textfooler` to `textattack attack`.\n",
        "\n",
        "> *Warning*: We're printing out 100 examples and, if the attack succeeds, their perturbations. The output of this command is going to be quite long!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL-Bo1bgseoK",
        "outputId": "e6e175c9-d1ad-40a3-bf77-3facc3b84c74"
      },
      "outputs": [],
      "source": [
        "!textattack attack --recipe textfooler --num-examples 100 --model ./outputs/2024-09-30-08-37-16-508338/best_model/ --dataset-from-huggingface rotten_tomatoes --dataset-split test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyrJM3CaseoL"
      },
      "source": [
        "Looks like our model was 84% successful (makes sense - same evaluation set as `textattack eval`!), meaning that TextAttack attacked the model with 84 examples (since the attack won't run if an example is originally mispredicted). The attack success rate was 98.8%, meaning that TextFooler failed to find an adversarial example only 1.2% (1 out of 84) of the time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xwZqGWz8UVJ"
      },
      "source": [
        "#**TO DO: Robust models**\n",
        "\n",
        "Now that we have trained our model and saw that it was vulnerable to adversarial attacks, your next task is to improve its robustness. We can do so by training a model with adversarial data instead of the normal ones.\n",
        "\n",
        "1. To do so we need to update the training command from above to instruct textattack to use adversarial data generated with textfooler or any other attack during training.\n",
        "\n",
        "To complete the task you can take the help of the documentation of [TextAttack library ](https://textattack.readthedocs.io/en/latest/0_get_started/basic-Intro.html) and command line help option to adversarially train a model on the same dataset.\n",
        "\n",
        "***Hint***: Take a look at the [Trainer class in API](https://textattack.readthedocs.io/en/master/api/trainer.html) user guide and the [Making Vanilla Adversarial Training of NLP Models Feasible!](https://textattack.readthedocs.io/en/master/1start/A2TforVanillaAT.html)\n",
        "\n",
        "2. If the solution you found is expected to take more than 15 minutes to train, look again at the docummentation and adapt your parameters such that it will take between 5-10 minutes due to time restrictions for this class.\n",
        "\n",
        "3. Evaluate if the robustnes of the model has improved by attacking the newly trained model with TextFooler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1FpXmrp5V6g",
        "outputId": "741c115d-069e-4051-cd43-be695103c6d2"
      },
      "outputs": [],
      "source": [
        "!textattack train --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QQZ_cWlUMOz",
        "outputId": "1adc1816-83db-4dd2-cfed-8117ceb2d87a"
      },
      "outputs": [],
      "source": [
        "  ### Modify the  following command to perform adversarial training\n",
        "\n",
        "!textattack train --model-name-or-path distilbert-base-uncased --dataset rotten_tomatoes --model-num-labels 2 --model-max-length 64 --per-device-train-batch-size 128 --num-epochs 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk-hZgLVxJ00"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "03-textattack",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
