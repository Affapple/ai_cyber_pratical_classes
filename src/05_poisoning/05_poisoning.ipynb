{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvovv8GWM86j"
      },
      "source": [
        "# Clean-Label Feature Collision Attacks on a PyTorch Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-farw60M86l"
      },
      "source": [
        "In this notebook, we will learn how to use ART to run a clean-label feature collision poisoning attack on a neural network trained with PyTorch. We will be training our data on a subset of the CIFAR-10 dataset. The methods described are derived from [this paper](https://arxiv.org/abs/1804.00792) by Shafahi, Huang, et. al. 2018."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xpTX5iKMM86m"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "from os.path import abspath\n",
        "\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from art import config\n",
        "from art.utils import load_dataset, get_file\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "from art.attacks.poisoning import FeatureCollisionAttack\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(301)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nJilnjaVM86n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "shape of x_train (1000, 3, 32, 32)\n",
            "shape of y_train (1000, 10)\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test), min_, max_ = load_dataset('cifar10')\n",
        "print(x_train.shape)\n",
        "x_train = np.transpose(x_train, (0, 3, 1, 2)).astype(np.float32)\n",
        "x_test = np.transpose(x_test, (0, 3, 1, 2)).astype(np.float32)\n",
        "num_samples_train = 1000\n",
        "num_samples_test = 1000\n",
        "x_train = x_train[0:num_samples_train]\n",
        "y_train = y_train[0:num_samples_train]\n",
        "x_test = x_test[0:num_samples_test]\n",
        "y_test = y_test[0:num_samples_test]\n",
        "\n",
        "class_descr = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "print(\"shape of x_train\",x_train.shape)\n",
        "print(\"shape of y_train\",y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TABklks5M86o"
      },
      "source": [
        "## Load Model to be Attacked\n",
        "\n",
        "In this example, we using a RESNET18 model pretrained on the CIFAR dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy5kRuqqM86p"
      },
      "outputs": [],
      "source": [
        "# Model Definition and pretrained model pulled from:\n",
        "# https://github.com/huyvnphan/PyTorch_CIFAR10\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "__all__ = [\n",
        "    \"ResNet\",\n",
        "    \"resnet18\",\n",
        "    \"resnet34\",\n",
        "    \"resnet50\",\n",
        "]\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block,\n",
        "        layers,\n",
        "        num_classes=10,\n",
        "        zero_init_residual=False,\n",
        "        groups=1,\n",
        "        width_per_group=64,\n",
        "        replace_stride_with_dilation=None,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        # CIFAR10: kernel_size 7 -> 3, stride 2 -> 1, padding 3->1\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        # END\n",
        "\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes,\n",
        "                planes,\n",
        "                stride,\n",
        "                downsample,\n",
        "                self.groups,\n",
        "                self.base_width,\n",
        "                previous_dilation,\n",
        "                norm_layer,\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, device, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        # Download the model state_dict from the link: and run your code\n",
        "        state_dict = torch.load(\n",
        "            'resnet18.pt', map_location=device\n",
        "        )\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, progress=True, device=\"cpu\", **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet(\n",
        "        \"resnet18\", BasicBlock, [2, 2, 2, 2], pretrained, progress, device, **kwargs\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H-h-103NM86q"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'resnet18.pt?dl=0'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Pretrained model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m classifier_model = \u001b[43mresnet18\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m classifier_model.eval() \u001b[38;5;66;03m# for evaluation\u001b[39;00m\n\u001b[32m      5\u001b[39m criterion =  nn.CrossEntropyLoss()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 281\u001b[39m, in \u001b[36mresnet18\u001b[39m\u001b[34m(pretrained, progress, device, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresnet18\u001b[39m(pretrained=\u001b[38;5;28;01mFalse\u001b[39;00m, progress=\u001b[38;5;28;01mTrue\u001b[39;00m, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, **kwargs):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Constructs a ResNet-18 model.\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[33;03m        pretrained (bool): If True, returns a model pre-trained on ImageNet\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m        progress (bool): If True, displays a progress bar of the download to stderr\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresnet18\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBasicBlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 268\u001b[39m, in \u001b[36m_resnet\u001b[39m\u001b[34m(arch, block, layers, pretrained, progress, device, **kwargs)\u001b[39m\n\u001b[32m    265\u001b[39m model = ResNet(block, layers, **kwargs)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Download the model state_dict from the link: and run your code\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     state_dict = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresnet18.pt?dl=0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     model.load_state_dict(state_dict)\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo/course-ai-security/src/05_poisoning/.venv/lib/python3.11/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo/course-ai-security/src/05_poisoning/.venv/lib/python3.11/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/repo/course-ai-security/src/05_poisoning/.venv/lib/python3.11/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'resnet18.pt?dl=0'"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "# Pretrained model\n",
        "classifier_model = resnet18(pretrained=True)\n",
        "classifier_model.eval() # for evaluation\n",
        "criterion =  nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(classifier_model.parameters(), lr=0.0001)\n",
        "classifier = PyTorchClassifier(clip_values=(min_, max_), model=classifier_model,\n",
        "                             preprocessing=((0.4914, 0.4822, 0.4465),(0.2471, 0.2435, 0.2616)),nb_classes=10,input_shape=(3,32,32),loss=criterion,\n",
        "                             optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRlLka9hM86r"
      },
      "source": [
        "## Choose Target Image from Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eHHY4LpM86s"
      },
      "outputs": [],
      "source": [
        "target_class = \"bird\" # one of ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "target_label = np.zeros(len(class_descr))\n",
        "target_label[class_descr.index(target_class)] = 1\n",
        "target_instance = np.expand_dims(x_test[np.argmax(y_test, axis=1) == class_descr.index(target_class)][3], axis=0)\n",
        "img_plot = np.transpose(target_instance[0],(1,2,0))\n",
        "fig = plt.imshow(img_plot)\n",
        "print(\"shape of target_instance\",target_instance.shape)\n",
        "print('true_class: ' + target_class)\n",
        "print('predicted_class: ' + class_descr[np.argmax(classifier.predict(target_instance), axis=1)[0]])\n",
        "\n",
        "feature_layer = classifier.layer_names[-2]\n",
        "print(feature_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-L101jdM86t"
      },
      "source": [
        "## Poison Training Images to Misclassify Test\n",
        "\n",
        "The attacker wants to make it such that whenever a prediction is made on this particular bird the output will be a frog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwINviwVM86t"
      },
      "outputs": [],
      "source": [
        "base_class = \"frog\" # one of ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "base_idxs = np.argmax(y_test, axis=1) == class_descr.index(base_class)\n",
        "base_instances = np.copy(x_test[base_idxs][:10])\n",
        "base_labels = y_test[base_idxs][:10]\n",
        "x_test_pred = np.argmax(classifier.predict(base_instances), axis=1)\n",
        "nb_correct_pred = np.sum(x_test_pred == np.argmax(base_labels, axis=1))\n",
        "\n",
        "print(\"New test data to be poisoned (10 images):\")\n",
        "print(\"Correctly classified: {}\".format(nb_correct_pred))\n",
        "print(\"Incorrectly classified: {}\".format(10-nb_correct_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPo9rQ58M86u"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(0, 9):\n",
        "    pred_label, true_label = class_descr[x_test_pred[i]], class_descr[np.argmax(base_labels[i])]\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    fig=plt.imshow(np.transpose(base_instances[i],(1,2,0)))\n",
        "    fig.axes.get_xaxis().set_visible(False)\n",
        "    fig.axes.get_yaxis().set_visible(False)\n",
        "    fig.axes.text(0.5, -0.1, pred_label + \" (\" + true_label + \")\", fontsize=12, transform=fig.axes.transAxes,\n",
        "                  horizontalalignment='center')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9KgkpJKM86v"
      },
      "source": [
        "The captions on the images can be read: `predicted label (true label)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0Zp4HGxM86v"
      },
      "source": [
        "## Creating Poison Frogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96Hv0ILDM86v",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "attack = FeatureCollisionAttack(classifier,\n",
        "                                target_instance,\n",
        "                                feature_layer,\n",
        "                                max_iter=10,\n",
        "                                similarity_coeff=256,\n",
        "                                watermark=0.3,\n",
        "                                learning_rate=1)\n",
        "poison, poison_labels = attack.poison(base_instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iPFymklM86w"
      },
      "outputs": [],
      "source": [
        "poison_pred = np.argmax(classifier.predict(poison), axis=1)\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(0, 9):\n",
        "    pred_label, true_label = class_descr[poison_pred[i]], class_descr[np.argmax(poison_labels[i])]\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    fig=plt.imshow(np.transpose(poison[i],(1,2,0)))\n",
        "    fig.axes.get_xaxis().set_visible(False)\n",
        "    fig.axes.get_yaxis().set_visible(False)\n",
        "    fig.axes.text(0.5, -0.1, pred_label + \" (\" + true_label + \")\", fontsize=12, transform=fig.axes.transAxes,\n",
        "                  horizontalalignment='center')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_9uVxuKM86w"
      },
      "source": [
        "Notice how the network classifies most of theses poison examples as frogs, and it's not incorrect to do so. The examples look mostly froggy. A slight watermark of the target instance is also added to push the poisons closer to the target class in feature space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc_4NWj0M86x"
      },
      "source": [
        "## Training with Poison Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSs_tmiGM86y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "adv_train = np.vstack([x_train, poison])\n",
        "adv_labels = np.vstack([y_train, poison_labels])\n",
        "classifier_model.train()\n",
        "classifier.fit(adv_train, adv_labels, nb_epochs=20, batch_size=4, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx_fAhbRLZa5"
      },
      "outputs": [],
      "source": [
        "print(poison.shape)\n",
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f65OSBhtM86y"
      },
      "source": [
        "## Fooled Network Misclassifies Bird"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKUT5AEMM86y"
      },
      "outputs": [],
      "source": [
        "fig = plt.imshow(np.transpose(target_instance[0],(1,2,0)))\n",
        "\n",
        "print('true_class: ' + target_class)\n",
        "print('predicted_class: ' + class_descr[np.argmax(classifier.predict(target_instance), axis=1)[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opSGxStRM86y"
      },
      "source": [
        "These attacks allow adversaries who can poison your dataset the ability to mislabel any particular target instance of their choosing without manipulating labels."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "05-poisoning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
