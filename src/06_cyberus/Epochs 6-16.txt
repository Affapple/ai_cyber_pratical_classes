"----------\n",
"epoch 6/16\n",
"/tmp/ipython-input-578018973.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
"  with torch.cuda.amp.autocast():\n",
"1/388, train_loss: 0.6357, step time: 1.1638\n",
"2/388, train_loss: 0.8766, step time: 1.1239\n",
"3/388, train_loss: 0.6053, step time: 1.2139\n",
"4/388, train_loss: 0.6509, step time: 1.1749\n",
"5/388, train_loss: 0.7324, step time: 1.1551\n",
"6/388, train_loss: 0.9232, step time: 1.0417\n",
"7/388, train_loss: 0.6423, step time: 1.0426\n",
"8/388, train_loss: 0.7685, step time: 1.1614\n",
"9/388, train_loss: 0.7311, step time: 1.3477\n",
"10/388, train_loss: 0.5179, step time: 1.2223\n",
"11/388, train_loss: 0.6580, step time: 1.0893\n",
"12/388, train_loss: 0.7001, step time: 1.1535\n",
"13/388, train_loss: 0.8587, step time: 1.0941\n",
"14/388, train_loss: 0.6165, step time: 1.0530\n",
"15/388, train_loss: 0.7558, step time: 1.2157\n",
"16/388, train_loss: 0.6230, step time: 1.1334\n",
"17/388, train_loss: 0.6555, step time: 1.1602\n",
"18/388, train_loss: 0.7103, step time: 1.1717\n",
"19/388, train_loss: 0.8064, step time: 1.1076\n",
"20/388, train_loss: 0.8123, step time: 1.0514\n",
"21/388, train_loss: 0.8931, step time: 1.2248\n",
"22/388, train_loss: 0.6828, step time: 1.1321\n",
"23/388, train_loss: 0.8670, step time: 1.1182\n",
"24/388, train_loss: 0.6935, step time: 1.1427\n",
"25/388, train_loss: 0.6680, step time: 1.1174\n",
"26/388, train_loss: 0.7637, step time: 1.1326\n",
"27/388, train_loss: 0.7244, step time: 1.1392\n",
"28/388, train_loss: 0.6967, step time: 1.1534\n",
"29/388, train_loss: 0.7387, step time: 1.1212\n",
"30/388, train_loss: 0.5801, step time: 1.1312\n",
"31/388, train_loss: 0.6236, step time: 1.0672\n",
"32/388, train_loss: 0.9800, step time: 1.0900\n",
"33/388, train_loss: 0.8640, step time: 1.1133\n",
"34/388, train_loss: 0.9154, step time: 1.1220\n",
"35/388, train_loss: 0.9161, step time: 1.0937\n",
"36/388, train_loss: 0.7740, step time: 1.1097\n",
"37/388, train_loss: 0.5810, step time: 1.0759\n",
"38/388, train_loss: 0.7175, step time: 1.0550\n",
"39/388, train_loss: 0.7876, step time: 1.0553\n",
"40/388, train_loss: 0.7353, step time: 1.1155\n",
"41/388, train_loss: 0.5070, step time: 1.1771\n",
"42/388, train_loss: 0.6663, step time: 1.0713\n",
"43/388, train_loss: 0.8062, step time: 1.1018\n",
"44/388, train_loss: 0.8098, step time: 1.0505\n",
"45/388, train_loss: 0.7483, step time: 1.0450\n",
"46/388, train_loss: 0.9351, step time: 1.0575\n",
"47/388, train_loss: 0.6903, step time: 1.1231\n",
"48/388, train_loss: 0.8477, step time: 1.1159\n",
"49/388, train_loss: 0.8791, step time: 1.0441\n",
"50/388, train_loss: 0.8595, step time: 1.0640\n",
"51/388, train_loss: 0.6545, step time: 1.1829\n",
"52/388, train_loss: 0.9500, step time: 1.1210\n",
"53/388, train_loss: 0.8845, step time: 1.2089\n",
"54/388, train_loss: 0.6561, step time: 1.1546\n",
"55/388, train_loss: 0.5781, step time: 1.1519\n",
"56/388, train_loss: 0.6608, step time: 1.1300\n",
"57/388, train_loss: 0.7525, step time: 1.2268\n",
"58/388, train_loss: 0.7589, step time: 1.1105\n",
"59/388, train_loss: 0.7728, step time: 1.1073\n",
"60/388, train_loss: 0.6141, step time: 1.1357\n",
"61/388, train_loss: 0.7665, step time: 1.2037\n",
"62/388, train_loss: 0.6666, step time: 1.2036\n",
"63/388, train_loss: 0.7030, step time: 1.1690\n",
"64/388, train_loss: 0.7080, step time: 1.0610\n",
"65/388, train_loss: 0.5807, step time: 1.1109\n",
"66/388, train_loss: 0.4498, step time: 1.0702\n",
"67/388, train_loss: 0.7240, step time: 1.1620\n",
"68/388, train_loss: 0.7262, step time: 1.1848\n",
"69/388, train_loss: 0.8231, step time: 1.1467\n",
"70/388, train_loss: 0.7307, step time: 1.1414\n",
"71/388, train_loss: 0.7454, step time: 1.1475\n",
"72/388, train_loss: 0.6497, step time: 1.1013\n",
"73/388, train_loss: 0.5849, step time: 1.1664\n",
"74/388, train_loss: 0.5758, step time: 1.1676\n",
"75/388, train_loss: 0.6518, step time: 1.1874\n",
"76/388, train_loss: 0.8349, step time: 1.1090\n",
"77/388, train_loss: 0.9412, step time: 1.0967\n",
"78/388, train_loss: 0.5927, step time: 1.0552\n",
"79/388, train_loss: 0.9224, step time: 1.1935\n",
"80/388, train_loss: 0.7848, step time: 1.1294\n",
"81/388, train_loss: 0.8504, step time: 1.1552\n",
"82/388, train_loss: 0.6612, step time: 1.1207\n",
"83/388, train_loss: 0.6938, step time: 1.1335\n",
"84/388, train_loss: 0.9604, step time: 1.1164\n",
"85/388, train_loss: 0.6630, step time: 1.1863\n",
"86/388, train_loss: 0.6270, step time: 1.1378\n",
"87/388, train_loss: 0.6491, step time: 1.1452\n",
"88/388, train_loss: 0.8288, step time: 1.1124\n",
"89/388, train_loss: 0.8368, step time: 1.1496\n",
"90/388, train_loss: 0.7446, step time: 1.0776\n",
"91/388, train_loss: 0.5398, step time: 1.1586\n",
"92/388, train_loss: 0.7889, step time: 1.1987\n",
"93/388, train_loss: 0.9117, step time: 1.1902\n",
"94/388, train_loss: 0.8605, step time: 1.0759\n",
"95/388, train_loss: 0.5387, step time: 1.1179\n",
"96/388, train_loss: 0.8925, step time: 1.1660\n",
"97/388, train_loss: 0.8722, step time: 1.2244\n",
"98/388, train_loss: 0.6700, step time: 1.1201\n",
"99/388, train_loss: 0.8471, step time: 1.1680\n",
"100/388, train_loss: 0.6529, step time: 1.0842\n",
"101/388, train_loss: 0.6397, step time: 1.1909\n",
"102/388, train_loss: 0.8053, step time: 1.1142\n",
"103/388, train_loss: 0.9479, step time: 1.2266\n",
"104/388, train_loss: 0.6988, step time: 1.1624\n",
"105/388, train_loss: 0.8827, step time: 1.1005\n",
"106/388, train_loss: 0.8389, step time: 1.1116\n",
"107/388, train_loss: 0.8237, step time: 1.1061\n",
"108/388, train_loss: 0.9575, step time: 1.0487\n",
"109/388, train_loss: 0.5409, step time: 1.1664\n",
"110/388, train_loss: 0.5561, step time: 1.1538\n",
"111/388, train_loss: 0.9469, step time: 1.1660\n",
"112/388, train_loss: 0.7291, step time: 1.0845\n",
"113/388, train_loss: 0.9727, step time: 1.0620\n",
"114/388, train_loss: 0.5798, step time: 1.0752\n",
"115/388, train_loss: 0.5439, step time: 1.1790\n",
"116/388, train_loss: 0.8204, step time: 1.1296\n",
"117/388, train_loss: 0.9503, step time: 1.1715\n",
"118/388, train_loss: 0.6836, step time: 1.0364\n",
"119/388, train_loss: 0.6795, step time: 1.1601\n",
"120/388, train_loss: 0.8938, step time: 1.1913\n",
"121/388, train_loss: 0.8356, step time: 1.0320\n",
"122/388, train_loss: 0.7764, step time: 1.1574\n",
"123/388, train_loss: 0.7379, step time: 1.0746\n",
"124/388, train_loss: 0.6070, step time: 1.1112\n",
"125/388, train_loss: 0.6041, step time: 1.1031\n",
"126/388, train_loss: 0.7028, step time: 1.0991\n",
"127/388, train_loss: 0.6047, step time: 1.0935\n",
"128/388, train_loss: 0.8809, step time: 1.1778\n",
"129/388, train_loss: 0.9376, step time: 1.1545\n",
"130/388, train_loss: 0.5735, step time: 1.0463\n",
"131/388, train_loss: 0.8924, step time: 1.0246\n",
"132/388, train_loss: 0.8182, step time: 1.0520\n",
"133/388, train_loss: 0.6220, step time: 1.0164\n",
"134/388, train_loss: 0.8125, step time: 1.0528\n",
"135/388, train_loss: 0.8206, step time: 1.1531\n",
"136/388, train_loss: 0.7926, step time: 1.0879\n",
"137/388, train_loss: 0.9226, step time: 1.0240\n",
"138/388, train_loss: 0.6154, step time: 1.0691\n",
"139/388, train_loss: 0.5878, step time: 1.0342\n",
"140/388, train_loss: 0.9650, step time: 1.1102\n",
"141/388, train_loss: 0.6679, step time: 1.1296\n",
"142/388, train_loss: 0.8752, step time: 1.1052\n",
"143/388, train_loss: 0.8104, step time: 1.0257\n",
"144/388, train_loss: 0.9055, step time: 1.1140\n",
"145/388, train_loss: 0.5913, step time: 1.1606\n",
"146/388, train_loss: 0.6909, step time: 1.0488\n",
"147/388, train_loss: 0.7482, step time: 1.1902\n",
"148/388, train_loss: 0.8704, step time: 1.1247\n",
"149/388, train_loss: 0.5579, step time: 1.0366\n",
"150/388, train_loss: 0.7372, step time: 1.0751\n",
"151/388, train_loss: 0.9524, step time: 1.1503\n",
"152/388, train_loss: 0.7352, step time: 1.0824\n",
"153/388, train_loss: 0.8950, step time: 1.1173\n",
"154/388, train_loss: 0.5770, step time: 1.0722\n",
"155/388, train_loss: 0.6652, step time: 1.0921\n",
"156/388, train_loss: 0.7001, step time: 1.0604\n",
"157/388, train_loss: 0.8247, step time: 1.1117\n",
"158/388, train_loss: 0.7967, step time: 1.0628\n",
"159/388, train_loss: 0.6155, step time: 1.1738\n",
"160/388, train_loss: 0.6849, step time: 1.0971\n",
"161/388, train_loss: 0.9730, step time: 1.1610\n",
"162/388, train_loss: 0.4309, step time: 1.0572\n",
"163/388, train_loss: 0.5721, step time: 1.1217\n",
"164/388, train_loss: 0.7753, step time: 1.1380\n",
"165/388, train_loss: 0.9001, step time: 1.2625\n",
"166/388, train_loss: 0.6544, step time: 1.1443\n",
"167/388, train_loss: 0.4670, step time: 1.1005\n",
"168/388, train_loss: 0.6127, step time: 1.0606\n",
"169/388, train_loss: 0.5360, step time: 1.0505\n",
"170/388, train_loss: 0.9060, step time: 1.0636\n",
"171/388, train_loss: 0.6988, step time: 1.1270\n",
"172/388, train_loss: 0.8641, step time: 1.1782\n",
"173/388, train_loss: 0.6428, step time: 1.1675\n",
"174/388, train_loss: 0.6898, step time: 1.1249\n",
"175/388, train_loss: 0.4171, step time: 1.0373\n",
"176/388, train_loss: 0.8812, step time: 1.0514\n",
"177/388, train_loss: 0.7760, step time: 1.1992\n",
"178/388, train_loss: 0.7693, step time: 1.1703\n",
"179/388, train_loss: 0.8362, step time: 1.0975\n",
"180/388, train_loss: 0.7144, step time: 1.1413\n",
"181/388, train_loss: 0.8006, step time: 1.1655\n",
"182/388, train_loss: 0.7237, step time: 1.0949\n",
"183/388, train_loss: 0.7665, step time: 1.1318\n",
"184/388, train_loss: 0.4084, step time: 1.1718\n",
"185/388, train_loss: 0.8293, step time: 1.1660\n",
"186/388, train_loss: 0.4869, step time: 1.0448\n",
"187/388, train_loss: 0.6117, step time: 1.0350\n",
"188/388, train_loss: 0.5334, step time: 1.1641\n",
"189/388, train_loss: 0.4766, step time: 1.1926\n",
"190/388, train_loss: 0.5383, step time: 1.1782\n",
"191/388, train_loss: 0.8729, step time: 1.1559\n",
"192/388, train_loss: 0.5489, step time: 1.0615\n",
"193/388, train_loss: 0.6180, step time: 1.0970\n",
"194/388, train_loss: 0.8532, step time: 1.0679\n",
"195/388, train_loss: 0.6605, step time: 1.2055\n",
"196/388, train_loss: 0.6021, step time: 1.1913\n",
"197/388, train_loss: 0.6443, step time: 1.1114\n",
"198/388, train_loss: 0.8537, step time: 1.1379\n",
"199/388, train_loss: 0.7960, step time: 1.0474\n",
"200/388, train_loss: 0.9498, step time: 1.1296\n",
"201/388, train_loss: 0.8001, step time: 1.1057\n",
"202/388, train_loss: 0.8094, step time: 1.1709\n",
"203/388, train_loss: 0.5359, step time: 1.0921\n",
"204/388, train_loss: 0.7390, step time: 1.0542\n",
"205/388, train_loss: 0.7578, step time: 1.1006\n",
"206/388, train_loss: 0.9151, step time: 1.1043\n",
"207/388, train_loss: 0.5545, step time: 1.1611\n",
"208/388, train_loss: 0.6821, step time: 1.1683\n",
"209/388, train_loss: 0.8836, step time: 1.1877\n",
"210/388, train_loss: 0.8855, step time: 1.1259\n",
"211/388, train_loss: 0.9294, step time: 1.1152\n",
"212/388, train_loss: 0.6887, step time: 1.0511\n",
"213/388, train_loss: 0.6648, step time: 1.1751\n",
"214/388, train_loss: 0.6478, step time: 1.1582\n",
"215/388, train_loss: 0.7403, step time: 1.1885\n",
"216/388, train_loss: 0.6619, step time: 1.1218\n",
"217/388, train_loss: 0.7782, step time: 1.0436\n",
"218/388, train_loss: 0.8143, step time: 1.0870\n",
"219/388, train_loss: 0.6841, step time: 1.1017\n",
"220/388, train_loss: 0.8270, step time: 1.1026\n",
"221/388, train_loss: 0.7681, step time: 1.1290\n",
"222/388, train_loss: 0.6414, step time: 1.1199\n",
"223/388, train_loss: 0.4427, step time: 1.0987\n",
"224/388, train_loss: 0.5778, step time: 1.1419\n",
"225/388, train_loss: 0.9145, step time: 1.0988\n",
"226/388, train_loss: 0.8680, step time: 1.0560\n",
"227/388, train_loss: 0.7926, step time: 1.1148\n",
"228/388, train_loss: 0.6875, step time: 1.1160\n",
"229/388, train_loss: 0.4541, step time: 1.1027\n",
"230/388, train_loss: 0.8741, step time: 1.0756\n",
"231/388, train_loss: 0.7079, step time: 1.1721\n",
"232/388, train_loss: 0.7316, step time: 1.1230\n",
"233/388, train_loss: 0.5608, step time: 1.1775\n",
"234/388, train_loss: 0.5107, step time: 1.0630\n",
"235/388, train_loss: 0.7242, step time: 1.1816\n",
"236/388, train_loss: 0.7029, step time: 1.0965\n",
"237/388, train_loss: 0.6824, step time: 1.1946\n",
"238/388, train_loss: 0.7705, step time: 1.0579\n",
"239/388, train_loss: 0.5561, step time: 1.1039\n",
"240/388, train_loss: 0.4738, step time: 1.1319\n",
"241/388, train_loss: 0.5939, step time: 1.1046\n",
"242/388, train_loss: 0.7682, step time: 1.1257\n",
"243/388, train_loss: 0.7417, step time: 1.1757\n",
"244/388, train_loss: 0.8184, step time: 1.0730\n",
"245/388, train_loss: 0.6779, step time: 1.1568\n",
"246/388, train_loss: 0.7380, step time: 1.1202\n",
"247/388, train_loss: 0.5231, step time: 1.0729\n",
"248/388, train_loss: 0.7102, step time: 1.1171\n",
"249/388, train_loss: 0.7115, step time: 1.1251\n",
"250/388, train_loss: 0.9402, step time: 1.1354\n",
"251/388, train_loss: 0.6068, step time: 1.0974\n",
"252/388, train_loss: 0.8210, step time: 1.1034\n",
"253/388, train_loss: 0.5479, step time: 1.1841\n",
"254/388, train_loss: 0.6073, step time: 1.0856\n",
"255/388, train_loss: 0.7683, step time: 1.0548\n",
"256/388, train_loss: 0.6202, step time: 1.1665\n",
"257/388, train_loss: 0.6775, step time: 1.1703\n",
"258/388, train_loss: 0.7353, step time: 1.1434\n",
"259/388, train_loss: 0.6803, step time: 1.0424\n",
"260/388, train_loss: 0.8625, step time: 1.0990\n",
"261/388, train_loss: 0.8972, step time: 1.1026\n",
"262/388, train_loss: 0.9506, step time: 1.1686\n",
"263/388, train_loss: 0.7300, step time: 1.1160\n",
"264/388, train_loss: 0.7513, step time: 1.1068\n",
"265/388, train_loss: 0.6999, step time: 1.1049\n",
"266/388, train_loss: 0.8698, step time: 1.1404\n",
"267/388, train_loss: 0.7716, step time: 1.1229\n",
"268/388, train_loss: 0.8579, step time: 1.1419\n",
"269/388, train_loss: 0.8466, step time: 1.1949\n",
"270/388, train_loss: 0.5818, step time: 1.0510\n",
"271/388, train_loss: 0.5044, step time: 1.1808\n",
"272/388, train_loss: 0.6650, step time: 1.0623\n",
"273/388, train_loss: 0.9414, step time: 1.2113\n",
"274/388, train_loss: 0.6187, step time: 1.1501\n",
"275/388, train_loss: 0.6689, step time: 1.0994\n",
"276/388, train_loss: 0.6144, step time: 1.1303\n",
"277/388, train_loss: 0.5400, step time: 1.1550\n",
"278/388, train_loss: 0.8714, step time: 1.1434\n",
"279/388, train_loss: 0.6069, step time: 1.2228\n",
"280/388, train_loss: 0.9569, step time: 1.1397\n",
"281/388, train_loss: 0.8359, step time: 1.1255\n",
"282/388, train_loss: 0.8276, step time: 1.0908\n",
"283/388, train_loss: 0.7029, step time: 1.1955\n",
"284/388, train_loss: 0.7597, step time: 1.1152\n",
"285/388, train_loss: 0.7138, step time: 1.2062\n",
"286/388, train_loss: 0.9364, step time: 1.1331\n",
"287/388, train_loss: 0.7842, step time: 1.1881\n",
"288/388, train_loss: 0.5932, step time: 1.0535\n",
"289/388, train_loss: 0.6073, step time: 1.0490\n",
"290/388, train_loss: 0.6702, step time: 1.1192\n",
"291/388, train_loss: 0.7576, step time: 1.2276\n",
"292/388, train_loss: 0.7063, step time: 1.1652\n",
"293/388, train_loss: 0.8141, step time: 1.1774\n",
"294/388, train_loss: 0.9492, step time: 1.0678\n",
"295/388, train_loss: 0.6778, step time: 1.1183\n",
"296/388, train_loss: 0.7951, step time: 1.0626\n",
"297/388, train_loss: 0.7454, step time: 1.2215\n",
"298/388, train_loss: 0.5387, step time: 1.1352\n",
"299/388, train_loss: 0.6580, step time: 1.1783\n",
"300/388, train_loss: 0.7525, step time: 1.1335\n",
"301/388, train_loss: 0.8341, step time: 1.1056\n",
"302/388, train_loss: 0.8167, step time: 1.0644\n",
"303/388, train_loss: 0.9636, step time: 1.2457\n",
"304/388, train_loss: 0.5578, step time: 1.1623\n",
"305/388, train_loss: 0.8712, step time: 1.1915\n",
"306/388, train_loss: 0.9813, step time: 1.1196\n",
"307/388, train_loss: 0.4731, step time: 1.2077\n",
"308/388, train_loss: 0.7564, step time: 1.0540\n",
"309/388, train_loss: 0.6166, step time: 1.2046\n",
"310/388, train_loss: 0.7417, step time: 1.1432\n",
"311/388, train_loss: 0.7944, step time: 1.0639\n",
"312/388, train_loss: 0.7730, step time: 1.0926\n",
"313/388, train_loss: 0.6953, step time: 1.1607\n",
"314/388, train_loss: 0.8116, step time: 1.0547\n",
"315/388, train_loss: 0.8009, step time: 1.1997\n",
"316/388, train_loss: 0.8362, step time: 1.1546\n",
"317/388, train_loss: 0.8782, step time: 1.1688\n",
"318/388, train_loss: 0.7077, step time: 1.0520\n",
"319/388, train_loss: 0.5882, step time: 1.1869\n",
"320/388, train_loss: 0.7933, step time: 1.0741\n",
"321/388, train_loss: 0.6964, step time: 1.1807\n",
"322/388, train_loss: 0.7287, step time: 1.0497\n",
"323/388, train_loss: 0.8628, step time: 1.1943\n",
"324/388, train_loss: 0.8895, step time: 1.0734\n",
"325/388, train_loss: 0.6591, step time: 1.0817\n",
"326/388, train_loss: 0.6397, step time: 1.0613\n",
"327/388, train_loss: 0.6413, step time: 1.2017\n",
"328/388, train_loss: 0.9421, step time: 1.0875\n",
"329/388, train_loss: 0.5682, step time: 1.1042\n",
"330/388, train_loss: 0.6814, step time: 1.0499\n",
"331/388, train_loss: 0.8796, step time: 1.0716\n",
"332/388, train_loss: 0.8222, step time: 1.1437\n",
"333/388, train_loss: 0.6432, step time: 1.1616\n",
"334/388, train_loss: 0.5267, step time: 1.1351\n",
"335/388, train_loss: 0.9726, step time: 1.1684\n",
"336/388, train_loss: 0.7844, step time: 1.1402\n",
"337/388, train_loss: 0.8703, step time: 1.0770\n",
"338/388, train_loss: 0.5681, step time: 1.0608\n",
"339/388, train_loss: 0.4680, step time: 1.1934\n",
"340/388, train_loss: 0.6339, step time: 1.1296\n",
"341/388, train_loss: 0.7258, step time: 1.1111\n",
"342/388, train_loss: 0.6353, step time: 1.0476\n",
"343/388, train_loss: 0.8883, step time: 1.1641\n",
"344/388, train_loss: 0.6657, step time: 1.1396\n",
"345/388, train_loss: 0.7634, step time: 1.1847\n",
"346/388, train_loss: 0.7856, step time: 1.1418\n",
"347/388, train_loss: 0.4542, step time: 1.1686\n",
"348/388, train_loss: 0.4787, step time: 1.0616\n",
"349/388, train_loss: 0.7807, step time: 1.1006\n",
"350/388, train_loss: 0.6337, step time: 1.1400\n",
"351/388, train_loss: 0.7690, step time: 1.2164\n",
"352/388, train_loss: 0.5605, step time: 1.1120\n",
"353/388, train_loss: 0.9322, step time: 1.1212\n",
"354/388, train_loss: 0.7818, step time: 1.0521\n",
"355/388, train_loss: 0.6990, step time: 1.0796\n",
"356/388, train_loss: 0.8020, step time: 1.0541\n",
"357/388, train_loss: 0.7853, step time: 1.2179\n",
"358/388, train_loss: 0.7300, step time: 1.1338\n",
"359/388, train_loss: 0.6248, step time: 1.1694\n",
"360/388, train_loss: 0.7186, step time: 1.1180\n",
"361/388, train_loss: 0.9628, step time: 1.1431\n",
"362/388, train_loss: 0.8630, step time: 1.1667\n",
"363/388, train_loss: 0.7031, step time: 1.1461\n",
"364/388, train_loss: 0.8918, step time: 1.1365\n",
"365/388, train_loss: 0.5997, step time: 1.1018\n",
"366/388, train_loss: 0.8479, step time: 1.1367\n",
"367/388, train_loss: 0.8082, step time: 1.1816\n",
"368/388, train_loss: 0.8992, step time: 1.1253\n",
"369/388, train_loss: 0.7463, step time: 1.2020\n",
"370/388, train_loss: 0.8290, step time: 1.0492\n",
"371/388, train_loss: 0.7477, step time: 1.0879\n",
"372/388, train_loss: 0.8871, step time: 1.0548\n",
"373/388, train_loss: 0.6819, step time: 1.1277\n",
"374/388, train_loss: 0.6512, step time: 1.1650\n",
"375/388, train_loss: 0.6796, step time: 1.1323\n",
"376/388, train_loss: 0.6109, step time: 1.1223\n",
"377/388, train_loss: 0.7413, step time: 1.1057\n",
"378/388, train_loss: 0.7443, step time: 1.1454\n",
"379/388, train_loss: 0.4027, step time: 1.1128\n",
"380/388, train_loss: 0.6713, step time: 1.1686\n",
"381/388, train_loss: 0.8381, step time: 1.1648\n",
"382/388, train_loss: 0.8821, step time: 1.0578\n",
"383/388, train_loss: 0.8851, step time: 1.0537\n",
"384/388, train_loss: 0.7957, step time: 1.0512\n",
"385/388, train_loss: 0.9000, step time: 1.1658\n",
"386/388, train_loss: 0.5973, step time: 1.1599\n",
"387/388, train_loss: 0.8177, step time: 1.1099\n",
"388/388, train_loss: 0.6417, step time: 1.0339\n",
"epoch 6 average loss: 0.7336\n",
"/tmp/ipython-input-3132300594.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
"  with torch.cuda.amp.autocast():\n",
"/usr/local/lib/python3.12/dist-packages/monai/inferers/utils.py:231: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)\n",
"  win_data = inputs[unravel_slice[0]].to(sw_device)\n",
"/usr/local/lib/python3.12/dist-packages/monai/inferers/utils.py:370: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)\n",
"  out[idx_zm] += p\n",
"current epoch: 6 current mean dice: 0.5813 tc: 0.6211 wt: 0.7922 et: 0.3306\n",
"best mean dice: 0.5863 at epoch: 4\n",
"time consuming of epoch 6 is: 1027.6427\n",
"----------\n",
"epoch 7/16\n",
"1/388, train_loss: 0.7192, step time: 1.1930\n",
"2/388, train_loss: 0.8227, step time: 1.0735\n",
"3/388, train_loss: 0.9471, step time: 1.0931\n",
"4/388, train_loss: 0.6459, step time: 1.0851\n",
"5/388, train_loss: 0.5452, step time: 1.2294\n",
"6/388, train_loss: 0.8462, step time: 1.1485\n",
"7/388, train_loss: 0.7387, step time: 1.1558\n",
"8/388, train_loss: 0.8961, step time: 1.1021\n",
"9/388, train_loss: 0.5299, step time: 1.0807\n",
"10/388, train_loss: 0.8348, step time: 1.0729\n",
"11/388, train_loss: 0.8462, step time: 1.1508\n",
"12/388, train_loss: 0.6822, step time: 1.1534\n",
"13/388, train_loss: 0.6607, step time: 1.1345\n",
"14/388, train_loss: 0.9571, step time: 1.1139\n",
"15/388, train_loss: 0.9298, step time: 1.0381\n",
"16/388, train_loss: 0.7621, step time: 1.0527\n",
"17/388, train_loss: 0.8132, step time: 1.1334\n",
"18/388, train_loss: 0.8387, step time: 1.1339\n",
"19/388, train_loss: 0.8814, step time: 1.1481\n",
"20/388, train_loss: 0.6339, step time: 1.0753\n",
"21/388, train_loss: 0.5076, step time: 1.0565\n",
"22/388, train_loss: 0.4629, step time: 1.0767\n",
"23/388, train_loss: 0.6567, step time: 1.1756\n",
"24/388, train_loss: 0.8823, step time: 1.1658\n",
"25/388, train_loss: 0.6007, step time: 1.1396\n",
"26/388, train_loss: 0.4538, step time: 1.1145\n",
"27/388, train_loss: 0.7328, step time: 1.1884\n",
"28/388, train_loss: 0.7749, step time: 1.1136\n",
"29/388, train_loss: 0.9076, step time: 1.1743\n",
"30/388, train_loss: 0.5873, step time: 1.1223\n",
"31/388, train_loss: 0.8606, step time: 1.1718\n",
"32/388, train_loss: 0.9502, step time: 1.1397\n",
"33/388, train_loss: 0.7461, step time: 1.1862\n",
"34/388, train_loss: 0.8506, step time: 1.0466\n",
"35/388, train_loss: 0.6159, step time: 1.2523\n",
"36/388, train_loss: 0.9153, step time: 1.1118\n",
"37/388, train_loss: 0.7747, step time: 1.1497\n",
"38/388, train_loss: 0.9806, step time: 1.1111\n",
"39/388, train_loss: 0.6403, step time: 1.1062\n",
"40/388, train_loss: 0.5640, step time: 1.0419\n",
"41/388, train_loss: 0.8119, step time: 1.1735\n",
"42/388, train_loss: 0.6501, step time: 1.1876\n",
"43/388, train_loss: 0.6544, step time: 1.0859\n",
"44/388, train_loss: 0.7646, step time: 1.0715\n",
"45/388, train_loss: 0.4088, step time: 1.1089\n",
"46/388, train_loss: 0.9500, step time: 1.1135\n",
"47/388, train_loss: 0.5224, step time: 1.1129\n",
"48/388, train_loss: 0.7653, step time: 1.1947\n",
"49/388, train_loss: 0.8099, step time: 1.1174\n",
"50/388, train_loss: 0.7909, step time: 1.0632\n",
"51/388, train_loss: 0.4433, step time: 1.1841\n",
"52/388, train_loss: 0.9635, step time: 1.1394\n",
"53/388, train_loss: 0.8954, step time: 1.2536\n",
"54/388, train_loss: 0.9652, step time: 1.1542\n",
"55/388, train_loss: 0.8272, step time: 1.1390\n",
"56/388, train_loss: 0.8567, step time: 1.0543\n",
"57/388, train_loss: 0.8025, step time: 1.1602\n",
"58/388, train_loss: 0.8688, step time: 1.1363\n",
"59/388, train_loss: 0.8974, step time: 1.1956\n",
"60/388, train_loss: 0.9430, step time: 1.1321\n",
"61/388, train_loss: 0.6545, step time: 1.1107\n",
"62/388, train_loss: 0.6580, step time: 1.1395\n",
"63/388, train_loss: 0.5243, step time: 1.1085\n",
"64/388, train_loss: 0.7126, step time: 1.1202\n",
"65/388, train_loss: 0.5869, step time: 1.2542\n",
"66/388, train_loss: 0.7828, step time: 1.1723\n",
"67/388, train_loss: 0.6107, step time: 1.1096\n",
"68/388, train_loss: 0.8199, step time: 1.1218\n",
"69/388, train_loss: 0.7715, step time: 1.1893\n",
"70/388, train_loss: 0.5984, step time: 1.0553\n",
"71/388, train_loss: 0.5411, step time: 1.1687\n",
"72/388, train_loss: 0.6338, step time: 1.1706\n",
"73/388, train_loss: 0.7243, step time: 1.1085\n",
"74/388, train_loss: 0.7409, step time: 1.1118\n",
"75/388, train_loss: 0.7367, step time: 1.0546\n",
"76/388, train_loss: 0.7256, step time: 1.1157\n",
"77/388, train_loss: 0.5776, step time: 1.1107\n",
"78/388, train_loss: 0.5688, step time: 1.1768\n",
"79/388, train_loss: 0.6178, step time: 1.0954\n",
"80/388, train_loss: 0.6731, step time: 1.0620\n",
"81/388, train_loss: 0.8345, step time: 1.1432\n",
"82/388, train_loss: 0.8273, step time: 1.1434\n",
"83/388, train_loss: 0.4515, step time: 1.1050\n",
"84/388, train_loss: 0.6120, step time: 1.1804\n",
"85/388, train_loss: 0.5522, step time: 1.1151\n",
"86/388, train_loss: 0.8737, step time: 1.0524\n",
"87/388, train_loss: 0.6899, step time: 1.0748\n",
"88/388, train_loss: 0.7272, step time: 1.1277\n",
"89/388, train_loss: 0.8309, step time: 1.1099\n",
"90/388, train_loss: 0.5444, step time: 1.1878\n",
"91/388, train_loss: 0.8790, step time: 1.1677\n",
"92/388, train_loss: 0.7123, step time: 1.1294\n",
"93/388, train_loss: 0.6914, step time: 1.0465\n",
"94/388, train_loss: 0.7192, step time: 1.1400\n",
"95/388, train_loss: 0.6607, step time: 1.1378\n",
"96/388, train_loss: 0.9499, step time: 1.0885\n",
"97/388, train_loss: 0.6229, step time: 1.1006\n",
"98/388, train_loss: 0.9106, step time: 1.0517\n",
"99/388, train_loss: 0.9334, step time: 1.1024\n",
"100/388, train_loss: 0.6181, step time: 1.1336\n",
"101/388, train_loss: 0.9208, step time: 1.1335\n",
"102/388, train_loss: 0.6194, step time: 1.1105\n",
"103/388, train_loss: 0.5208, step time: 1.1399\n",
"104/388, train_loss: 0.6065, step time: 1.0820\n",
"105/388, train_loss: 0.5919, step time: 1.1019\n",
"106/388, train_loss: 0.6779, step time: 1.1009\n",
"107/388, train_loss: 0.7828, step time: 1.1111\n",
"108/388, train_loss: 0.6450, step time: 1.1517\n",
"109/388, train_loss: 0.7708, step time: 1.1727\n",
"110/388, train_loss: 0.9726, step time: 1.0933\n",
"111/388, train_loss: 0.7209, step time: 1.0489\n",
"112/388, train_loss: 0.5401, step time: 1.1027\n",
"113/388, train_loss: 0.7702, step time: 1.1871\n",
"114/388, train_loss: 0.5662, step time: 1.1475\n",
"115/388, train_loss: 0.5412, step time: 1.1186\n",
"116/388, train_loss: 0.7518, step time: 1.0862\n",
"117/388, train_loss: 0.4625, step time: 1.1121\n",
"118/388, train_loss: 0.6760, step time: 1.0761\n",
"119/388, train_loss: 0.9523, step time: 1.1258\n",
"120/388, train_loss: 0.4033, step time: 1.1317\n",
"121/388, train_loss: 0.8022, step time: 1.1778\n",
"122/388, train_loss: 0.5557, step time: 1.1437\n",
"123/388, train_loss: 0.7645, step time: 1.1066\n",
"124/388, train_loss: 0.6915, step time: 1.0546\n",
"125/388, train_loss: 0.5813, step time: 1.1044\n",
"126/388, train_loss: 0.7019, step time: 1.1358\n",
"127/388, train_loss: 0.8998, step time: 1.0963\n",
"128/388, train_loss: 0.7350, step time: 1.1323\n",
"129/388, train_loss: 0.7464, step time: 1.0558\n",
"130/388, train_loss: 0.6886, step time: 1.0585\n",
"131/388, train_loss: 0.8804, step time: 1.1063\n",
"132/388, train_loss: 0.6573, step time: 1.0855\n",
"133/388, train_loss: 0.5615, step time: 1.1762\n",
"134/388, train_loss: 0.6876, step time: 1.1399\n",
"135/388, train_loss: 0.8211, step time: 1.0396\n",
"136/388, train_loss: 0.7506, step time: 1.1226\n",
"137/388, train_loss: 0.6630, step time: 1.1032\n",
"138/388, train_loss: 0.6119, step time: 1.0783\n",
"139/388, train_loss: 0.7960, step time: 1.1756\n",
"140/388, train_loss: 0.8851, step time: 1.0907\n",
"141/388, train_loss: 0.9425, step time: 1.1314\n",
"142/388, train_loss: 0.6768, step time: 1.1402\n",
"143/388, train_loss: 0.5300, step time: 1.0647\n",
"144/388, train_loss: 0.6235, step time: 1.1078\n",
"145/388, train_loss: 0.8213, step time: 1.1843\n",
"146/388, train_loss: 0.7212, step time: 1.0773\n",
"147/388, train_loss: 0.6531, step time: 1.1125\n",
"148/388, train_loss: 0.8081, step time: 1.1269\n",
"149/388, train_loss: 0.8350, step time: 1.0449\n",
"150/388, train_loss: 0.6354, step time: 1.1368\n",
"151/388, train_loss: 0.6587, step time: 1.1782\n",
"152/388, train_loss: 0.5539, step time: 1.0544\n",
"153/388, train_loss: 0.8273, step time: 1.1637\n",
"154/388, train_loss: 0.6914, step time: 1.0536\n",
"155/388, train_loss: 0.7577, step time: 1.0956\n",
"156/388, train_loss: 0.6697, step time: 1.0736\n",
"157/388, train_loss: 0.6186, step time: 1.2681\n",
"158/388, train_loss: 0.6244, step time: 1.0750\n",
"159/388, train_loss: 0.8639, step time: 1.1584\n",
"160/388, train_loss: 0.4689, step time: 1.0604\n",
"161/388, train_loss: 0.7500, step time: 1.0538\n",
"162/388, train_loss: 0.6969, step time: 1.1072\n",
"163/388, train_loss: 0.6728, step time: 1.2085\n",
"164/388, train_loss: 0.5939, step time: 1.1364\n",
"165/388, train_loss: 0.7480, step time: 1.1569\n",
"166/388, train_loss: 0.6642, step time: 1.1298\n",
"167/388, train_loss: 0.5739, step time: 1.0628\n",
"168/388, train_loss: 0.5992, step time: 1.0942\n",
"169/388, train_loss: 0.6322, step time: 1.1742\n",
"170/388, train_loss: 0.4001, step time: 1.0822\n",
"171/388, train_loss: 0.8330, step time: 1.0605\n",
"172/388, train_loss: 0.5460, step time: 1.1266\n",
"173/388, train_loss: 0.8361, step time: 1.1297\n",
"174/388, train_loss: 0.6078, step time: 1.1012\n",
"175/388, train_loss: 0.6702, step time: 1.1893\n",
"176/388, train_loss: 0.9306, step time: 1.0814\n",
"177/388, train_loss: 0.9203, step time: 1.1109\n",
"178/388, train_loss: 0.6377, step time: 1.1080\n",
"179/388, train_loss: 0.7802, step time: 1.1744\n",
"180/388, train_loss: 0.8010, step time: 1.1210\n",
"181/388, train_loss: 0.6842, step time: 1.2860\n",
"182/388, train_loss: 0.8288, step time: 1.0493\n",
"183/388, train_loss: 0.6563, step time: 1.1038\n",
"184/388, train_loss: 0.7054, step time: 1.0582\n",
"185/388, train_loss: 0.7481, step time: 1.0996\n",
"186/388, train_loss: 0.5539, step time: 1.0611\n",
"187/388, train_loss: 0.6061, step time: 1.2074\n",
"188/388, train_loss: 0.6993, step time: 1.0541\n",
"189/388, train_loss: 0.6647, step time: 1.0421\n",
"190/388, train_loss: 0.5552, step time: 1.1350\n",
"191/388, train_loss: 0.4870, step time: 1.1970\n",
"192/388, train_loss: 0.7352, step time: 1.1178\n",
"193/388, train_loss: 0.8886, step time: 1.1893\n",
"194/388, train_loss: 0.8719, step time: 1.0659\n",
"195/388, train_loss: 0.7372, step time: 1.1463\n",
"196/388, train_loss: 0.7865, step time: 1.1397\n",
"197/388, train_loss: 0.8836, step time: 1.1411\n",
"198/388, train_loss: 0.7290, step time: 1.0535\n",
"199/388, train_loss: 0.6069, step time: 1.1900\n",
"200/388, train_loss: 0.8203, step time: 1.1379\n",
"201/388, train_loss: 0.6449, step time: 1.0899\n",
"202/388, train_loss: 0.6843, step time: 1.0676\n",
"203/388, train_loss: 0.7311, step time: 1.1043\n",
"204/388, train_loss: 0.6962, step time: 1.0542\n",
"205/388, train_loss: 0.7965, step time: 1.2031\n",
"206/388, train_loss: 0.6704, step time: 1.1253\n",
"207/388, train_loss: 0.7342, step time: 1.1954\n",
"208/388, train_loss: 0.7761, step time: 1.0850\n",
"209/388, train_loss: 0.8516, step time: 1.0800\n",
"210/388, train_loss: 0.5403, step time: 1.0916\n",
"211/388, train_loss: 0.8827, step time: 1.1961\n",
"212/388, train_loss: 0.5815, step time: 1.0412\n",
"213/388, train_loss: 0.5919, step time: 1.0476\n",
"214/388, train_loss: 0.5203, step time: 1.0578\n",
"215/388, train_loss: 0.6180, step time: 1.1878\n",
"216/388, train_loss: 0.8918, step time: 1.1439\n",
"217/388, train_loss: 0.6711, step time: 1.1264\n",
"218/388, train_loss: 0.5377, step time: 1.0669\n",
"219/388, train_loss: 0.6572, step time: 1.1152\n",
"220/388, train_loss: 0.8143, step time: 1.0605\n",
"221/388, train_loss: 0.8178, step time: 1.0373\n",
"222/388, train_loss: 0.6838, step time: 1.1234\n",
"223/388, train_loss: 0.4696, step time: 1.1717\n",
"224/388, train_loss: 0.8175, step time: 1.1283\n",
"225/388, train_loss: 0.7857, step time: 1.1645\n",
"226/388, train_loss: 0.9592, step time: 1.0756\n",
"227/388, train_loss: 0.8136, step time: 1.0742\n",
"228/388, train_loss: 0.9513, step time: 1.0606\n",
"229/388, train_loss: 0.6050, step time: 1.1993\n",
"230/388, train_loss: 0.7468, step time: 1.1189\n",
"231/388, train_loss: 0.6941, step time: 1.1183\n",
"232/388, train_loss: 0.5342, step time: 1.0755\n",
"233/388, train_loss: 0.5362, step time: 1.1765\n",
"234/388, train_loss: 0.6526, step time: 1.0637\n",
"235/388, train_loss: 0.9363, step time: 1.1339\n",
"236/388, train_loss: 0.7578, step time: 1.1283\n",
"237/388, train_loss: 0.8183, step time: 1.1611\n",
"238/388, train_loss: 0.7073, step time: 1.1341\n",
"239/388, train_loss: 0.7793, step time: 1.1666\n",
"240/388, train_loss: 0.6931, step time: 1.0733\n",
"241/388, train_loss: 0.9014, step time: 1.1793\n",
"242/388, train_loss: 0.8464, step time: 1.1399\n",
"243/388, train_loss: 0.6750, step time: 1.1188\n",
"244/388, train_loss: 0.6953, step time: 1.0895\n",
"245/388, train_loss: 0.8578, step time: 1.1827\n",
"246/388, train_loss: 0.8533, step time: 1.1207\n",
"247/388, train_loss: 0.8118, step time: 1.1884\n",
"248/388, train_loss: 0.6107, step time: 1.1290\n",
"249/388, train_loss: 0.7455, step time: 1.1755\n",
"250/388, train_loss: 0.7296, step time: 1.1332\n",
"251/388, train_loss: 0.8460, step time: 1.1332\n",
"252/388, train_loss: 0.6488, step time: 1.1350\n",
"253/388, train_loss: 0.7311, step time: 1.1287\n",
"254/388, train_loss: 0.8141, step time: 1.0588\n",
"255/388, train_loss: 0.7617, step time: 1.0405\n",
"256/388, train_loss: 0.6165, step time: 1.0560\n",
"257/388, train_loss: 0.4466, step time: 1.0672\n",
"258/388, train_loss: 0.7425, step time: 1.0926\n",
"259/388, train_loss: 0.9465, step time: 1.2229\n",
"260/388, train_loss: 0.6052, step time: 1.1346\n",
"261/388, train_loss: 0.7279, step time: 1.1502\n",
"262/388, train_loss: 0.7719, step time: 1.1264\n",
"263/388, train_loss: 0.6013, step time: 1.0671\n",
"264/388, train_loss: 0.6226, step time: 1.1218\n",
"265/388, train_loss: 0.6693, step time: 1.2469\n",
"266/388, train_loss: 0.7883, step time: 1.0917\n",
"267/388, train_loss: 0.8705, step time: 1.1888\n",
"268/388, train_loss: 0.8303, step time: 1.1065\n",
"269/388, train_loss: 0.5448, step time: 1.1034\n",
"270/388, train_loss: 0.4265, step time: 1.0545\n",
"271/388, train_loss: 0.6528, step time: 1.2138\n",
"272/388, train_loss: 0.6012, step time: 1.1012\n",
"273/388, train_loss: 0.4887, step time: 1.0490\n",
"274/388, train_loss: 0.7209, step time: 1.0457\n",
"275/388, train_loss: 0.9415, step time: 1.0687\n",
"276/388, train_loss: 0.8914, step time: 1.0705\n",
"277/388, train_loss: 0.8675, step time: 1.2653\n",
"278/388, train_loss: 0.8339, step time: 1.1286\n",
"279/388, train_loss: 0.8666, step time: 1.0448\n",
"280/388, train_loss: 0.7625, step time: 1.1282\n",
"281/388, train_loss: 0.7176, step time: 1.1165\n",
"282/388, train_loss: 0.7781, step time: 1.0966\n",
"283/388, train_loss: 0.7873, step time: 1.2446\n",
"284/388, train_loss: 0.7581, step time: 1.1509\n",
"285/388, train_loss: 0.9022, step time: 1.1870\n",
"286/388, train_loss: 0.6667, step time: 1.0745\n",
"287/388, train_loss: 0.8159, step time: 1.1894\n",
"288/388, train_loss: 0.9111, step time: 1.1335\n",
"289/388, train_loss: 0.4748, step time: 1.1831\n",
"290/388, train_loss: 0.8240, step time: 1.1285\n",
"291/388, train_loss: 0.6557, step time: 1.1801\n",
"292/388, train_loss: 0.7232, step time: 1.1324\n",
"293/388, train_loss: 0.8127, step time: 1.0437\n",
"294/388, train_loss: 0.5940, step time: 1.1349\n",
"295/388, train_loss: 0.8586, step time: 1.2290\n",
"296/388, train_loss: 0.7234, step time: 1.1301\n",
"297/388, train_loss: 0.7644, step time: 1.0936\n",
"298/388, train_loss: 0.5908, step time: 1.0782\n",
"299/388, train_loss: 0.7206, step time: 1.1011\n",
"300/388, train_loss: 0.9711, step time: 1.1037\n",
"301/388, train_loss: 0.7340, step time: 1.2421\n",
"302/388, train_loss: 0.6828, step time: 1.1215\n",
"303/388, train_loss: 0.5986, step time: 1.2086\n",
"304/388, train_loss: 0.8548, step time: 1.1213\n",
"305/388, train_loss: 0.7004, step time: 1.1601\n",
"306/388, train_loss: 0.7124, step time: 1.0667\n",
"307/388, train_loss: 0.8832, step time: 1.2289\n",
"308/388, train_loss: 0.8877, step time: 1.1674\n",
"309/388, train_loss: 0.7264, step time: 1.1770\n",
"310/388, train_loss: 0.6912, step time: 1.1325\n",
"311/388, train_loss: 0.8506, step time: 1.0714\n",
"312/388, train_loss: 0.6800, step time: 1.1234\n",
"313/388, train_loss: 0.6560, step time: 1.2098\n",
"314/388, train_loss: 0.5928, step time: 1.1414\n",
"315/388, train_loss: 0.7217, step time: 1.1736\n",
"316/388, train_loss: 0.7675, step time: 1.0583\n",
"317/388, train_loss: 0.9556, step time: 1.1837\n",
"318/388, train_loss: 0.6102, step time: 1.0620\n",
"319/388, train_loss: 0.9712, step time: 1.2134\n",
"320/388, train_loss: 0.7989, step time: 1.1574\n",
"321/388, train_loss: 0.8567, step time: 1.0405\n",
"322/388, train_loss: 0.8664, step time: 1.1275\n",
"323/388, train_loss: 0.5790, step time: 1.1567\n",
"324/388, train_loss: 0.8725, step time: 1.1070\n",
"325/388, train_loss: 0.6759, step time: 1.2280\n",
"326/388, train_loss: 0.9204, step time: 1.1368\n",
"327/388, train_loss: 0.6262, step time: 1.1056\n",
"328/388, train_loss: 0.5774, step time: 1.0508\n",
"329/388, train_loss: 0.8539, step time: 1.1575\n",
"330/388, train_loss: 0.6496, step time: 1.1142\n",
"331/388, train_loss: 0.9377, step time: 1.2151\n",
"332/388, train_loss: 0.5422, step time: 1.0497\n",
"333/388, train_loss: 0.9170, step time: 1.1296\n",
"334/388, train_loss: 0.5669, step time: 1.0646\n",
"335/388, train_loss: 0.8862, step time: 1.0427\n",
"336/388, train_loss: 0.7352, step time: 1.1191\n",
"337/388, train_loss: 0.8879, step time: 1.2518\n",
"338/388, train_loss: 0.7191, step time: 1.1540\n",
"339/388, train_loss: 0.6324, step time: 1.1082\n",
"340/388, train_loss: 0.8497, step time: 1.0888\n",
"341/388, train_loss: 0.9337, step time: 1.1160\n",
"342/388, train_loss: 0.8031, step time: 1.0898\n",
"343/388, train_loss: 0.7947, step time: 1.1926\n",
"344/388, train_loss: 0.6964, step time: 1.1410\n",
"345/388, train_loss: 0.8575, step time: 1.0987\n",
"346/388, train_loss: 0.6719, step time: 1.0804\n",
"347/388, train_loss: 0.6540, step time: 1.1527\n",
"348/388, train_loss: 0.7644, step time: 1.0540\n",
"349/388, train_loss: 0.8541, step time: 1.2355\n",
"350/388, train_loss: 0.6370, step time: 1.1356\n",
"351/388, train_loss: 0.6794, step time: 1.1741\n",
"352/388, train_loss: 0.8765, step time: 1.1020\n",
"353/388, train_loss: 0.7094, step time: 1.0427\n",
"354/388, train_loss: 0.6106, step time: 1.1019\n",
"355/388, train_loss: 0.9401, step time: 1.2057\n",
"356/388, train_loss: 0.8579, step time: 1.1111\n",
"357/388, train_loss: 0.8761, step time: 1.1672\n",
"358/388, train_loss: 0.8820, step time: 1.0477\n",
"359/388, train_loss: 0.6991, step time: 1.1660\n",
"360/388, train_loss: 0.9368, step time: 1.1108\n",
"361/388, train_loss: 0.7978, step time: 1.2099\n",
"362/388, train_loss: 0.7876, step time: 1.0526\n",
"363/388, train_loss: 0.4634, step time: 1.1884\n",
"364/388, train_loss: 0.6290, step time: 1.0680\n",
"365/388, train_loss: 0.5678, step time: 1.1033\n",
"366/388, train_loss: 0.6414, step time: 1.0967\n",
"367/388, train_loss: 0.7270, step time: 1.1859\n",
"368/388, train_loss: 0.8710, step time: 1.1087\n",
"369/388, train_loss: 0.8688, step time: 1.1782\n",
"370/388, train_loss: 0.6989, step time: 1.1143\n",
"371/388, train_loss: 0.9794, step time: 1.0816\n",
"372/388, train_loss: 0.5749, step time: 1.0601\n",
"373/388, train_loss: 0.6469, step time: 1.1709\n",
"374/388, train_loss: 0.7722, step time: 1.0763\n",
"375/388, train_loss: 0.7915, step time: 1.1757\n",
"376/388, train_loss: 0.5565, step time: 1.0663\n",
"377/388, train_loss: 0.7302, step time: 1.0443\n",
"378/388, train_loss: 0.7707, step time: 1.1016\n",
"379/388, train_loss: 0.6707, step time: 1.2453\n",
"380/388, train_loss: 0.6881, step time: 1.0556\n",
"381/388, train_loss: 0.7460, step time: 1.1683\n",
"382/388, train_loss: 0.6774, step time: 1.1041\n",
"383/388, train_loss: 0.6315, step time: 1.1634\n",
"384/388, train_loss: 0.7408, step time: 1.1482\n",
"385/388, train_loss: 0.7537, step time: 1.1583\n",
"386/388, train_loss: 0.8008, step time: 1.0399\n",
"387/388, train_loss: 0.7791, step time: 1.0114\n",
"388/388, train_loss: 0.7747, step time: 1.0147\n",
"epoch 7 average loss: 0.7287\n",
"saved new best metric model\n",
"current epoch: 7 current mean dice: 0.6254 tc: 0.6914 wt: 0.8099 et: 0.3751\n",
"best mean dice: 0.6254 at epoch: 7\n",
"time consuming of epoch 7 is: 1013.1899\n",
"----------\n",
"epoch 8/16\n",
"1/388, train_loss: 0.5521, step time: 1.1602\n",
"2/388, train_loss: 0.6643, step time: 1.1104\n",
"3/388, train_loss: 0.4631, step time: 1.1039\n",
"4/388, train_loss: 0.6414, step time: 1.1450\n",
"5/388, train_loss: 0.4397, step time: 1.0537\n",
"6/388, train_loss: 0.5993, step time: 1.0491\n",
"7/388, train_loss: 0.7280, step time: 1.1048\n",
"8/388, train_loss: 0.6444, step time: 1.1419\n",
"9/388, train_loss: 0.8720, step time: 1.1334\n",
"10/388, train_loss: 0.6294, step time: 1.0659\n",
"11/388, train_loss: 0.7151, step time: 1.0873\n",
"12/388, train_loss: 0.6118, step time: 1.0747\n",
"13/388, train_loss: 0.6984, step time: 1.0706\n",
"14/388, train_loss: 0.5313, step time: 1.1035\n",
"15/388, train_loss: 0.8658, step time: 1.2134\n",
"16/388, train_loss: 0.8000, step time: 1.0582\n",
"17/388, train_loss: 0.8772, step time: 1.1056\n",
"18/388, train_loss: 0.5538, step time: 1.1216\n",
"19/388, train_loss: 0.5692, step time: 1.1107\n",
"20/388, train_loss: 0.8452, step time: 1.1309\n",
"21/388, train_loss: 0.9037, step time: 1.2217\n",
"22/388, train_loss: 0.7339, step time: 1.1252\n",
"23/388, train_loss: 0.7380, step time: 1.0510\n",
"24/388, train_loss: 0.7305, step time: 1.0796\n",
"25/388, train_loss: 0.9008, step time: 1.1036\n",
"26/388, train_loss: 0.6514, step time: 1.0599\n",
"27/388, train_loss: 0.8537, step time: 1.1342\n",
"28/388, train_loss: 0.7549, step time: 1.1632\n",
"29/388, train_loss: 0.6761, step time: 1.1519\n",
"30/388, train_loss: 0.7324, step time: 1.1180\n",
"31/388, train_loss: 0.7108, step time: 1.0558\n",
"32/388, train_loss: 0.8415, step time: 1.1179\n",
"33/388, train_loss: 0.7343, step time: 1.2073\n",
"34/388, train_loss: 0.9163, step time: 1.1640\n",
"35/388, train_loss: 0.7311, step time: 1.1612\n",
"36/388, train_loss: 0.9083, step time: 1.1317\n",
"37/388, train_loss: 0.5592, step time: 1.0700\n",
"38/388, train_loss: 0.6214, step time: 1.1227\n",
"39/388, train_loss: 0.7874, step time: 1.0448\n",
"40/388, train_loss: 0.7705, step time: 1.1552\n",
"41/388, train_loss: 0.7232, step time: 1.1669\n",
"42/388, train_loss: 0.8522, step time: 1.1095\n",
"43/388, train_loss: 0.6484, step time: 1.0960\n",
"44/388, train_loss: 0.6939, step time: 1.0510\n",
"45/388, train_loss: 0.5909, step time: 1.1097\n",
"46/388, train_loss: 0.7175, step time: 1.1352\n",
"47/388, train_loss: 0.7150, step time: 1.1348\n",
"48/388, train_loss: 0.9612, step time: 1.1283\n",
"49/388, train_loss: 0.6634, step time: 1.0368\n",
"50/388, train_loss: 0.8778, step time: 1.1287\n",
"51/388, train_loss: 0.6692, step time: 1.1461\n",
"52/388, train_loss: 0.8011, step time: 1.0795\n",
"53/388, train_loss: 0.7397, step time: 1.1691\n",
"54/388, train_loss: 0.8029, step time: 1.0815\n",
"55/388, train_loss: 0.5257, step time: 1.0816\n",
"56/388, train_loss: 0.6660, step time: 1.1253\n",
"57/388, train_loss: 0.8021, step time: 1.1490\n",
"58/388, train_loss: 0.5937, step time: 1.0502\n",
"59/388, train_loss: 0.7283, step time: 1.2123\n",
"60/388, train_loss: 0.8799, step time: 1.1292\n",
"61/388, train_loss: 0.6407, step time: 1.0691\n",
"62/388, train_loss: 0.7875, step time: 1.0776\n",
"63/388, train_loss: 0.6343, step time: 1.1593\n",
"64/388, train_loss: 0.5974, step time: 1.0583\n",
"65/388, train_loss: 0.8051, step time: 1.2339\n",
"66/388, train_loss: 0.6573, step time: 1.1338\n",
"67/388, train_loss: 0.8468, step time: 1.0725\n",
"68/388, train_loss: 0.6292, step time: 1.1235\n",
"69/388, train_loss: 0.7552, step time: 1.1526\n",
"70/388, train_loss: 0.9372, step time: 1.1345\n",
"71/388, train_loss: 0.3803, step time: 1.1174\n",
"72/388, train_loss: 0.8793, step time: 1.1218\n",
"73/388, train_loss: 0.5667, step time: 1.1101\n",
"74/388, train_loss: 0.5706, step time: 1.1398\n",
"75/388, train_loss: 0.6304, step time: 1.1079\n",
"76/388, train_loss: 0.9805, step time: 1.0668\n",
"77/388, train_loss: 0.7716, step time: 1.1862\n",
"78/388, train_loss: 0.5522, step time: 1.1708\n",
"79/388, train_loss: 0.5691, step time: 1.0999\n",
"80/388, train_loss: 0.6197, step time: 1.0450\n",
"81/388, train_loss: 0.7572, step time: 1.2171\n",
"82/388, train_loss: 0.9449, step time: 1.1184\n",
"83/388, train_loss: 0.7367, step time: 1.1067\n",
"84/388, train_loss: 0.6248, step time: 1.1483\n",
"85/388, train_loss: 0.8767, step time: 1.1372\n",
"86/388, train_loss: 0.7977, step time: 1.1222\n",
"87/388, train_loss: 0.5903, step time: 1.0512\n",
"88/388, train_loss: 0.6009, step time: 1.0863\n",
"89/388, train_loss: 0.6479, step time: 1.1604\n",
"90/388, train_loss: 0.9269, step time: 1.0356\n",
"91/388, train_loss: 0.5925, step time: 1.1838\n",
"92/388, train_loss: 0.4766, step time: 1.0463\n",
"93/388, train_loss: 0.8452, step time: 1.0469\n",
"94/388, train_loss: 0.7158, step time: 1.0536\n",
"95/388, train_loss: 0.7024, step time: 1.1780\n",
"96/388, train_loss: 0.8714, step time: 1.0852\n",
"97/388, train_loss: 0.8102, step time: 1.1850\n",
"98/388, train_loss: 0.7075, step time: 1.0610\n",
"99/388, train_loss: 0.9198, step time: 1.1057\n",
"100/388, train_loss: 0.9436, step time: 1.0539\n",
"101/388, train_loss: 0.8828, step time: 1.0975\n",
"102/388, train_loss: 0.6953, step time: 1.0708\n",
"103/388, train_loss: 0.6865, step time: 1.1088\n",
"104/388, train_loss: 0.4298, step time: 1.1275\n",
"105/388, train_loss: 0.8130, step time: 1.1897\n",
"106/388, train_loss: 0.6592, step time: 1.0530\n",
"107/388, train_loss: 0.7523, step time: 1.0921\n",
"108/388, train_loss: 0.6892, step time: 1.0808\n",
"109/388, train_loss: 0.5674, step time: 1.1553\n",
"110/388, train_loss: 0.9613, step time: 1.0850\n",
"111/388, train_loss: 0.7247, step time: 1.1628\n",
"112/388, train_loss: 0.6962, step time: 1.0680\n",
"113/388, train_loss: 0.5268, step time: 1.0957\n",
"114/388, train_loss: 0.8412, step time: 1.0472\n",
"115/388, train_loss: 0.5711, step time: 1.2175\n",
"116/388, train_loss: 0.5205, step time: 1.1479\n",
"117/388, train_loss: 0.6428, step time: 1.1277\n",
"118/388, train_loss: 0.7577, step time: 1.0499\n",
"119/388, train_loss: 0.6364, step time: 1.1595\n",
"120/388, train_loss: 0.9720, step time: 1.0487\n",
"121/388, train_loss: 0.6060, step time: 1.2387\n",
"122/388, train_loss: 0.4359, step time: 1.1743\n",
"123/388, train_loss: 0.9222, step time: 1.1555\n",
"124/388, train_loss: 0.5109, step time: 1.1268\n",
"125/388, train_loss: 0.6666, step time: 1.1109\n",
"126/388, train_loss: 0.9445, step time: 1.0556\n",
"127/388, train_loss: 0.6392, step time: 1.1559\n",
"128/388, train_loss: 0.8061, step time: 1.1308\n",
"129/388, train_loss: 0.9814, step time: 1.1636\n",
"130/388, train_loss: 0.8936, step time: 1.1243\n",
"131/388, train_loss: 0.8075, step time: 1.0974\n",
"132/388, train_loss: 0.6923, step time: 1.0887\n",
"133/388, train_loss: 0.5552, step time: 1.0591\n",
"134/388, train_loss: 0.7122, step time: 1.1246\n",
"135/388, train_loss: 0.6391, step time: 1.1701\n",
"136/388, train_loss: 0.5206, step time: 1.1246\n",
"137/388, train_loss: 0.5691, step time: 1.0994\n",
"138/388, train_loss: 0.4904, step time: 1.1435\n",
"139/388, train_loss: 0.5483, step time: 1.0350\n",
"140/388, train_loss: 0.8561, step time: 1.1308\n",
"141/388, train_loss: 0.6813, step time: 1.1598\n",
"142/388, train_loss: 0.5339, step time: 1.1154\n",
"143/388, train_loss: 0.7535, step time: 1.0939\n",
"144/388, train_loss: 0.9336, step time: 1.0641\n",
"145/388, train_loss: 0.8086, step time: 1.0382\n",
"146/388, train_loss: 0.7417, step time: 1.0490\n",
"147/388, train_loss: 0.6866, step time: 1.1462\n",
"148/388, train_loss: 0.8512, step time: 1.0564\n",
"149/388, train_loss: 0.8521, step time: 1.0297\n",
"150/388, train_loss: 0.8391, step time: 1.0465\n",
"151/388, train_loss: 0.6694, step time: 1.1832\n",
"152/388, train_loss: 0.6047, step time: 1.1406\n",
"153/388, train_loss: 0.8271, step time: 1.1865\n",
"154/388, train_loss: 0.7880, step time: 1.1037\n",
"155/388, train_loss: 0.8760, step time: 1.0902\n",
"156/388, train_loss: 0.7934, step time: 1.1047\n",
"157/388, train_loss: 0.7157, step time: 1.0485\n",
"158/388, train_loss: 0.8010, step time: 1.1100\n",
"159/388, train_loss: 0.6954, step time: 1.1046\n",
"160/388, train_loss: 0.5645, step time: 1.1122\n",
"161/388, train_loss: 0.6435, step time: 1.0743\n",
"162/388, train_loss: 0.8104, step time: 1.0954\n",
"163/388, train_loss: 0.8812, step time: 1.0898\n",
"164/388, train_loss: 0.5122, step time: 1.0657\n",
"165/388, train_loss: 0.5777, step time: 1.0181\n",
"166/388, train_loss: 0.6293, step time: 1.1392\n",
"167/388, train_loss: 0.6285, step time: 1.1434\n",
"168/388, train_loss: 0.9444, step time: 1.1105\n",
"169/388, train_loss: 0.8017, step time: 1.0330\n",
"170/388, train_loss: 0.6731, step time: 1.0267\n",
"171/388, train_loss: 0.6645, step time: 1.0491\n",
"172/388, train_loss: 0.7798, step time: 1.0975\n",
"173/388, train_loss: 0.5636, step time: 1.1207\n",
"174/388, train_loss: 0.5539, step time: 1.0569\n",
"175/388, train_loss: 0.3701, step time: 1.0708\n",
"176/388, train_loss: 0.6704, step time: 1.0993\n",
"177/388, train_loss: 0.7525, step time: 1.0215\n",
"178/388, train_loss: 0.8126, step time: 1.0761\n",
"179/388, train_loss: 0.9265, step time: 1.1551\n",
"180/388, train_loss: 0.6673, step time: 1.1163\n",
"181/388, train_loss: 0.5890, step time: 1.0410\n",
"182/388, train_loss: 0.7613, step time: 1.0295\n",
"183/388, train_loss: 0.6753, step time: 1.0680\n",
"184/388, train_loss: 0.3800, step time: 1.0387\n",
"185/388, train_loss: 0.6100, step time: 1.1648\n",
"186/388, train_loss: 0.7055, step time: 1.0534\n",
"187/388, train_loss: 0.9453, step time: 1.0705\n",
"188/388, train_loss: 0.7920, step time: 1.0887\n",
"189/388, train_loss: 0.5671, step time: 1.1285\n",
"190/388, train_loss: 0.7395, step time: 1.0997\n",
"191/388, train_loss: 0.6690, step time: 1.0800\n",
"192/388, train_loss: 0.7378, step time: 1.1069\n",
"193/388, train_loss: 0.5580, step time: 1.0285\n",
"194/388, train_loss: 0.8777, step time: 1.0194\n",
"195/388, train_loss: 0.7910, step time: 1.0913\n",
"196/388, train_loss: 0.7361, step time: 1.0970\n",
"197/388, train_loss: 0.6175, step time: 1.1497\n",
"198/388, train_loss: 0.8087, step time: 1.0581\n",
"199/388, train_loss: 0.6225, step time: 1.1415\n",
"200/388, train_loss: 0.8572, step time: 1.0765\n",
"201/388, train_loss: 0.9437, step time: 1.0090\n",
"202/388, train_loss: 0.5758, step time: 1.0911\n",
"203/388, train_loss: 0.6666, step time: 1.2093\n",
"204/388, train_loss: 0.7080, step time: 1.1028\n",
"205/388, train_loss: 0.7241, step time: 1.1214\n",
"206/388, train_loss: 0.7894, step time: 1.1059\n",
"207/388, train_loss: 0.8244, step time: 1.0197\n",
"208/388, train_loss: 0.6441, step time: 1.0489\n",
"209/388, train_loss: 0.4748, step time: 1.1974\n",
"210/388, train_loss: 0.5722, step time: 1.1326\n",
"211/388, train_loss: 0.8389, step time: 1.1631\n",
"212/388, train_loss: 0.6874, step time: 1.0440\n",
"213/388, train_loss: 0.8796, step time: 1.0814\n",
"214/388, train_loss: 0.8024, step time: 1.0490\n",
"215/388, train_loss: 0.6188, step time: 1.1216\n",
"216/388, train_loss: 0.6284, step time: 1.1264\n",
"217/388, train_loss: 0.6528, step time: 1.0887\n",
"218/388, train_loss: 0.6052, step time: 1.0775\n",
"219/388, train_loss: 0.5716, step time: 1.1138\n",
"220/388, train_loss: 0.9676, step time: 1.0920\n",
"221/388, train_loss: 0.7525, step time: 1.0901\n",
"222/388, train_loss: 0.4555, step time: 1.0637\n",
"223/388, train_loss: 0.4998, step time: 1.2039\n",
"224/388, train_loss: 0.4842, step time: 1.1198\n",
"225/388, train_loss: 0.7766, step time: 1.0633\n",
"226/388, train_loss: 0.8790, step time: 1.0492\n",
"227/388, train_loss: 0.4238, step time: 1.0834\n",
"228/388, train_loss: 0.7473, step time: 1.0942\n",
"229/388, train_loss: 0.8685, step time: 1.2247\n",
"230/388, train_loss: 0.8751, step time: 1.0431\n",
"231/388, train_loss: 0.9346, step time: 1.1040\n",
"232/388, train_loss: 0.5466, step time: 1.1192\n",
"233/388, train_loss: 0.5538, step time: 1.1070\n",
"234/388, train_loss: 0.5847, step time: 1.1277\n",
"235/388, train_loss: 0.7582, step time: 1.1131\n",
"236/388, train_loss: 0.8971, step time: 1.1403\n",
"237/388, train_loss: 0.7691, step time: 1.1900\n",
"238/388, train_loss: 0.7910, step time: 1.1206\n",
"239/388, train_loss: 0.7101, step time: 1.1737\n",
"240/388, train_loss: 0.4165, step time: 1.0605\n",
"241/388, train_loss: 0.5356, step time: 1.2012\n",
"242/388, train_loss: 0.6146, step time: 1.1235\n",
"243/388, train_loss: 0.8595, step time: 1.1663\n",
"244/388, train_loss: 0.8334, step time: 1.1360\n",
"245/388, train_loss: 0.6204, step time: 1.0435\n",
"246/388, train_loss: 0.8979, step time: 1.0528\n",
"247/388, train_loss: 0.6159, step time: 1.0487\n",
"248/388, train_loss: 0.6382, step time: 1.1624\n",
"249/388, train_loss: 0.7026, step time: 1.1912\n",
"250/388, train_loss: 0.8700, step time: 1.0693\n",
"251/388, train_loss: 0.6974, step time: 1.0955\n",
"252/388, train_loss: 0.6110, step time: 1.0652\n",
"253/388, train_loss: 0.5846, step time: 1.0365\n",
"254/388, train_loss: 0.6446, step time: 1.1221\n",
"255/388, train_loss: 0.7770, step time: 1.1622\n",
"256/388, train_loss: 0.8430, step time: 1.1166\n",
"257/388, train_loss: 0.7170, step time: 1.0389\n",
"258/388, train_loss: 0.8521, step time: 1.0473\n",
"259/388, train_loss: 0.8037, step time: 1.0403\n",
"260/388, train_loss: 0.7190, step time: 1.1356\n",
"261/388, train_loss: 0.4403, step time: 1.1278\n",
"262/388, train_loss: 0.6361, step time: 1.1309\n",
"263/388, train_loss: 0.7749, step time: 1.0290\n",
"264/388, train_loss: 0.8047, step time: 1.0989\n",
"265/388, train_loss: 0.8149, step time: 1.1964\n",
"266/388, train_loss: 0.5826, step time: 1.1243\n",
"267/388, train_loss: 0.7599, step time: 1.1641\n",
"268/388, train_loss: 0.8395, step time: 1.0981\n",
"269/388, train_loss: 0.6636, step time: 1.1242\n",
"270/388, train_loss: 0.6496, step time: 1.1117\n",
"271/388, train_loss: 0.5507, step time: 1.1715\n",
"272/388, train_loss: 0.7030, step time: 1.0777\n",
"273/388, train_loss: 0.5351, step time: 1.1879\n",
"274/388, train_loss: 0.9341, step time: 1.1417\n",
"275/388, train_loss: 0.5928, step time: 1.0410\n",
"276/388, train_loss: 0.6421, step time: 1.1250\n",
"277/388, train_loss: 0.7847, step time: 1.1601\n",
"278/388, train_loss: 0.5251, step time: 1.0692\n",
"279/388, train_loss: 0.7140, step time: 1.2415\n",
"280/388, train_loss: 0.5286, step time: 1.1099\n",
"281/388, train_loss: 0.6144, step time: 1.1965\n",
"282/388, train_loss: 0.5851, step time: 1.0583\n",
"283/388, train_loss: 0.6725, step time: 1.1060\n",
"284/388, train_loss: 0.8869, step time: 1.0529\n",
"285/388, train_loss: 0.7723, step time: 1.1951\n",
"286/388, train_loss: 0.5013, step time: 1.1480\n",
"287/388, train_loss: 0.6172, step time: 1.1314\n",
"288/388, train_loss: 0.7873, step time: 1.1360\n",
"289/388, train_loss: 0.8702, step time: 1.0490\n",
"290/388, train_loss: 0.6106, step time: 1.0944\n",
"291/388, train_loss: 0.8061, step time: 1.0608\n",
"292/388, train_loss: 0.8268, step time: 1.1473\n",
"293/388, train_loss: 0.4676, step time: 1.1020\n",
"294/388, train_loss: 0.9587, step time: 1.1420\n",
"295/388, train_loss: 0.6406, step time: 1.0938\n",
"296/388, train_loss: 0.8556, step time: 1.0390\n",
"297/388, train_loss: 0.8269, step time: 1.1678\n",
"298/388, train_loss: 0.7201, step time: 1.1625\n",
"299/388, train_loss: 0.5535, step time: 1.1899\n",
"300/388, train_loss: 0.8761, step time: 1.1209\n",
"301/388, train_loss: 0.6177, step time: 1.0993\n",
"302/388, train_loss: 0.7183, step time: 1.0616\n",
"303/388, train_loss: 0.9282, step time: 1.1584\n",
"304/388, train_loss: 0.8071, step time: 1.1331\n",
"305/388, train_loss: 0.5908, step time: 1.1308\n",
"306/388, train_loss: 0.9169, step time: 1.1028\n",
"307/388, train_loss: 0.9483, step time: 1.0989\n",
"308/388, train_loss: 0.6660, step time: 1.0781\n",
"309/388, train_loss: 0.6412, step time: 1.0641\n",
"310/388, train_loss: 0.7574, step time: 1.1386\n",
"311/388, train_loss: 0.5794, step time: 1.1846\n",
"312/388, train_loss: 0.4324, step time: 1.0654\n",
"313/388, train_loss: 0.6959, step time: 1.0562\n",
"314/388, train_loss: 0.7108, step time: 1.0679\n",
"315/388, train_loss: 0.8104, step time: 1.0789\n",
"316/388, train_loss: 0.6319, step time: 1.1634\n",
"317/388, train_loss: 0.7338, step time: 1.1946\n",
"318/388, train_loss: 0.5908, step time: 1.1373\n",
"319/388, train_loss: 0.7622, step time: 1.1091\n",
"320/388, train_loss: 0.6250, step time: 1.1164\n",
"321/388, train_loss: 0.9257, step time: 1.0550\n",
"322/388, train_loss: 0.5170, step time: 1.1484\n",
"323/388, train_loss: 0.7155, step time: 1.1668\n",
"324/388, train_loss: 0.4245, step time: 1.1127\n",
"325/388, train_loss: 0.9396, step time: 1.1140\n",
"326/388, train_loss: 0.5084, step time: 1.0924\n",
"327/388, train_loss: 0.5590, step time: 1.1117\n",
"328/388, train_loss: 0.5958, step time: 1.1080\n",
"329/388, train_loss: 0.6797, step time: 1.1033\n",
"330/388, train_loss: 0.9029, step time: 1.1120\n",
"331/388, train_loss: 0.6775, step time: 1.1085\n",
"332/388, train_loss: 0.5598, step time: 1.0794\n",
"333/388, train_loss: 0.7807, step time: 1.0613\n",
"334/388, train_loss: 0.5782, step time: 1.0913\n",
"335/388, train_loss: 0.8295, step time: 1.1761\n",
"336/388, train_loss: 0.6784, step time: 1.0990\n",
"337/388, train_loss: 0.8770, step time: 1.1068\n",
"338/388, train_loss: 0.7636, step time: 1.0668\n",
"339/388, train_loss: 0.8372, step time: 1.1265\n",
"340/388, train_loss: 0.4593, step time: 1.0954\n",
"341/388, train_loss: 0.7880, step time: 1.1155\n",
"342/388, train_loss: 0.9207, step time: 1.0530\n",
"343/388, train_loss: 0.5185, step time: 1.0451\n",
"344/388, train_loss: 0.7708, step time: 1.1096\n",
"345/388, train_loss: 0.6953, step time: 1.0357\n",
"346/388, train_loss: 0.5570, step time: 1.1315\n",
"347/388, train_loss: 0.7375, step time: 1.1740\n",
"348/388, train_loss: 0.7125, step time: 1.1777\n",
"349/388, train_loss: 0.6796, step time: 1.1674\n",
"350/388, train_loss: 0.9641, step time: 1.0460\n",
"351/388, train_loss: 0.6804, step time: 1.1630\n",
"352/388, train_loss: 0.8536, step time: 1.0541\n",
"353/388, train_loss: 0.7005, step time: 1.2005\n",
"354/388, train_loss: 0.6377, step time: 1.1142\n",
"355/388, train_loss: 0.9193, step time: 1.1641\n",
"356/388, train_loss: 0.7149, step time: 1.0426\n",
"357/388, train_loss: 0.7877, step time: 1.0423\n",
"358/388, train_loss: 0.6449, step time: 1.0527\n",
"359/388, train_loss: 0.5009, step time: 1.2263\n",
"360/388, train_loss: 0.7338, step time: 1.1433\n",
"361/388, train_loss: 0.5610, step time: 1.1508\n",
"362/388, train_loss: 0.8239, step time: 1.0916\n",
"363/388, train_loss: 0.6784, step time: 1.0384\n",
"364/388, train_loss: 0.6120, step time: 1.0599\n",
"365/388, train_loss: 0.7382, step time: 1.2309\n",
"366/388, train_loss: 0.5482, step time: 1.1369\n",
"367/388, train_loss: 0.5500, step time: 1.1304\n",
"368/388, train_loss: 0.7222, step time: 1.1113\n",
"369/388, train_loss: 0.7890, step time: 1.0773\n",
"370/388, train_loss: 0.5770, step time: 1.0554\n",
"371/388, train_loss: 0.7743, step time: 1.1034\n",
"372/388, train_loss: 0.8752, step time: 1.1878\n",
"373/388, train_loss: 0.6233, step time: 1.1182\n",
"374/388, train_loss: 0.6960, step time: 1.0803\n",
"375/388, train_loss: 0.4007, step time: 1.1004\n",
"376/388, train_loss: 0.6213, step time: 1.0615\n",
"377/388, train_loss: 0.4447, step time: 1.0433\n",
"378/388, train_loss: 0.5576, step time: 1.0573\n",
"379/388, train_loss: 0.6732, step time: 1.1803\n",
"380/388, train_loss: 0.8493, step time: 1.1166\n",
"381/388, train_loss: 0.8196, step time: 1.1084\n",
"382/388, train_loss: 0.6423, step time: 1.1384\n",
"383/388, train_loss: 0.4515, step time: 1.0973\n",
"384/388, train_loss: 0.7314, step time: 1.1358\n",
"385/388, train_loss: 0.4745, step time: 1.2047\n",
"386/388, train_loss: 0.6145, step time: 1.1146\n",
"387/388, train_loss: 0.5420, step time: 1.0231\n",
"388/388, train_loss: 0.8370, step time: 1.0239\n",
"epoch 8 average loss: 0.7058\n",
"current epoch: 8 current mean dice: 0.5829 tc: 0.6158 wt: 0.7999 et: 0.3329\n",
"best mean dice: 0.6254 at epoch: 7\n",
"time consuming of epoch 8 is: 974.5612\n",
"----------\n",
"epoch 9/16\n",
"1/388, train_loss: 0.6069, step time: 1.2319\n",
"2/388, train_loss: 0.7883, step time: 1.1360\n",
"3/388, train_loss: 0.9050, step time: 1.1320\n",
"4/388, train_loss: 0.6321, step time: 1.0592\n",
"5/388, train_loss: 0.7024, step time: 1.1702\n",
"6/388, train_loss: 0.4885, step time: 1.0699\n",
"7/388, train_loss: 0.4653, step time: 1.1640\n",
"8/388, train_loss: 0.7033, step time: 1.1413\n",
"9/388, train_loss: 0.6842, step time: 1.1059\n",
"10/388, train_loss: 0.4606, step time: 1.0677\n",
"11/388, train_loss: 0.7732, step time: 1.1076\n",
"12/388, train_loss: 0.7808, step time: 1.0605\n",
"13/388, train_loss: 0.6899, step time: 1.1220\n",
"14/388, train_loss: 0.8137, step time: 1.1159\n",
"15/388, train_loss: 0.7530, step time: 1.1087\n",
"16/388, train_loss: 0.5507, step time: 1.1254\n",
"17/388, train_loss: 0.8032, step time: 1.1498\n",
"18/388, train_loss: 0.7461, step time: 1.1546\n",
"19/388, train_loss: 0.6745, step time: 1.1602\n",
"20/388, train_loss: 0.8217, step time: 1.0551\n",
"21/388, train_loss: 0.8829, step time: 1.0493\n",
"22/388, train_loss: 0.7824, step time: 1.1504\n",
"23/388, train_loss: 0.6662, step time: 1.1048\n",
"24/388, train_loss: 0.5163, step time: 1.1372\n",
"25/388, train_loss: 0.5174, step time: 1.1842\n",
"26/388, train_loss: 0.7840, step time: 1.1012\n",
"27/388, train_loss: 0.9757, step time: 1.1327\n",
"28/388, train_loss: 0.7221, step time: 1.0959\n",
"29/388, train_loss: 0.6661, step time: 1.2197\n",
"30/388, train_loss: 0.6371, step time: 1.1770\n",
"31/388, train_loss: 0.3903, step time: 1.1081\n",
"32/388, train_loss: 0.6755, step time: 1.1024\n",
"33/388, train_loss: 0.6056, step time: 1.1588\n",
"34/388, train_loss: 0.7875, step time: 1.0479\n",
"35/388, train_loss: 0.6598, step time: 1.0790\n",
"36/388, train_loss: 0.6650, step time: 1.1329\n",
"37/388, train_loss: 0.4633, step time: 1.1338\n",
"38/388, train_loss: 0.5282, step time: 1.1135\n",
"39/388, train_loss: 0.7084, step time: 1.2067\n",
"40/388, train_loss: 0.6164, step time: 1.0646\n",
"41/388, train_loss: 0.8041, step time: 1.1739\n",
"42/388, train_loss: 0.6518, step time: 1.1457\n",
"43/388, train_loss: 0.4151, step time: 1.1101\n",
"44/388, train_loss: 0.9165, step time: 1.0544\n",
"45/388, train_loss: 0.8982, step time: 1.1722\n",
"46/388, train_loss: 0.6683, step time: 1.1334\n",
"47/388, train_loss: 0.5790, step time: 1.1489\n",
"48/388, train_loss: 0.5657, step time: 1.1662\n",
"49/388, train_loss: 0.8474, step time: 1.2104\n",
"50/388, train_loss: 0.6554, step time: 1.1377\n",
"51/388, train_loss: 0.9298, step time: 1.1174\n",
"52/388, train_loss: 0.4306, step time: 1.1309\n",
"53/388, train_loss: 0.7019, step time: 1.2366\n",
"54/388, train_loss: 0.8121, step time: 1.1239\n",
"55/388, train_loss: 0.5507, step time: 1.1166\n",
"56/388, train_loss: 0.7783, step time: 1.1122\n",
"57/388, train_loss: 0.8142, step time: 1.1673\n",
"58/388, train_loss: 0.7148, step time: 1.0524\n",
"59/388, train_loss: 0.5085, step time: 1.2254\n",
"60/388, train_loss: 0.5712, step time: 1.1202\n",
"61/388, train_loss: 0.6511, step time: 1.1663\n",
"62/388, train_loss: 0.5547, step time: 1.1010\n",
"63/388, train_loss: 0.5516, step time: 1.1756\n",
"64/388, train_loss: 0.6984, step time: 1.0679\n",
"65/388, train_loss: 0.5656, step time: 1.1747\n",
"66/388, train_loss: 0.6915, step time: 1.1377\n",
"67/388, train_loss: 0.8285, step time: 1.1217\n",
"68/388, train_loss: 0.5009, step time: 1.0615\n",
"69/388, train_loss: 0.6412, step time: 1.1356\n",
"70/388, train_loss: 0.5055, step time: 1.0711\n",
"71/388, train_loss: 0.9177, step time: 1.1573\n",
"72/388, train_loss: 0.6295, step time: 1.0607\n",
"73/388, train_loss: 0.7486, step time: 1.2038\n",
"74/388, train_loss: 0.6821, step time: 1.0596\n",
"75/388, train_loss: 0.7443, step time: 1.1069\n",
"76/388, train_loss: 0.7049, step time: 1.0875\n",
"77/388, train_loss: 0.8254, step time: 1.2452\n",
"78/388, train_loss: 0.7427, step time: 1.1108\n",
"79/388, train_loss: 0.6000, step time: 1.0520\n",
"80/388, train_loss: 0.8566, step time: 1.1129\n",
"81/388, train_loss: 0.5199, step time: 1.1238\n",
"82/388, train_loss: 0.5478, step time: 1.0424\n",
"83/388, train_loss: 0.6041, step time: 1.2269\n",
"84/388, train_loss: 0.4682, step time: 1.1579\n",
"85/388, train_loss: 0.4616, step time: 1.1291\n",
"86/388, train_loss: 0.6495, step time: 1.1479\n",
"87/388, train_loss: 0.9272, step time: 1.1013\n",
"88/388, train_loss: 0.5123, step time: 1.0692\n",
"89/388, train_loss: 0.6206, step time: 1.2432\n",
"90/388, train_loss: 0.6232, step time: 1.1199\n",
"91/388, train_loss: 0.5844, step time: 1.1769\n",
"92/388, train_loss: 0.6865, step time: 1.0849\n",
"93/388, train_loss: 0.7200, step time: 1.1237\n",
"94/388, train_loss: 0.7568, step time: 1.1022\n",
"95/388, train_loss: 0.6520, step time: 1.2230\n",
"96/388, train_loss: 0.8740, step time: 1.1088\n",
"97/388, train_loss: 0.9246, step time: 1.1015\n",
"98/388, train_loss: 0.6509, step time: 1.1223\n",
"99/388, train_loss: 0.8246, step time: 1.0569\n",
"100/388, train_loss: 0.5116, step time: 1.0484\n",
"101/388, train_loss: 0.7887, step time: 1.2065\n",
"102/388, train_loss: 0.5877, step time: 1.1137\n",
"103/388, train_loss: 0.4974, step time: 1.1497\n",
"104/388, train_loss: 0.6388, step time: 1.1252\n",
"105/388, train_loss: 0.7029, step time: 1.1029\n",
"106/388, train_loss: 0.7886, step time: 1.1297\n",
"107/388, train_loss: 0.4636, step time: 1.2235\n",
"108/388, train_loss: 0.8482, step time: 1.1378\n",
"109/388, train_loss: 0.5418, step time: 1.1711\n",
"110/388, train_loss: 0.4832, step time: 1.0566\n",
"111/388, train_loss: 0.7026, step time: 1.1594\n",
"112/388, train_loss: 0.5915, step time: 1.1103\n",
"113/388, train_loss: 0.5972, step time: 1.2304\n",
"114/388, train_loss: 0.6543, step time: 1.0542\n",
"115/388, train_loss: 0.5891, step time: 1.1191\n",
"116/388, train_loss: 0.3194, step time: 1.1038\n",
"117/388, train_loss: 0.5316, step time: 1.1539\n",
"118/388, train_loss: 0.9537, step time: 1.0538\n",
"119/388, train_loss: 0.5373, step time: 1.2259\n",
"120/388, train_loss: 0.7826, step time: 1.1339\n",
"121/388, train_loss: 0.6199, step time: 1.1165\n",
"122/388, train_loss: 0.5133, step time: 1.1432\n",
"123/388, train_loss: 0.7487, step time: 1.1495\n",
"124/388, train_loss: 0.4720, step time: 1.0760\n",
"125/388, train_loss: 0.5209, step time: 1.1642\n",
"126/388, train_loss: 0.6638, step time: 1.1180\n",
"127/388, train_loss: 0.6278, step time: 1.1083\n",
"128/388, train_loss: 0.9671, step time: 1.1026\n",
"129/388, train_loss: 0.5634, step time: 1.0434\n",
"130/388, train_loss: 0.8677, step time: 1.0454\n",
"131/388, train_loss: 0.8721, step time: 1.2243\n",
"132/388, train_loss: 0.9745, step time: 1.1738\n",
"133/388, train_loss: 0.7982, step time: 1.1065\n",
"134/388, train_loss: 0.6406, step time: 1.0457\n",
"135/388, train_loss: 0.6716, step time: 1.0827\n",
"136/388, train_loss: 0.8232, step time: 1.1314\n",
"137/388, train_loss: 0.6479, step time: 1.0631\n",
"138/388, train_loss: 0.5672, step time: 1.1381\n",
"139/388, train_loss: 0.7890, step time: 1.1865\n",
"140/388, train_loss: 0.8290, step time: 1.0581\n",
"141/388, train_loss: 0.6929, step time: 1.1056\n",
"142/388, train_loss: 0.4548, step time: 1.1203\n",
"143/388, train_loss: 0.5353, step time: 1.1096\n",
"144/388, train_loss: 0.4975, step time: 1.1687\n",
"145/388, train_loss: 0.5166, step time: 1.1758\n",
"146/388, train_loss: 0.3595, step time: 1.0996\n",
"147/388, train_loss: 0.8188, step time: 1.1084\n",
"148/388, train_loss: 0.7795, step time: 1.0519\n",
"149/388, train_loss: 0.7141, step time: 1.1379\n",
"150/388, train_loss: 0.6672, step time: 1.1696\n",
"151/388, train_loss: 0.9191, step time: 1.2066\n",
"152/388, train_loss: 0.5856, step time: 1.1170\n",
"153/388, train_loss: 0.5492, step time: 1.1100\n",
"154/388, train_loss: 0.7473, step time: 1.0758\n",
"155/388, train_loss: 0.8253, step time: 1.0530\n",
"156/388, train_loss: 0.5223, step time: 1.1523\n",
"157/388, train_loss: 0.7847, step time: 1.1442\n",
"158/388, train_loss: 0.7315, step time: 1.0568\n",
"159/388, train_loss: 0.4585, step time: 1.1434\n",
"160/388, train_loss: 0.6796, step time: 1.0668\n",
"161/388, train_loss: 0.8250, step time: 1.0316\n",
"162/388, train_loss: 0.5981, step time: 1.1201\n",
"163/388, train_loss: 0.6992, step time: 1.1130\n",
"164/388, train_loss: 0.4644, step time: 1.1256\n",
"165/388, train_loss: 0.8410, step time: 1.0558\n",
"166/388, train_loss: 0.6525, step time: 1.0872\n",
"167/388, train_loss: 0.7454, step time: 1.1646\n",
"168/388, train_loss: 0.4001, step time: 1.0656\n",
"169/388, train_loss: 0.5804, step time: 1.1910\n",
"170/388, train_loss: 0.7099, step time: 1.0383\n",
"171/388, train_loss: 0.8312, step time: 1.0381\n",
"172/388, train_loss: 0.4398, step time: 1.1307\n",
"173/388, train_loss: 0.7742, step time: 1.1000\n",
"174/388, train_loss: 0.5582, step time: 1.1324\n",
"175/388, train_loss: 0.5722, step time: 1.2042\n",
"176/388, train_loss: 0.7370, step time: 1.0635\n",
"177/388, train_loss: 0.7344, step time: 1.1578\n",
"178/388, train_loss: 0.8419, step time: 1.0921\n",
"179/388, train_loss: 0.6529, step time: 1.0587\n",
"180/388, train_loss: 0.6958, step time: 1.1421\n",
"181/388, train_loss: 0.5278, step time: 1.2257\n",
"182/388, train_loss: 0.5514, step time: 1.1126\n",
"183/388, train_loss: 0.9581, step time: 1.1122\n",
"184/388, train_loss: 0.6752, step time: 1.0552\n",
"185/388, train_loss: 0.6136, step time: 1.1764\n",
"186/388, train_loss: 0.5426, step time: 1.1321\n",
"187/388, train_loss: 0.6936, step time: 1.2098\n",
"188/388, train_loss: 0.3772, step time: 1.1545\n",
"189/388, train_loss: 0.5096, step time: 1.1717\n",
"190/388, train_loss: 0.8110, step time: 1.1379\n",
"191/388, train_loss: 0.9312, step time: 1.0513\n",
"192/388, train_loss: 0.5620, step time: 1.1252\n",
"193/388, train_loss: 0.6111, step time: 1.2458\n",
"194/388, train_loss: 0.4643, step time: 1.2199\n",
"195/388, train_loss: 0.5251, step time: 1.1927\n",
"196/388, train_loss: 0.6172, step time: 1.0565\n",
"197/388, train_loss: 0.6198, step time: 1.0750\n",
"198/388, train_loss: 0.4608, step time: 1.0531\n",
"199/388, train_loss: 0.4508, step time: 1.1702\n",
"200/388, train_loss: 0.6378, step time: 1.1666\n",
"201/388, train_loss: 0.5493, step time: 1.1732\n",
"202/388, train_loss: 0.6193, step time: 1.0848\n",
"203/388, train_loss: 0.8147, step time: 1.0630\n",
"204/388, train_loss: 0.8187, step time: 1.1339\n",
"205/388, train_loss: 0.5193, step time: 1.0525\n",
"206/388, train_loss: 0.6587, step time: 1.1189\n",
"207/388, train_loss: 0.9304, step time: 1.1868\n",
"208/388, train_loss: 0.8069, step time: 1.0998\n",
"209/388, train_loss: 0.4582, step time: 1.0989\n",
"210/388, train_loss: 0.3476, step time: 1.1360\n",
"211/388, train_loss: 0.7037, step time: 1.0464\n",
"212/388, train_loss: 0.8245, step time: 1.1145\n",
"213/388, train_loss: 0.4808, step time: 1.1494\n",
"214/388, train_loss: 0.7125, step time: 1.0659\n",
"215/388, train_loss: 0.7108, step time: 1.0420\n",
"216/388, train_loss: 0.5414, step time: 1.1237\n",
"217/388, train_loss: 0.9076, step time: 1.0437\n",
"218/388, train_loss: 0.4464, step time: 1.1373\n",
"219/388, train_loss: 0.5447, step time: 1.1379\n",
"220/388, train_loss: 0.5279, step time: 1.0458\n",
"221/388, train_loss: 0.4198, step time: 1.0574\n",
"222/388, train_loss: 0.9149, step time: 1.1420\n",
"223/388, train_loss: 0.6470, step time: 1.0463\n",
"224/388, train_loss: 0.5700, step time: 1.1257\n",
"225/388, train_loss: 0.5402, step time: 1.2070\n",
"226/388, train_loss: 0.3163, step time: 1.1240\n",
"227/388, train_loss: 0.5938, step time: 1.0566\n",
"228/388, train_loss: 0.5160, step time: 1.0939\n",
"229/388, train_loss: 0.9153, step time: 1.1371\n",
"230/388, train_loss: 0.7962, step time: 1.1265\n",
"231/388, train_loss: 0.9043, step time: 1.1727\n",
"232/388, train_loss: 0.6654, step time: 1.1253\n",
"233/388, train_loss: 0.4375, step time: 1.1546\n",
"234/388, train_loss: 0.8339, step time: 1.1317\n",
"235/388, train_loss: 0.3661, step time: 1.0493\n",
"236/388, train_loss: 0.8264, step time: 1.0805\n",
"237/388, train_loss: 0.8117, step time: 1.1853\n",
"238/388, train_loss: 0.9138, step time: 1.1729\n",
"239/388, train_loss: 0.9176, step time: 1.1697\n",
"240/388, train_loss: 0.9333, step time: 1.0605\n",
"241/388, train_loss: 0.8612, step time: 1.1030\n",
"242/388, train_loss: 0.5318, step time: 1.1290\n",
"243/388, train_loss: 0.6456, step time: 1.2074\n",
"244/388, train_loss: 0.5151, step time: 1.1208\n",
"245/388, train_loss: 0.7369, step time: 1.1636\n",
"246/388, train_loss: 0.4181, step time: 1.0456\n",
"247/388, train_loss: 0.7671, step time: 1.0577\n",
"248/388, train_loss: 0.5500, step time: 1.1363\n",
"249/388, train_loss: 0.5029, step time: 1.0423\n",
"250/388, train_loss: 0.6813, step time: 1.1999\n",
"251/388, train_loss: 0.5838, step time: 1.1182\n",
"252/388, train_loss: 0.5761, step time: 1.1316\n",
"253/388, train_loss: 0.7582, step time: 1.0726\n",
"254/388, train_loss: 0.5217, step time: 1.0531\n",
"255/388, train_loss: 0.8185, step time: 1.0439\n",
"256/388, train_loss: 0.6772, step time: 1.1285\n",
"257/388, train_loss: 0.6797, step time: 1.1688\n",
"258/388, train_loss: 0.5431, step time: 1.0624\n",
"259/388, train_loss: 0.9511, step time: 1.1121\n",
"260/388, train_loss: 0.7184, step time: 1.1386\n",
"261/388, train_loss: 0.8437, step time: 1.1428\n",
"262/388, train_loss: 0.5274, step time: 1.0973\n",
"263/388, train_loss: 0.6175, step time: 1.1390\n",
"264/388, train_loss: 0.5617, step time: 1.0609\n",
"265/388, train_loss: 0.9266, step time: 1.0973\n",
"266/388, train_loss: 0.7368, step time: 1.0560\n",
"267/388, train_loss: 0.8607, step time: 1.0696\n",
"268/388, train_loss: 0.7350, step time: 1.0603\n",
"269/388, train_loss: 0.9000, step time: 1.2551\n",
"270/388, train_loss: 0.7466, step time: 1.0506\n",
"271/388, train_loss: 0.5075, step time: 1.1167\n",
"272/388, train_loss: 0.5963, step time: 1.0458\n",
"273/388, train_loss: 0.5684, step time: 1.1001\n",
"274/388, train_loss: 0.4594, step time: 1.0780\n",
"275/388, train_loss: 0.5631, step time: 1.1303\n",
"276/388, train_loss: 0.9221, step time: 1.1308\n",
"277/388, train_loss: 0.6459, step time: 1.0462\n",
"278/388, train_loss: 0.4967, step time: 1.0481\n",
"279/388, train_loss: 0.5249, step time: 1.0541\n",
"280/388, train_loss: 0.7101, step time: 1.0624\n",
"281/388, train_loss: 0.7269, step time: 1.2024\n",
"282/388, train_loss: 0.7252, step time: 1.1392\n",
"283/388, train_loss: 0.5058, step time: 1.1827\n",
"284/388, train_loss: 0.7613, step time: 1.0595\n",
"285/388, train_loss: 0.5195, step time: 1.0661\n",
"286/388, train_loss: 0.5210, step time: 1.1108\n",
"287/388, train_loss: 0.4932, step time: 1.1056\n",
"288/388, train_loss: 0.7345, step time: 1.1316\n",
"289/388, train_loss: 0.8353, step time: 1.1693\n",
"290/388, train_loss: 0.3276, step time: 1.0507\n",
"291/388, train_loss: 0.8719, step time: 1.1183\n",
"292/388, train_loss: 0.4688, step time: 1.1249\n",
"293/388, train_loss: 0.6085, step time: 1.1126\n",
"294/388, train_loss: 0.5670, step time: 1.0478\n",
"295/388, train_loss: 0.7568, step time: 1.1371\n",
"296/388, train_loss: 0.5530, step time: 1.1260\n",
"297/388, train_loss: 0.6040, step time: 1.0497\n",
"298/388, train_loss: 0.4385, step time: 1.0441\n",
"299/388, train_loss: 0.7106, step time: 1.0482\n",
"300/388, train_loss: 0.8268, step time: 1.1384\n",
"301/388, train_loss: 0.7770, step time: 1.2363\n",
"302/388, train_loss: 0.3274, step time: 1.0520\n",
"303/388, train_loss: 0.7026, step time: 1.1153\n",
"304/388, train_loss: 0.4091, step time: 1.0667\n",
"305/388, train_loss: 0.5967, step time: 1.1231\n",
"306/388, train_loss: 0.6141, step time: 1.0592\n",
"307/388, train_loss: 0.8281, step time: 1.1877\n",
"308/388, train_loss: 0.5599, step time: 1.0573\n",
"309/388, train_loss: 0.6776, step time: 1.0434\n",
"310/388, train_loss: 0.8011, step time: 1.0960\n",
"311/388, train_loss: 0.6280, step time: 1.1029\n",
"312/388, train_loss: 0.6339, step time: 1.0976\n",
"313/388, train_loss: 0.5444, step time: 1.1574\n",
"314/388, train_loss: 0.5373, step time: 1.0664\n",
"315/388, train_loss: 0.7189, step time: 1.1854\n",
"316/388, train_loss: 0.5012, step time: 1.1326\n",
"317/388, train_loss: 0.4886, step time: 1.1742\n",
"318/388, train_loss: 0.7881, step time: 1.1367\n",
"319/388, train_loss: 0.9125, step time: 1.2052\n",
"320/388, train_loss: 0.8434, step time: 1.1203\n",
"321/388, train_loss: 0.8123, step time: 1.0534\n",
"322/388, train_loss: 0.5445, step time: 1.0968\n",
"323/388, train_loss: 0.6984, step time: 1.1137\n",
"324/388, train_loss: 0.6403, step time: 1.0718\n",
"325/388, train_loss: 0.5104, step time: 1.2038\n",
"326/388, train_loss: 0.2921, step time: 1.1544\n",
"327/388, train_loss: 0.6171, step time: 1.1900\n",
"328/388, train_loss: 0.6064, step time: 1.0631\n",
"329/388, train_loss: 0.9586, step time: 1.0387\n",
"330/388, train_loss: 0.7801, step time: 1.1250\n",
"331/388, train_loss: 0.5200, step time: 1.2507\n",
"332/388, train_loss: 0.3180, step time: 1.1366\n",
"333/388, train_loss: 0.5232, step time: 1.1264\n",
"334/388, train_loss: 0.5535, step time: 1.0596\n",
"335/388, train_loss: 0.5583, step time: 1.1178\n",
"336/388, train_loss: 0.5369, step time: 1.0759\n",
"337/388, train_loss: 0.4599, step time: 1.1113\n",
"338/388, train_loss: 0.6779, step time: 1.1771\n",
"339/388, train_loss: 0.3846, step time: 1.1024\n",
"340/388, train_loss: 0.4837, step time: 1.1017\n",
"341/388, train_loss: 0.8229, step time: 1.1083\n",
"342/388, train_loss: 0.5401, step time: 1.1253\n",
"343/388, train_loss: 0.5444, step time: 1.0438\n",
"344/388, train_loss: 0.7873, step time: 1.1627\n",
"345/388, train_loss: 0.6254, step time: 1.1081\n",
"346/388, train_loss: 0.6598, step time: 1.1363\n",
"347/388, train_loss: 0.5490, step time: 1.0516\n",
"348/388, train_loss: 0.3127, step time: 1.0525\n",
"349/388, train_loss: 0.5393, step time: 1.0421\n",
"350/388, train_loss: 0.6440, step time: 1.0942\n",
"351/388, train_loss: 0.7131, step time: 1.1676\n",
"352/388, train_loss: 0.7982, step time: 1.1222\n",
"353/388, train_loss: 0.6900, step time: 1.0384\n",
"354/388, train_loss: 0.4405, step time: 1.1250\n",
"355/388, train_loss: 0.6445, step time: 1.1802\n",
"356/388, train_loss: 0.3967, step time: 1.1262\n",
"357/388, train_loss: 0.9394, step time: 1.1798\n",
"358/388, train_loss: 0.9644, step time: 1.0512\n",
"359/388, train_loss: 0.7328, step time: 1.0966\n",
"360/388, train_loss: 0.4867, step time: 1.0926\n",
"361/388, train_loss: 0.5695, step time: 1.1709\n",
"362/388, train_loss: 0.6443, step time: 1.0758\n",
"363/388, train_loss: 0.6557, step time: 1.1603\n",
"364/388, train_loss: 0.6945, step time: 1.1198\n",
"365/388, train_loss: 0.8006, step time: 1.1161\n",
"366/388, train_loss: 0.2674, step time: 1.1241\n",
"367/388, train_loss: 0.5887, step time: 1.1210\n",
"368/388, train_loss: 0.5371, step time: 1.0634\n",
"369/388, train_loss: 0.8123, step time: 1.1634\n",
"370/388, train_loss: 0.4114, step time: 1.0529\n",
"371/388, train_loss: 0.5457, step time: 1.0516\n",
"372/388, train_loss: 0.6073, step time: 1.1433\n",
"373/388, train_loss: 0.8704, step time: 1.0888\n",
"374/388, train_loss: 0.4389, step time: 1.0613\n",
"375/388, train_loss: 0.5783, step time: 1.1697\n",
"376/388, train_loss: 0.8804, step time: 1.1371\n",
"377/388, train_loss: 0.5529, step time: 1.1326\n",
"378/388, train_loss: 0.5180, step time: 1.1213\n",
"379/388, train_loss: 0.4902, step time: 1.0697\n",
"380/388, train_loss: 0.6918, step time: 1.0579\n",
"381/388, train_loss: 0.3887, step time: 1.1913\n",
"382/388, train_loss: 0.5047, step time: 1.1343\n",
"383/388, train_loss: 0.9127, step time: 1.1769\n",
"384/388, train_loss: 0.6564, step time: 1.1234\n",
"385/388, train_loss: 0.4866, step time: 1.1721\n",
"386/388, train_loss: 0.6647, step time: 1.1067\n",
"387/388, train_loss: 0.7616, step time: 1.0403\n",
"388/388, train_loss: 0.4806, step time: 1.0480\n",
"epoch 9 average loss: 0.6515\n",
"current epoch: 9 current mean dice: 0.5857 tc: 0.6368 wt: 0.7774 et: 0.3427\n",
"best mean dice: 0.6254 at epoch: 7\n",
"time consuming of epoch 9 is: 992.4738\n",
"----------\n",
"epoch 10/16\n",
"1/388, train_loss: 0.4271, step time: 1.1726\n",
"2/388, train_loss: 0.6537, step time: 1.0619\n",
"3/388, train_loss: 0.7738, step time: 1.1064\n",
"4/388, train_loss: 0.6820, step time: 1.1336\n",
"5/388, train_loss: 0.5878, step time: 1.1126\n",
"6/388, train_loss: 0.6081, step time: 1.0511\n",
"7/388, train_loss: 0.4738, step time: 1.0873\n",
"8/388, train_loss: 0.9034, step time: 1.1281\n",
"9/388, train_loss: 0.9164, step time: 1.0989\n",
"10/388, train_loss: 0.4784, step time: 1.1331\n",
"11/388, train_loss: 0.5763, step time: 1.1735\n",
"12/388, train_loss: 0.3519, step time: 1.1444\n",
"13/388, train_loss: 0.6041, step time: 1.0754\n",
"14/388, train_loss: 0.4887, step time: 1.1199\n",
"15/388, train_loss: 0.8741, step time: 1.0972\n",
"16/388, train_loss: 0.5679, step time: 1.1273\n",
"17/388, train_loss: 0.7263, step time: 1.2352\n",
"18/388, train_loss: 0.5275, step time: 1.0928\n",
"19/388, train_loss: 0.7024, step time: 1.1126\n",
"20/388, train_loss: 0.4821, step time: 1.1561\n",
"21/388, train_loss: 0.7328, step time: 1.1032\n",
"22/388, train_loss: 0.3843, step time: 1.1259\n",
"23/388, train_loss: 0.9052, step time: 1.1253\n",
"24/388, train_loss: 0.4722, step time: 1.1401\n",
"25/388, train_loss: 0.6868, step time: 1.1237\n",
"26/388, train_loss: 0.6236, step time: 1.0933\n",
"27/388, train_loss: 0.4650, step time: 1.1021\n",
"28/388, train_loss: 0.4379, step time: 1.0757\n",
"29/388, train_loss: 0.2892, step time: 1.0899\n",
"30/388, train_loss: 0.9289, step time: 1.1475\n",
"31/388, train_loss: 0.6862, step time: 1.0985\n",
"32/388, train_loss: 0.5308, step time: 1.1245\n",
"33/388, train_loss: 0.2880, step time: 1.0983\n",
"34/388, train_loss: 0.4269, step time: 1.0587\n",
"35/388, train_loss: 0.7613, step time: 1.0718\n",
"36/388, train_loss: 0.2543, step time: 1.1385\n",
"37/388, train_loss: 0.6715, step time: 1.1052\n",
"38/388, train_loss: 0.7787, step time: 1.1240\n",
"39/388, train_loss: 0.4834, step time: 1.0872\n",
"40/388, train_loss: 0.6373, step time: 1.0939\n",
"41/388, train_loss: 0.3619, step time: 1.0689\n",
"42/388, train_loss: 0.5995, step time: 1.0546\n",
"43/388, train_loss: 0.3716, step time: 1.1908\n",
"44/388, train_loss: 0.6974, step time: 1.0920\n",
"45/388, train_loss: 0.7730, step time: 1.0452\n",
"46/388, train_loss: 0.3721, step time: 1.1302\n",
"47/388, train_loss: 0.7700, step time: 1.1189\n",
"48/388, train_loss: 0.9421, step time: 1.1000\n",
"49/388, train_loss: 0.3607, step time: 1.2222\n",
"50/388, train_loss: 0.7378, step time: 1.1186\n",
"51/388, train_loss: 0.4524, step time: 1.0610\n",
"52/388, train_loss: 0.3724, step time: 1.1091\n",
"53/388, train_loss: 0.6033, step time: 1.0978\n",
"54/388, train_loss: 0.6837, step time: 1.0321\n",
"55/388, train_loss: 0.4833, step time: 1.2346\n",
"56/388, train_loss: 0.8576, step time: 1.0458\n",
"57/388, train_loss: 0.8806, step time: 1.0913\n",
"58/388, train_loss: 0.6130, step time: 1.0515\n",
"59/388, train_loss: 0.4633, step time: 1.0971\n",
"60/388, train_loss: 0.5528, step time: 1.1057\n",
"61/388, train_loss: 0.6014, step time: 1.1865\n",
"62/388, train_loss: 0.8247, step time: 1.0755\n",
"63/388, train_loss: 0.7646, step time: 1.1716\n",
"64/388, train_loss: 0.7299, step time: 1.0584\n",
"65/388, train_loss: 0.7097, step time: 1.1056\n",
"66/388, train_loss: 0.4934, step time: 1.0694\n",
"67/388, train_loss: 0.5684, step time: 1.0987\n",
"68/388, train_loss: 0.4191, step time: 1.1325\n",
"69/388, train_loss: 0.4918, step time: 1.1339\n",
"70/388, train_loss: 0.8162, step time: 1.0503\n",
"71/388, train_loss: 0.5225, step time: 1.0437\n",
"72/388, train_loss: 0.7291, step time: 1.1137\n",
"73/388, train_loss: 0.5134, step time: 1.1162\n",
"74/388, train_loss: 0.7358, step time: 1.1073\n",
"75/388, train_loss: 0.5380, step time: 1.1924\n",
"76/388, train_loss: 0.6510, step time: 1.0608\n",
"77/388, train_loss: 0.4570, step time: 1.0489\n",
"78/388, train_loss: 0.5305, step time: 1.0654\n",
"79/388, train_loss: 0.4375, step time: 1.0645\n",
"80/388, train_loss: 0.8488, step time: 1.1339\n",
"81/388, train_loss: 0.8212, step time: 1.2221\n",
"82/388, train_loss: 0.2893, step time: 1.1410\n",
"83/388, train_loss: 0.6191, step time: 1.1665\n",
"84/388, train_loss: 0.4824, step time: 1.1339\n",
"85/388, train_loss: 0.6280, step time: 1.1856\n",
"86/388, train_loss: 0.6686, step time: 1.1271\n",
"87/388, train_loss: 0.8169, step time: 1.1602\n",
"88/388, train_loss: 0.5633, step time: 1.1231\n",
"89/388, train_loss: 0.4742, step time: 1.1067\n",
"90/388, train_loss: 0.7010, step time: 1.1458\n",
"91/388, train_loss: 0.6719, step time: 1.0584\n",
"92/388, train_loss: 0.5468, step time: 1.1200\n",
"93/388, train_loss: 0.6192, step time: 1.1050\n",
"94/388, train_loss: 0.9066, step time: 1.1082\n",
"95/388, train_loss: 0.4795, step time: 1.1704\n",
"96/388, train_loss: 0.7007, step time: 1.0780\n",
"97/388, train_loss: 0.9629, step time: 1.0764\n",
"98/388, train_loss: 0.7730, step time: 1.0482\n",
"99/388, train_loss: 0.5491, step time: 1.0490\n",
"100/388, train_loss: 0.7441, step time: 1.1303\n",
"101/388, train_loss: 0.4843, step time: 1.1165\n",
"102/388, train_loss: 0.5019, step time: 1.0581\n",
"103/388, train_loss: 0.8752, step time: 1.0557\n",
"104/388, train_loss: 0.6353, step time: 1.1214\n",
"105/388, train_loss: 0.5369, step time: 1.1600\n",
"106/388, train_loss: 0.4273, step time: 1.1262\n",
"107/388, train_loss: 0.7280, step time: 1.1735\n",
"108/388, train_loss: 0.4773, step time: 1.0704\n",
"109/388, train_loss: 0.4020, step time: 1.0421\n",
"110/388, train_loss: 0.6512, step time: 1.0635\n",
"111/388, train_loss: 0.6603, step time: 1.0950\n",
"112/388, train_loss: 0.6727, step time: 1.1223\n",
"113/388, train_loss: 0.5808, step time: 1.2061\n",
"114/388, train_loss: 0.5441, step time: 1.1247\n",
"115/388, train_loss: 0.7958, step time: 1.1125\n",
"116/388, train_loss: 0.4282, step time: 1.1282\n",
"117/388, train_loss: 0.5327, step time: 1.0417\n",
"118/388, train_loss: 0.4151, step time: 1.1307\n",
"119/388, train_loss: 0.6708, step time: 1.1906\n",
"120/388, train_loss: 0.4432, step time: 1.1004\n",
"121/388, train_loss: 0.5000, step time: 1.1013\n",
"122/388, train_loss: 0.3876, step time: 1.1029\n",
"123/388, train_loss: 0.9443, step time: 1.1026\n",
"124/388, train_loss: 0.4343, step time: 1.0958\n",
"125/388, train_loss: 0.4538, step time: 1.1632\n",
"126/388, train_loss: 0.4124, step time: 1.1160\n",
"127/388, train_loss: 0.5103, step time: 1.1841\n",
"128/388, train_loss: 0.8327, step time: 1.0475\n",
"129/388, train_loss: 0.8825, step time: 1.0946\n",
"130/388, train_loss: 0.5835, step time: 1.0703\n",
"131/388, train_loss: 0.5816, step time: 1.0662\n",
"132/388, train_loss: 0.5333, step time: 1.1449\n",
"133/388, train_loss: 0.4959, step time: 1.1538\n",
"134/388, train_loss: 0.5530, step time: 1.1213\n",
"135/388, train_loss: 0.4498, step time: 1.0492\n",
"136/388, train_loss: 0.4516, step time: 1.1127\n",
"137/388, train_loss: 0.4685, step time: 1.0829\n",
"138/388, train_loss: 0.5519, step time: 1.1182\n",
"139/388, train_loss: 0.6395, step time: 1.1772\n",
"140/388, train_loss: 0.5610, step time: 1.0816\n",
"141/388, train_loss: 0.4107, step time: 1.0618\n",
"142/388, train_loss: 0.4787, step time: 1.0680\n",
"143/388, train_loss: 0.5169, step time: 1.0339\n",
"144/388, train_loss: 0.3879, step time: 1.1300\n",
"145/388, train_loss: 0.7686, step time: 1.2078\n",
"146/388, train_loss: 0.4903, step time: 1.1285\n",
"147/388, train_loss: 0.5507, step time: 1.1068\n",
"148/388, train_loss: 0.7480, step time: 1.1340\n",
"149/388, train_loss: 0.4137, step time: 1.0560\n",
"150/388, train_loss: 0.8889, step time: 1.1297\n",
"151/388, train_loss: 0.5242, step time: 1.2558\n",
"152/388, train_loss: 0.3933, step time: 1.1361\n",
"153/388, train_loss: 0.5879, step time: 1.0833\n",
"154/388, train_loss: 0.7804, step time: 1.1344\n",
"155/388, train_loss: 0.6295, step time: 1.0406\n",
"156/388, train_loss: 0.7402, step time: 1.0642\n",
"157/388, train_loss: 0.6963, step time: 1.1922\n",
"158/388, train_loss: 0.4809, step time: 1.1103\n",
"159/388, train_loss: 0.6006, step time: 1.1735\n",
"160/388, train_loss: 0.3696, step time: 1.1115\n",
"161/388, train_loss: 0.5985, step time: 1.0388\n",
"162/388, train_loss: 0.6112, step time: 1.1082\n",
"163/388, train_loss: 0.5702, step time: 1.0567\n",
"164/388, train_loss: 0.3255, step time: 1.1249\n",
"165/388, train_loss: 0.7911, step time: 1.1174\n",
"166/388, train_loss: 0.3827, step time: 1.1290\n",
"167/388, train_loss: 0.4186, step time: 1.1162\n",
"168/388, train_loss: 0.6435, step time: 1.1052\n",
"169/388, train_loss: 0.7269, step time: 1.1427\n",
"170/388, train_loss: 0.7155, step time: 1.1219\n",
"171/388, train_loss: 0.4974, step time: 1.2133\n",
"172/388, train_loss: 0.4428, step time: 1.1209\n",
"173/388, train_loss: 0.5525, step time: 1.0457\n",
"174/388, train_loss: 0.3551, step time: 1.1083\n",
"175/388, train_loss: 0.5738, step time: 1.0900\n",
"176/388, train_loss: 0.9145, step time: 1.1353\n",
"177/388, train_loss: 0.8906, step time: 1.1572\n",
"178/388, train_loss: 0.3700, step time: 1.1206\n",
"179/388, train_loss: 0.8000, step time: 1.1510\n",
"180/388, train_loss: 0.6574, step time: 1.0625\n",
"181/388, train_loss: 0.3529, step time: 1.0949\n",
"182/388, train_loss: 0.3382, step time: 1.1115\n",
"183/388, train_loss: 0.6488, step time: 1.1378\n",
"184/388, train_loss: 0.2736, step time: 1.1226\n",
"185/388, train_loss: 0.4175, step time: 1.0418\n",
"186/388, train_loss: 0.3964, step time: 1.1049\n",
"187/388, train_loss: 0.4461, step time: 1.0976\n",
"188/388, train_loss: 0.7307, step time: 1.0656\n",
"189/388, train_loss: 0.6715, step time: 1.1054\n",
"190/388, train_loss: 0.4849, step time: 1.1800\n",
"191/388, train_loss: 0.3122, step time: 1.1247\n",
"192/388, train_loss: 0.5119, step time: 1.0785\n",
"193/388, train_loss: 0.6465, step time: 1.0539\n",
"194/388, train_loss: 0.4462, step time: 1.0877\n",
"195/388, train_loss: 0.8099, step time: 1.1233\n",
"196/388, train_loss: 0.5585, step time: 1.1266\n",
"197/388, train_loss: 0.4379, step time: 1.2091\n",
"198/388, train_loss: 0.6269, step time: 1.0464\n",
"199/388, train_loss: 0.8082, step time: 1.0780\n",
"200/388, train_loss: 0.4564, step time: 1.0828\n",
"201/388, train_loss: 0.4383, step time: 1.0857\n",
"202/388, train_loss: 0.7494, step time: 1.1375\n",
"203/388, train_loss: 0.6264, step time: 1.1761\n",
"204/388, train_loss: 0.4245, step time: 1.1493\n",
"205/388, train_loss: 0.3564, step time: 1.0888\n",
"206/388, train_loss: 0.4158, step time: 1.0548\n",
"207/388, train_loss: 0.5885, step time: 1.0988\n",
"208/388, train_loss: 0.4252, step time: 1.1406\n",
"209/388, train_loss: 0.7904, step time: 1.1813\n",
"210/388, train_loss: 0.2323, step time: 1.0502\n",
"211/388, train_loss: 0.4285, step time: 1.1112\n",
"212/388, train_loss: 0.4793, step time: 1.0594\n",
"213/388, train_loss: 0.4393, step time: 1.0934\n",
"214/388, train_loss: 0.4911, step time: 1.0695\n",
"215/388, train_loss: 0.5688, step time: 1.2178\n",
"216/388, train_loss: 0.4697, step time: 1.0555\n",
"217/388, train_loss: 0.8439, step time: 1.1566\n",
"218/388, train_loss: 0.5617, step time: 1.0378\n",
"219/388, train_loss: 0.4721, step time: 1.0927\n",
"220/388, train_loss: 0.6069, step time: 1.0569\n",
"221/388, train_loss: 0.6425, step time: 1.1361\n",
"222/388, train_loss: 0.7077, step time: 1.1377\n",
"223/388, train_loss: 0.9315, step time: 1.1915\n",
"224/388, train_loss: 0.8461, step time: 1.1474\n",
"225/388, train_loss: 0.6434, step time: 1.1007\n",
"226/388, train_loss: 0.5486, step time: 1.1252\n",
"227/388, train_loss: 0.4925, step time: 1.1039\n",
"228/388, train_loss: 0.4849, step time: 1.1721\n",
"229/388, train_loss: 0.4665, step time: 1.0934\n",
"230/388, train_loss: 0.6380, step time: 1.1218\n",
"231/388, train_loss: 0.4854, step time: 1.0467\n",
"232/388, train_loss: 0.4421, step time: 1.1311\n",
"233/388, train_loss: 0.7249, step time: 1.0505\n",
"234/388, train_loss: 0.6707, step time: 1.0887\n",
"235/388, train_loss: 0.5706, step time: 1.1124\n",
"236/388, train_loss: 0.4981, step time: 1.1198\n",
"237/388, train_loss: 0.5180, step time: 1.0405\n",
"238/388, train_loss: 0.7041, step time: 1.1215\n",
"239/388, train_loss: 0.4333, step time: 1.1596\n",
"240/388, train_loss: 0.6480, step time: 1.1337\n",
"241/388, train_loss: 0.6820, step time: 1.1687\n",
"242/388, train_loss: 0.4792, step time: 1.0575\n",
"243/388, train_loss: 0.3853, step time: 1.0343\n",
"244/388, train_loss: 0.5166, step time: 1.1188\n",
"245/388, train_loss: 0.3493, step time: 1.0355\n",
"246/388, train_loss: 0.5358, step time: 1.0815\n",
"247/388, train_loss: 0.3363, step time: 1.1976\n",
"248/388, train_loss: 0.3587, step time: 1.1406\n",
"249/388, train_loss: 0.8019, step time: 1.0594\n",
"250/388, train_loss: 0.2905, step time: 1.0602\n",
"251/388, train_loss: 0.3891, step time: 1.1001\n",
"252/388, train_loss: 0.6004, step time: 1.0429\n",
"253/388, train_loss: 0.7563, step time: 1.2291\n",
"254/388, train_loss: 0.5281, step time: 1.1182\n",
"255/388, train_loss: 0.3842, step time: 1.1470\n",
"256/388, train_loss: 0.3962, step time: 1.0812\n",
"257/388, train_loss: 0.6627, step time: 1.0598\n",
"258/388, train_loss: 0.7112, step time: 1.0958\n",
"259/388, train_loss: 0.3155, step time: 1.1004\n",
"260/388, train_loss: 0.5472, step time: 1.1715\n",
"261/388, train_loss: 0.5019, step time: 1.1754\n",
"262/388, train_loss: 0.4956, step time: 1.1116\n",
"263/388, train_loss: 0.3273, step time: 1.1067\n",
"264/388, train_loss: 0.5388, step time: 1.0796\n",
"265/388, train_loss: 0.5244, step time: 1.0943\n",
"266/388, train_loss: 0.7117, step time: 1.1611\n",
"267/388, train_loss: 0.7064, step time: 1.1812\n",
"268/388, train_loss: 0.5186, step time: 1.1266\n",
"269/388, train_loss: 0.8385, step time: 1.0410\n",
"270/388, train_loss: 0.4555, step time: 1.1109\n",
"271/388, train_loss: 0.8591, step time: 1.1018\n",
"272/388, train_loss: 0.9038, step time: 1.1326\n",
"273/388, train_loss: 0.5669, step time: 1.1599\n",
"274/388, train_loss: 0.3136, step time: 1.1266\n",
"275/388, train_loss: 0.5506, step time: 1.0588\n",
"276/388, train_loss: 0.5665, step time: 1.1249\n",
"277/388, train_loss: 0.7460, step time: 1.0809\n",
"278/388, train_loss: 0.2468, step time: 1.0770\n",
"279/388, train_loss: 0.4787, step time: 1.2073\n",
"280/388, train_loss: 0.5552, step time: 1.1250\n",
"281/388, train_loss: 0.5098, step time: 1.0400\n",
"282/388, train_loss: 0.7407, step time: 1.0938\n",
"283/388, train_loss: 0.7197, step time: 1.1243\n",
"284/388, train_loss: 0.6202, step time: 1.1127\n",
"285/388, train_loss: 0.4751, step time: 1.1341\n",
"286/388, train_loss: 0.2213, step time: 1.1195\n",
"287/388, train_loss: 0.5799, step time: 1.1114\n",
"288/388, train_loss: 0.5871, step time: 1.0919\n",
"289/388, train_loss: 0.4742, step time: 1.0906\n",
"290/388, train_loss: 0.3758, step time: 1.1121\n",
"291/388, train_loss: 0.6202, step time: 1.0753\n",
"292/388, train_loss: 0.5349, step time: 1.1179\n",
"293/388, train_loss: 0.5127, step time: 1.1309\n",
"294/388, train_loss: 0.7293, step time: 1.0837\n",
"295/388, train_loss: 0.4812, step time: 1.0359\n",
"296/388, train_loss: 0.2275, step time: 1.0866\n",
"297/388, train_loss: 0.9044, step time: 1.0849\n",
"298/388, train_loss: 0.8239, step time: 1.1648\n",
"299/388, train_loss: 0.3850, step time: 1.1178\n",
"300/388, train_loss: 0.7529, step time: 1.1240\n",
"301/388, train_loss: 0.3794, step time: 1.1098\n",
"302/388, train_loss: 0.4452, step time: 1.0554\n",
"303/388, train_loss: 0.2994, step time: 1.1651\n",
"304/388, train_loss: 0.3912, step time: 1.1332\n",
"305/388, train_loss: 0.4020, step time: 1.1875\n",
"306/388, train_loss: 0.3603, step time: 1.0649\n",
"307/388, train_loss: 0.6855, step time: 1.0460\n",
"308/388, train_loss: 0.6379, step time: 1.0551\n",
"309/388, train_loss: 0.5440, step time: 1.0422\n",
"310/388, train_loss: 0.4307, step time: 1.1313\n",
"311/388, train_loss: 0.4149, step time: 1.2333\n",
"312/388, train_loss: 0.6836, step time: 1.1178\n",
"313/388, train_loss: 0.5293, step time: 1.1551\n",
"314/388, train_loss: 0.3272, step time: 1.1345\n",
"315/388, train_loss: 0.3498, step time: 1.0473\n",
"316/388, train_loss: 0.6737, step time: 1.1317\n",
"317/388, train_loss: 0.4127, step time: 1.1811\n",
"318/388, train_loss: 0.4793, step time: 1.1314\n",
"319/388, train_loss: 0.4524, step time: 1.1095\n",
"320/388, train_loss: 0.7217, step time: 1.0630\n",
"321/388, train_loss: 0.3294, step time: 1.0435\n",
"322/388, train_loss: 0.4987, step time: 1.1422\n",
"323/388, train_loss: 0.6901, step time: 1.2123\n",
"324/388, train_loss: 0.4903, step time: 1.1804\n",
"325/388, train_loss: 0.5687, step time: 1.1917\n",
"326/388, train_loss: 0.6770, step time: 1.0470\n",
"327/388, train_loss: 0.4609, step time: 1.1076\n",
"328/388, train_loss: 0.5186, step time: 1.1237\n",
"329/388, train_loss: 0.5937, step time: 1.1711\n",
"330/388, train_loss: 0.4723, step time: 1.1578\n",
"331/388, train_loss: 0.6360, step time: 1.1843\n",
"332/388, train_loss: 0.3682, step time: 1.0705\n",
"333/388, train_loss: 0.2680, step time: 1.0977\n",
"334/388, train_loss: 0.8926, step time: 1.0640\n",
"335/388, train_loss: 0.3696, step time: 1.1125\n",
"336/388, train_loss: 0.3786, step time: 1.1307\n",
"337/388, train_loss: 0.4196, step time: 1.1554\n",
"338/388, train_loss: 0.3856, step time: 1.0775\n",
"339/388, train_loss: 0.4037, step time: 1.1654\n",
"340/388, train_loss: 0.4723, step time: 1.0539\n",
"341/388, train_loss: 0.1941, step time: 1.0465\n",
"342/388, train_loss: 0.6849, step time: 1.1618\n",
"343/388, train_loss: 0.6132, step time: 1.1296\n",
"344/388, train_loss: 0.2551, step time: 1.0996\n",
"345/388, train_loss: 0.5043, step time: 1.0680\n",
"346/388, train_loss: 0.7856, step time: 1.1079\n",
"347/388, train_loss: 0.3977, step time: 1.1918\n",
"348/388, train_loss: 0.5952, step time: 1.1168\n",
"349/388, train_loss: 0.5028, step time: 1.1380\n",
"350/388, train_loss: 0.3719, step time: 1.1310\n",
"351/388, train_loss: 0.2236, step time: 1.0847\n",
"352/388, train_loss: 0.3959, step time: 1.1109\n",
"353/388, train_loss: 0.4769, step time: 1.0548\n",
"354/388, train_loss: 0.5334, step time: 1.1111\n",
"355/388, train_loss: 0.5612, step time: 1.1254\n",
"356/388, train_loss: 0.4671, step time: 1.1202\n",
"357/388, train_loss: 0.4447, step time: 1.1122\n",
"358/388, train_loss: 0.4097, step time: 1.0899\n",
"359/388, train_loss: 0.2753, step time: 1.1375\n",
"360/388, train_loss: 0.3848, step time: 1.1349\n",
"361/388, train_loss: 0.4671, step time: 1.1437\n",
"362/388, train_loss: 0.3880, step time: 1.0742\n",
"363/388, train_loss: 0.5408, step time: 1.1609\n",
"364/388, train_loss: 0.8371, step time: 1.1017\n",
"365/388, train_loss: 0.6778, step time: 1.1142\n",
"366/388, train_loss: 0.7224, step time: 1.0618\n",
"367/388, train_loss: 0.5183, step time: 1.2197\n",
"368/388, train_loss: 0.5207, step time: 1.0659\n",
"369/388, train_loss: 0.2888, step time: 1.1051\n",
"370/388, train_loss: 0.5280, step time: 1.1148\n",
"371/388, train_loss: 0.9016, step time: 1.0896\n",
"372/388, train_loss: 0.6463, step time: 1.1077\n",
"373/388, train_loss: 0.4379, step time: 1.2420\n",
"374/388, train_loss: 0.3958, step time: 1.1292\n",
"375/388, train_loss: 0.4547, step time: 1.1135\n",
"376/388, train_loss: 0.8124, step time: 1.0593\n",
"377/388, train_loss: 0.4777, step time: 1.1095\n",
"378/388, train_loss: 0.5685, step time: 1.1242\n",
"379/388, train_loss: 0.5924, step time: 1.1701\n",
"380/388, train_loss: 0.4033, step time: 1.1323\n",
"381/388, train_loss: 0.4599, step time: 1.0416\n",
"382/388, train_loss: 0.3871, step time: 1.1388\n",
"383/388, train_loss: 0.4701, step time: 1.1001\n",
"384/388, train_loss: 0.3731, step time: 1.0932\n",
"385/388, train_loss: 0.4148, step time: 1.1054\n",
"386/388, train_loss: 0.4780, step time: 1.1596\n",
"387/388, train_loss: 0.2510, step time: 1.0326\n",
"388/388, train_loss: 0.7352, step time: 1.0355\n",
"epoch 10 average loss: 0.5570\n",
"current epoch: 10 current mean dice: 0.6124 tc: 0.6560 wt: 0.8179 et: 0.3633\n",
"best mean dice: 0.6254 at epoch: 7\n",
"time consuming of epoch 10 is: 955.5523\n",
"----------\n",
"epoch 11/16\n",
"1/388, train_loss: 0.4326, step time: 1.1771\n",
"2/388, train_loss: 0.5167, step time: 1.0829\n",
"3/388, train_loss: 0.2733, step time: 1.1288\n",
"4/388, train_loss: 0.7062, step time: 1.1325\n",
"5/388, train_loss: 0.6021, step time: 1.1827\n",
"6/388, train_loss: 0.4827, step time: 1.0651\n",
"7/388, train_loss: 0.6836, step time: 1.1200\n",
"8/388, train_loss: 0.2503, step time: 1.0621\n",
"9/388, train_loss: 0.4098, step time: 1.0500\n",
"10/388, train_loss: 0.5992, step time: 1.1831\n",
"11/388, train_loss: 0.4540, step time: 1.1650\n",
"12/388, train_loss: 0.2846, step time: 1.1365\n",
"13/388, train_loss: 0.5507, step time: 1.0796\n",
"14/388, train_loss: 0.4609, step time: 1.0777\n",
"15/388, train_loss: 0.4706, step time: 1.0544\n",
"16/388, train_loss: 0.5278, step time: 1.1253\n",
"17/388, train_loss: 0.3234, step time: 1.1136\n",
"18/388, train_loss: 0.7999, step time: 1.1272\n",
"19/388, train_loss: 0.5694, step time: 1.1868\n",
"20/388, train_loss: 0.4310, step time: 1.1796\n",
"21/388, train_loss: 0.4514, step time: 1.0487\n",
"22/388, train_loss: 0.3576, step time: 1.1222\n",
"23/388, train_loss: 0.4666, step time: 1.1755\n",
"24/388, train_loss: 0.2372, step time: 1.1311\n",
"25/388, train_loss: 0.4339, step time: 1.1064\n",
"26/388, train_loss: 0.5453, step time: 1.1275\n",
"27/388, train_loss: 0.5496, step time: 1.0364\n",
"28/388, train_loss: 0.5945, step time: 1.1258\n",
"29/388, train_loss: 0.4070, step time: 1.1355\n",
"30/388, train_loss: 0.2972, step time: 1.0998\n",
"31/388, train_loss: 0.2466, step time: 1.1154\n",
"32/388, train_loss: 0.4093, step time: 1.1071\n",
"33/388, train_loss: 0.5598, step time: 1.1741\n",
"34/388, train_loss: 0.7230, step time: 1.0620\n",
"35/388, train_loss: 0.2002, step time: 1.1876\n",
"36/388, train_loss: 0.3517, step time: 1.0504\n",
"37/388, train_loss: 0.4524, step time: 1.1042\n",
"38/388, train_loss: 0.2494, step time: 1.0821\n",
"39/388, train_loss: 0.3367, step time: 1.0708\n",
"40/388, train_loss: 0.6365, step time: 1.0537\n",
"41/388, train_loss: 0.5001, step time: 1.2312\n",
"42/388, train_loss: 0.3539, step time: 1.1177\n",
"43/388, train_loss: 0.2726, step time: 1.0381\n",
"44/388, train_loss: 0.4907, step time: 1.0690\n",
"45/388, train_loss: 0.7216, step time: 1.1008\n",
"46/388, train_loss: 0.2665, step time: 1.0450\n",
"47/388, train_loss: 0.6992, step time: 1.2178\n",
"48/388, train_loss: 0.4153, step time: 1.1148\n",
"49/388, train_loss: 0.3120, step time: 1.1422\n",
"50/388, train_loss: 0.2958, step time: 1.0843\n",
"51/388, train_loss: 0.3433, step time: 1.1108\n",
"52/388, train_loss: 0.4925, step time: 1.1228\n",
"53/388, train_loss: 0.3916, step time: 1.1393\n",
"54/388, train_loss: 0.2639, step time: 1.1594\n",
"55/388, train_loss: 0.7683, step time: 1.1368\n",
"56/388, train_loss: 0.3451, step time: 1.0817\n",
"57/388, train_loss: 0.1721, step time: 1.1711\n",
"58/388, train_loss: 0.3649, step time: 1.1280\n",
"59/388, train_loss: 0.5791, step time: 1.1527\n",
"60/388, train_loss: 0.8762, step time: 1.1496\n",
"61/388, train_loss: 0.6131, step time: 1.1193\n",
"62/388, train_loss: 0.5341, step time: 1.1276\n",
"63/388, train_loss: 0.3884, step time: 1.1638\n",
"64/388, train_loss: 0.6657, step time: 1.1194\n",
"65/388, train_loss: 0.5309, step time: 1.0782\n",
"66/388, train_loss: 0.4109, step time: 1.1565\n",
"67/388, train_loss: 0.4609, step time: 1.1815\n",
"68/388, train_loss: 0.5218, step time: 1.0508\n",
"69/388, train_loss: 0.4743, step time: 1.2084\n",
"70/388, train_loss: 0.4373, step time: 1.1204\n",
"71/388, train_loss: 0.6723, step time: 1.1029\n",
"72/388, train_loss: 0.4643, step time: 1.1536\n",
"73/388, train_loss: 0.5520, step time: 1.1854\n",
"74/388, train_loss: 0.4497, step time: 1.1224\n",
"75/388, train_loss: 0.3542, step time: 1.0935\n",
"76/388, train_loss: 0.5426, step time: 1.1305\n",
"77/388, train_loss: 0.3838, step time: 1.0807\n",
"78/388, train_loss: 0.7395, step time: 1.1175\n",
"79/388, train_loss: 0.4391, step time: 1.1212\n",
"80/388, train_loss: 0.6117, step time: 1.1248\n",
"81/388, train_loss: 0.4476, step time: 1.1190\n",
"82/388, train_loss: 0.5668, step time: 1.1202\n",
"83/388, train_loss: 0.4887, step time: 1.0383\n",
"84/388, train_loss: 0.6197, step time: 1.1312\n",
"85/388, train_loss: 0.2825, step time: 1.2441\n",
"86/388, train_loss: 0.3812, step time: 1.0613\n",
"87/388, train_loss: 0.6536, step time: 1.1792\n",
"88/388, train_loss: 0.1911, step time: 1.0805\n",
"89/388, train_loss: 0.7939, step time: 1.0949\n",
"90/388, train_loss: 0.3795, step time: 1.1730\n",
"91/388, train_loss: 0.3000, step time: 1.2312\n",
"92/388, train_loss: 0.3490, step time: 1.1050\n",
"93/388, train_loss: 0.7078, step time: 1.1048\n",
"94/388, train_loss: 0.5386, step time: 1.1311\n",
"95/388, train_loss: 0.3687, step time: 1.0687\n",
"96/388, train_loss: 0.7042, step time: 1.1323\n",
"97/388, train_loss: 0.3962, step time: 1.1731\n",
"98/388, train_loss: 0.4215, step time: 1.1316\n",
"99/388, train_loss: 0.6510, step time: 1.1707\n",
"100/388, train_loss: 0.3323, step time: 1.1388\n",
"101/388, train_loss: 0.4904, step time: 1.1272\n",
"102/388, train_loss: 0.3755, step time: 1.0465\n",
"103/388, train_loss: 0.3385, step time: 1.2667\n",
"104/388, train_loss: 0.5047, step time: 1.1151\n",
"105/388, train_loss: 0.6950, step time: 1.1151\n",
"106/388, train_loss: 0.6452, step time: 1.0619\n",
"107/388, train_loss: 0.5605, step time: 1.0971\n",
"108/388, train_loss: 0.4255, step time: 1.0910\n",
"109/388, train_loss: 0.3392, step time: 1.0442\n",
"110/388, train_loss: 0.5496, step time: 1.1612\n",
"111/388, train_loss: 0.5475, step time: 1.1973\n",
"112/388, train_loss: 0.7928, step time: 1.0583\n",
"113/388, train_loss: 0.4279, step time: 1.0500\n",
"114/388, train_loss: 0.8197, step time: 1.1368\n",
"115/388, train_loss: 0.4443, step time: 1.1335\n",
"116/388, train_loss: 0.7259, step time: 1.1624\n",
"117/388, train_loss: 0.4302, step time: 1.1165\n",
"118/388, train_loss: 0.3455, step time: 1.0585\n",
"119/388, train_loss: 0.5485, step time: 1.1687\n",
"120/388, train_loss: 0.7802, step time: 1.1229\n",
"121/388, train_loss: 0.3911, step time: 1.1757\n",
"122/388, train_loss: 0.4354, step time: 1.1766\n",
"123/388, train_loss: 0.4271, step time: 1.0963\n",
"124/388, train_loss: 0.5710, step time: 1.1150\n",
"125/388, train_loss: 0.6283, step time: 1.0932\n",
"126/388, train_loss: 0.7721, step time: 1.0756\n",
"127/388, train_loss: 0.3735, step time: 1.1101\n",
"128/388, train_loss: 0.4046, step time: 1.2157\n",
"129/388, train_loss: 0.6724, step time: 1.1588\n",
"130/388, train_loss: 0.2148, step time: 1.0819\n",
"131/388, train_loss: 0.4612, step time: 1.0935\n",
"132/388, train_loss: 0.2146, step time: 1.1291\n",
"133/388, train_loss: 0.4624, step time: 1.0589\n",
"134/388, train_loss: 0.5672, step time: 1.0524\n",
"135/388, train_loss: 0.4341, step time: 1.2295\n",
"136/388, train_loss: 0.6455, step time: 1.0803\n",
"137/388, train_loss: 0.3456, step time: 1.1705\n",
"138/388, train_loss: 0.3072, step time: 1.0543\n",
"139/388, train_loss: 0.4356, step time: 1.1109\n",
"140/388, train_loss: 0.8163, step time: 1.1173\n",
"141/388, train_loss: 0.3127, step time: 1.1072\n",
"142/388, train_loss: 0.4905, step time: 1.1223\n",
"143/388, train_loss: 0.5152, step time: 1.1381\n",
"144/388, train_loss: 0.3188, step time: 1.0696\n",
"145/388, train_loss: 0.3667, step time: 1.1779\n",
"146/388, train_loss: 0.4235, step time: 1.1662\n",
"147/388, train_loss: 0.4352, step time: 1.1363\n",
"148/388, train_loss: 0.3905, step time: 1.1576\n",
"149/388, train_loss: 0.4181, step time: 1.1755\n",
"150/388, train_loss: 0.4554, step time: 1.0565\n",
"151/388, train_loss: 0.4038, step time: 1.1524\n",
"152/388, train_loss: 0.2663, step time: 1.1298\n",
"153/388, train_loss: 0.3829, step time: 1.1092\n",
"154/388, train_loss: 0.6782, step time: 1.1404\n",
"155/388, train_loss: 0.3874, step time: 1.0864\n",
"156/388, train_loss: 0.3878, step time: 1.0853\n",
"157/388, train_loss: 0.4087, step time: 1.0804\n",
"158/388, train_loss: 0.3701, step time: 1.1339\n",
"159/388, train_loss: 0.3378, step time: 1.1149\n",
"160/388, train_loss: 0.5706, step time: 1.1252\n",
"161/388, train_loss: 0.6253, step time: 1.2023\n",
"162/388, train_loss: 0.3401, step time: 1.0567\n",
"163/388, train_loss: 0.3724, step time: 1.0434\n",
"164/388, train_loss: 0.4182, step time: 1.0788\n",
"165/388, train_loss: 0.6565, step time: 1.1569\n",
"166/388, train_loss: 0.1589, step time: 1.1086\n",
"167/388, train_loss: 0.3680, step time: 1.2144\n",
"168/388, train_loss: 0.2520, step time: 1.0509\n",
"169/388, train_loss: 0.5618, step time: 1.1391\n",
"170/388, train_loss: 0.6660, step time: 1.1245\n",
"171/388, train_loss: 0.2484, step time: 1.0460\n",
"172/388, train_loss: 0.4606, step time: 1.0598\n",
"173/388, train_loss: 0.2171, step time: 1.2230\n",
"174/388, train_loss: 0.4670, step time: 1.1007\n",
"175/388, train_loss: 0.5932, step time: 1.1842\n",
"176/388, train_loss: 0.3064, step time: 1.0541\n",
"177/388, train_loss: 0.8257, step time: 1.1108\n",
"178/388, train_loss: 0.4001, step time: 1.0563\n",
"179/388, train_loss: 0.3639, step time: 1.1132\n",
"180/388, train_loss: 0.2968, step time: 1.1767\n",
"181/388, train_loss: 0.2476, step time: 1.1808\n",
"182/388, train_loss: 0.3441, step time: 1.1237\n",
"183/388, train_loss: 0.3944, step time: 1.0624\n",
"184/388, train_loss: 0.3413, step time: 1.0462\n",
"185/388, train_loss: 0.5732, step time: 1.1081\n",
"186/388, train_loss: 0.1607, step time: 1.1563\n",
"187/388, train_loss: 0.3314, step time: 1.1793\n",
"188/388, train_loss: 0.7892, step time: 1.1062\n",
"189/388, train_loss: 0.3770, step time: 1.0404\n",
"190/388, train_loss: 0.2628, step time: 1.1321\n",
"191/388, train_loss: 0.9019, step time: 1.1043\n",
"192/388, train_loss: 0.2562, step time: 1.0574\n",
"193/388, train_loss: 0.4550, step time: 1.1501\n",
"194/388, train_loss: 0.3557, step time: 1.1376\n",
"195/388, train_loss: 0.3388, step time: 1.1135\n",
"196/388, train_loss: 0.2551, step time: 1.1019\n",
"197/388, train_loss: 0.7124, step time: 1.0831\n",
"198/388, train_loss: 0.4728, step time: 1.1341\n",
"199/388, train_loss: 0.4157, step time: 1.1621\n",
"200/388, train_loss: 0.3824, step time: 1.1352\n",
"201/388, train_loss: 0.5104, step time: 1.1565\n",
"202/388, train_loss: 0.3519, step time: 1.0596\n",
"203/388, train_loss: 0.3932, step time: 1.0731\n",
"204/388, train_loss: 0.3548, step time: 1.1138\n",
"205/388, train_loss: 0.3173, step time: 1.1097\n",
"206/388, train_loss: 0.2833, step time: 1.1257\n",
"207/388, train_loss: 0.3943, step time: 1.1087\n",
"208/388, train_loss: 0.5350, step time: 1.1056\n",
"209/388, train_loss: 0.3277, step time: 1.1676\n",
"210/388, train_loss: 0.5116, step time: 1.1511\n",
"211/388, train_loss: 0.4621, step time: 1.1819\n",
"212/388, train_loss: 0.4881, step time: 1.1499\n",
"213/388, train_loss: 0.4280, step time: 1.0397\n",
"214/388, train_loss: 0.2812, step time: 1.0884\n",
"215/388, train_loss: 0.4264, step time: 1.0779\n",
"216/388, train_loss: 0.4775, step time: 1.1098\n",
"217/388, train_loss: 0.2247, step time: 1.1456\n",
"218/388, train_loss: 0.4837, step time: 1.1507\n",
"219/388, train_loss: 0.6822, step time: 1.2010\n",
"220/388, train_loss: 0.7096, step time: 1.1263\n",
"221/388, train_loss: 0.5148, step time: 1.1253\n",
"222/388, train_loss: 0.6473, step time: 1.0613\n",
"223/388, train_loss: 0.2768, step time: 1.0466\n",
"224/388, train_loss: 0.3891, step time: 1.0741\n",
"225/388, train_loss: 0.7352, step time: 1.1196\n",
"226/388, train_loss: 0.4033, step time: 1.1034\n",
"227/388, train_loss: 0.8506, step time: 1.0874\n",
"228/388, train_loss: 0.3597, step time: 1.1180\n",
"229/388, train_loss: 0.5170, step time: 1.1204\n",
"230/388, train_loss: 0.3848, step time: 1.1673\n",
"231/388, train_loss: 0.3194, step time: 1.1799\n",
"232/388, train_loss: 0.6484, step time: 1.1210\n",
"233/388, train_loss: 0.4713, step time: 1.0542\n",
"234/388, train_loss: 0.3084, step time: 1.1076\n",
"235/388, train_loss: 0.3909, step time: 1.0667\n",
"236/388, train_loss: 0.3974, step time: 1.0465\n",
"237/388, train_loss: 0.4017, step time: 1.2131\n",
"238/388, train_loss: 0.8611, step time: 1.0590\n",
"239/388, train_loss: 0.5054, step time: 1.0393\n",
"240/388, train_loss: 0.4884, step time: 1.1197\n",
"241/388, train_loss: 0.3781, step time: 1.0555\n",
"242/388, train_loss: 0.4040, step time: 1.0451\n",
"243/388, train_loss: 0.4293, step time: 1.1042\n",
"244/388, train_loss: 0.1535, step time: 1.0544\n",
"245/388, train_loss: 0.4198, step time: 1.0534\n",
"246/388, train_loss: 0.3631, step time: 1.1992\n",
"247/388, train_loss: 0.4408, step time: 1.1238\n",
"248/388, train_loss: 0.5052, step time: 1.1257\n",
"249/388, train_loss: 0.8850, step time: 1.1856\n",
"250/388, train_loss: 0.4090, step time: 1.0508\n",
"251/388, train_loss: 0.5637, step time: 1.1851\n",
"252/388, train_loss: 0.2044, step time: 1.1208\n",
"253/388, train_loss: 0.4102, step time: 1.1754\n",
"254/388, train_loss: 0.3260, step time: 1.0556\n",
"255/388, train_loss: 0.2750, step time: 1.1583\n",
"256/388, train_loss: 0.3631, step time: 1.1725\n",
"257/388, train_loss: 0.3617, step time: 1.1206\n",
"258/388, train_loss: 0.3662, step time: 1.0690\n",
"259/388, train_loss: 0.2955, step time: 1.0598\n",
"260/388, train_loss: 0.3063, step time: 1.0471\n",
"261/388, train_loss: 0.3802, step time: 1.1694\n",
"262/388, train_loss: 0.4049, step time: 1.1109\n",
"263/388, train_loss: 0.9025, step time: 1.1868\n",
"264/388, train_loss: 0.3594, step time: 1.1296\n",
"265/388, train_loss: 0.3511, step time: 1.1321\n",
"266/388, train_loss: 0.6685, step time: 1.1147\n",
"267/388, train_loss: 0.2859, step time: 1.1088\n",
"268/388, train_loss: 0.4944, step time: 1.1896\n",
"269/388, train_loss: 0.4477, step time: 1.1319\n",
"270/388, train_loss: 0.4209, step time: 1.1187\n",
"271/388, train_loss: 0.3078, step time: 1.2028\n",
"272/388, train_loss: 0.7326, step time: 1.1226\n",
"273/388, train_loss: 0.8193, step time: 1.1934\n",
"274/388, train_loss: 0.4616, step time: 1.1951\n",
"275/388, train_loss: 0.3327, step time: 1.1617\n",
"276/388, train_loss: 0.4394, step time: 1.0709\n",
"277/388, train_loss: 0.4457, step time: 1.1062\n",
"278/388, train_loss: 0.2076, step time: 1.1234\n",
"279/388, train_loss: 0.3880, step time: 1.0509\n",
"280/388, train_loss: 0.5446, step time: 1.1543\n",
"281/388, train_loss: 0.6373, step time: 1.1702\n",
"282/388, train_loss: 0.1803, step time: 1.1132\n",
"283/388, train_loss: 0.2989, step time: 1.1094\n",
"284/388, train_loss: 0.3472, step time: 1.0517\n",
"285/388, train_loss: 0.3189, step time: 1.0398\n",
"286/388, train_loss: 0.7080, step time: 1.0627\n",
"287/388, train_loss: 0.4014, step time: 1.2579\n",
"288/388, train_loss: 0.4740, step time: 1.1367\n",
"289/388, train_loss: 0.5742, step time: 1.0496\n",
"290/388, train_loss: 0.9087, step time: 1.1028\n",
"291/388, train_loss: 0.4137, step time: 1.0445\n",
"292/388, train_loss: 0.2677, step time: 1.0607\n",
"293/388, train_loss: 0.3160, step time: 1.1940\n",
"294/388, train_loss: 0.5662, step time: 1.1260\n",
"295/388, train_loss: 0.5624, step time: 1.1163\n",
"296/388, train_loss: 0.3801, step time: 1.1215\n",
"297/388, train_loss: 0.5531, step time: 1.0725\n",
"298/388, train_loss: 0.3435, step time: 1.0811\n",
"299/388, train_loss: 0.3656, step time: 1.2285\n",
"300/388, train_loss: 0.3694, step time: 1.1407\n",
"301/388, train_loss: 0.4780, step time: 1.1270\n",
"302/388, train_loss: 0.6559, step time: 1.0545\n",
"303/388, train_loss: 0.3024, step time: 1.0596\n",
"304/388, train_loss: 0.2956, step time: 1.0819\n",
"305/388, train_loss: 0.5723, step time: 1.1859\n",
"306/388, train_loss: 0.4586, step time: 1.1556\n",
"307/388, train_loss: 0.3994, step time: 1.0976\n",
"308/388, train_loss: 0.2923, step time: 1.0555\n",
"309/388, train_loss: 0.5086, step time: 1.0394\n",
"310/388, train_loss: 0.8282, step time: 1.0995\n",
"311/388, train_loss: 0.4740, step time: 1.1095\n",
"312/388, train_loss: 0.3217, step time: 1.1512\n",
"313/388, train_loss: 0.7459, step time: 1.1118\n",
"314/388, train_loss: 0.5851, step time: 1.0459\n",
"315/388, train_loss: 0.4792, step time: 1.1081\n",
"316/388, train_loss: 0.3724, step time: 1.1252\n",
"317/388, train_loss: 0.2653, step time: 1.1133\n",
"318/388, train_loss: 0.3080, step time: 1.1544\n",
"319/388, train_loss: 0.7469, step time: 1.1605\n",
"320/388, train_loss: 0.2822, step time: 1.1289\n",
"321/388, train_loss: 0.2327, step time: 1.0388\n",
"322/388, train_loss: 0.7012, step time: 1.1257\n",
"323/388, train_loss: 0.3589, step time: 1.0558\n",
"324/388, train_loss: 0.4065, step time: 1.0502\n",
"325/388, train_loss: 0.4438, step time: 1.1800\n",
"326/388, train_loss: 0.3294, step time: 1.1180\n",
"327/388, train_loss: 0.3839, step time: 1.0802\n",
"328/388, train_loss: 0.4352, step time: 1.1036\n",
"329/388, train_loss: 0.3710, step time: 1.0342\n",
"330/388, train_loss: 0.5302, step time: 1.0527\n",
"331/388, train_loss: 0.4605, step time: 1.2252\n",
"332/388, train_loss: 0.7024, step time: 1.1463\n",
"333/388, train_loss: 0.3716, step time: 1.1981\n",
"334/388, train_loss: 0.3084, step time: 1.1231\n",
"335/388, train_loss: 0.2365, step time: 1.0607\n",
"336/388, train_loss: 0.3140, step time: 1.1123\n",
"337/388, train_loss: 0.4135, step time: 1.2211\n",
"338/388, train_loss: 0.5486, step time: 1.1544\n",
"339/388, train_loss: 0.3908, step time: 1.1301\n",
"340/388, train_loss: 0.3795, step time: 1.1495\n",
"341/388, train_loss: 0.5319, step time: 1.0741\n",
"342/388, train_loss: 0.3222, step time: 1.0454\n",
"343/388, train_loss: 0.3957, step time: 1.0543\n",
"344/388, train_loss: 0.2408, step time: 1.1329\n",
"345/388, train_loss: 0.5070, step time: 1.1711\n",
"346/388, train_loss: 0.6248, step time: 1.0573\n",
"347/388, train_loss: 0.5248, step time: 1.0976\n",
"348/388, train_loss: 0.1794, step time: 1.1396\n",
"349/388, train_loss: 0.5382, step time: 1.0733\n",
"350/388, train_loss: 0.6996, step time: 1.1554\n",
"351/388, train_loss: 0.4019, step time: 1.1593\n",
"352/388, train_loss: 0.6343, step time: 1.1110\n",
"353/388, train_loss: 0.4350, step time: 1.0793\n",
"354/388, train_loss: 0.3144, step time: 1.1390\n",
"355/388, train_loss: 0.6341, step time: 1.0612\n",
"356/388, train_loss: 0.3387, step time: 1.1899\n",
"357/388, train_loss: 0.3740, step time: 1.1607\n",
"358/388, train_loss: 0.3787, step time: 1.0529\n",
"359/388, train_loss: 0.4682, step time: 1.1045\n",
"360/388, train_loss: 0.3727, step time: 1.0651\n",
"361/388, train_loss: 0.7944, step time: 1.1002\n",
"362/388, train_loss: 0.2542, step time: 1.1507\n",
"363/388, train_loss: 0.4911, step time: 1.1255\n",
"364/388, train_loss: 0.5943, step time: 1.0519\n",
"365/388, train_loss: 0.8474, step time: 1.0806\n",
"366/388, train_loss: 0.3875, step time: 1.1337\n",
"367/388, train_loss: 0.3805, step time: 1.1869\n",
"368/388, train_loss: 0.3241, step time: 1.1424\n",
"369/388, train_loss: 0.8040, step time: 1.1215\n",
"370/388, train_loss: 0.4404, step time: 1.0516\n",
"371/388, train_loss: 0.5723, step time: 1.0961\n",
"372/388, train_loss: 0.7229, step time: 1.1337\n",
"373/388, train_loss: 0.6820, step time: 1.0963\n",
"374/388, train_loss: 0.5242, step time: 1.0961\n",
"375/388, train_loss: 0.3330, step time: 1.2105\n",
"376/388, train_loss: 0.6815, step time: 1.1093\n",
"377/388, train_loss: 0.4438, step time: 1.0369\n",
"378/388, train_loss: 0.4253, step time: 1.1229\n",
"379/388, train_loss: 0.6004, step time: 1.1026\n",
"380/388, train_loss: 0.6984, step time: 1.0493\n",
"381/388, train_loss: 0.2583, step time: 1.0479\n",
"382/388, train_loss: 0.2844, step time: 1.1368\n",
"383/388, train_loss: 0.4617, step time: 1.1058\n",
"384/388, train_loss: 0.4191, step time: 1.0880\n",
"385/388, train_loss: 0.3851, step time: 1.0979\n",
"386/388, train_loss: 0.9164, step time: 1.1121\n",
"387/388, train_loss: 0.2935, step time: 1.0358\n",
"388/388, train_loss: 0.4455, step time: 1.0448\n",
"epoch 11 average loss: 0.4602\n",
"saved new best metric model\n",
"current epoch: 11 current mean dice: 0.6538 tc: 0.7196 wt: 0.8371 et: 0.4046\n",
"best mean dice: 0.6538 at epoch: 11\n",
"time consuming of epoch 11 is: 965.7621\n",
"----------\n",
"epoch 12/16\n",
"1/388, train_loss: 0.3865, step time: 1.1318\n",
"2/388, train_loss: 0.2256, step time: 1.0524\n",
"3/388, train_loss: 0.2645, step time: 1.0545\n",
"4/388, train_loss: 0.2425, step time: 1.1196\n",
"5/388, train_loss: 0.3473, step time: 1.2446\n",
"6/388, train_loss: 0.3227, step time: 1.0771\n",
"7/388, train_loss: 0.3842, step time: 1.1083\n",
"8/388, train_loss: 0.3847, step time: 1.0735\n",
"9/388, train_loss: 0.2879, step time: 1.0479\n",
"10/388, train_loss: 0.3409, step time: 1.1032\n",
"11/388, train_loss: 0.5048, step time: 1.1515\n",
"12/388, train_loss: 0.3792, step time: 1.1161\n",
"13/388, train_loss: 0.3623, step time: 1.1904\n",
"14/388, train_loss: 0.4816, step time: 1.1593\n",
"15/388, train_loss: 0.3331, step time: 1.0956\n",
"16/388, train_loss: 0.3011, step time: 1.1203\n",
"17/388, train_loss: 0.4116, step time: 1.1142\n",
"18/388, train_loss: 0.3813, step time: 1.1458\n",
"19/388, train_loss: 0.3243, step time: 1.1175\n",
"20/388, train_loss: 0.3186, step time: 1.0472\n",
"21/388, train_loss: 0.8038, step time: 1.0953\n",
"22/388, train_loss: 0.4882, step time: 1.1214\n",
"23/388, train_loss: 0.3494, step time: 1.0415\n",
"24/388, train_loss: 0.4614, step time: 1.1381\n",
"25/388, train_loss: 0.3183, step time: 1.1497\n",
"26/388, train_loss: 0.2783, step time: 1.0966\n",
"27/388, train_loss: 0.2907, step time: 1.1142\n",
"28/388, train_loss: 0.7489, step time: 1.0509\n",
"29/388, train_loss: 0.3507, step time: 1.1019\n",
"30/388, train_loss: 0.3564, step time: 1.1334\n",
"31/388, train_loss: 0.6838, step time: 1.2241\n",
"32/388, train_loss: 0.4403, step time: 1.0476\n",
"33/388, train_loss: 0.4284, step time: 1.0530\n",
"34/388, train_loss: 0.5758, step time: 1.1003\n",
"35/388, train_loss: 0.3956, step time: 1.1001\n",
"36/388, train_loss: 0.5143, step time: 1.0425\n",
"37/388, train_loss: 0.5217, step time: 1.1743\n",
"38/388, train_loss: 0.4944, step time: 1.1943\n",
"39/388, train_loss: 0.2746, step time: 1.1069\n",
"40/388, train_loss: 0.5767, step time: 1.0972\n",
"41/388, train_loss: 0.6910, step time: 1.0959\n",
"42/388, train_loss: 0.3080, step time: 1.0923\n",
"43/388, train_loss: 0.3584, step time: 1.1291\n",
"44/388, train_loss: 0.4767, step time: 1.1061\n",
"45/388, train_loss: 0.5759, step time: 1.1258\n",
"46/388, train_loss: 0.4338, step time: 1.0532\n",
"47/388, train_loss: 0.4319, step time: 1.1062\n",
"48/388, train_loss: 0.5512, step time: 1.0749\n",
"49/388, train_loss: 0.6599, step time: 1.1171\n",
"50/388, train_loss: 0.5467, step time: 1.1761\n",
"51/388, train_loss: 0.5688, step time: 1.0562\n",
"52/388, train_loss: 0.5622, step time: 1.0496\n",
"53/388, train_loss: 0.3394, step time: 1.0409\n",
"54/388, train_loss: 0.5269, step time: 1.1066\n",
"55/388, train_loss: 0.4790, step time: 1.1123\n",
"56/388, train_loss: 0.2441, step time: 1.1157\n",
"57/388, train_loss: 0.3203, step time: 1.0834\n",
"58/388, train_loss: 0.4487, step time: 1.0890\n",
"59/388, train_loss: 0.3419, step time: 1.1097\n",
"60/388, train_loss: 0.6631, step time: 1.1093\n",
"61/388, train_loss: 0.3457, step time: 1.1213\n",
"62/388, train_loss: 0.6752, step time: 1.1312\n",
"63/388, train_loss: 0.3353, step time: 1.1019\n",
"64/388, train_loss: 0.8927, step time: 1.0474\n",
"65/388, train_loss: 0.5132, step time: 1.1024\n",
"66/388, train_loss: 0.4211, step time: 1.1361\n",
"67/388, train_loss: 0.6811, step time: 1.0360\n",
"68/388, train_loss: 0.8862, step time: 1.1814\n",
"69/388, train_loss: 0.2757, step time: 1.1619\n",
"70/388, train_loss: 0.4885, step time: 1.0679\n",
"71/388, train_loss: 0.3925, step time: 1.0984\n",
"72/388, train_loss: 0.3539, step time: 1.1130\n",
"73/388, train_loss: 0.2025, step time: 1.1019\n",
"74/388, train_loss: 0.6811, step time: 1.0577\n",
"75/388, train_loss: 0.3319, step time: 1.0915\n",
"76/388, train_loss: 0.4461, step time: 1.0794\n",
"77/388, train_loss: 0.3067, step time: 1.0689\n",
"78/388, train_loss: 0.4515, step time: 1.1084\n",
"79/388, train_loss: 0.5289, step time: 1.1180\n",
"80/388, train_loss: 0.2888, step time: 1.1055\n",
"81/388, train_loss: 0.3905, step time: 1.1256\n",
"82/388, train_loss: 0.3308, step time: 1.0912\n",
"83/388, train_loss: 0.3936, step time: 1.0469\n",
"84/388, train_loss: 0.4774, step time: 1.1312\n",
"85/388, train_loss: 0.8184, step time: 1.0596\n",
"86/388, train_loss: 0.2917, step time: 1.0893\n",
"87/388, train_loss: 0.3585, step time: 1.1481\n",
"88/388, train_loss: 0.6212, step time: 1.1423\n",
"89/388, train_loss: 0.3471, step time: 1.1919\n",
"90/388, train_loss: 0.5163, step time: 1.1031\n",
"91/388, train_loss: 0.3017, step time: 1.1039\n",
"92/388, train_loss: 0.3144, step time: 1.1249\n",
"93/388, train_loss: 0.3431, step time: 1.1307\n",
"94/388, train_loss: 0.4850, step time: 1.1739\n",
"95/388, train_loss: 0.2551, step time: 1.1764\n",
"96/388, train_loss: 0.2995, step time: 1.1190\n",
"97/388, train_loss: 0.1785, step time: 1.0659\n",
"98/388, train_loss: 0.4614, step time: 1.1293\n",
"99/388, train_loss: 0.4282, step time: 1.1186\n",
"100/388, train_loss: 0.1677, step time: 1.1638\n",
"101/388, train_loss: 0.3721, step time: 1.1586\n",
"102/388, train_loss: 0.3966, step time: 1.1068\n",
"103/388, train_loss: 0.6017, step time: 1.1034\n",
"104/388, train_loss: 0.3943, step time: 1.0718\n",
"105/388, train_loss: 0.5710, step time: 1.1037\n",
"106/388, train_loss: 0.4199, step time: 1.0874\n",
"107/388, train_loss: 0.3098, step time: 1.1917\n",
"108/388, train_loss: 0.2211, step time: 1.1415\n",
"109/388, train_loss: 0.3979, step time: 1.0457\n",
"110/388, train_loss: 0.4624, step time: 1.1071\n",
"111/388, train_loss: 0.1914, step time: 1.1043\n",
"112/388, train_loss: 0.7767, step time: 1.1116\n",
"113/388, train_loss: 0.4310, step time: 1.1748\n",
"114/388, train_loss: 0.2207, step time: 1.0596\n",
"115/388, train_loss: 0.2301, step time: 1.0396\n",
"116/388, train_loss: 0.2899, step time: 1.0465\n",
"117/388, train_loss: 0.2640, step time: 1.1044\n",
"118/388, train_loss: 0.2352, step time: 1.1025\n",
"119/388, train_loss: 0.3354, step time: 1.2132\n",
"120/388, train_loss: 0.7815, step time: 1.0862\n",
"121/388, train_loss: 0.1652, step time: 1.1140\n",
"122/388, train_loss: 0.4781, step time: 1.0508\n",
"123/388, train_loss: 0.2738, step time: 1.0425\n",
"124/388, train_loss: 0.6627, step time: 1.0495\n",
"125/388, train_loss: 0.4205, step time: 1.1595\n",
"126/388, train_loss: 0.6896, step time: 1.1630\n",
"127/388, train_loss: 0.3543, step time: 1.1164\n",
"128/388, train_loss: 0.2382, step time: 1.1186\n",
"129/388, train_loss: 0.3714, step time: 1.0969\n",
"130/388, train_loss: 0.6451, step time: 1.0596\n",
"131/388, train_loss: 0.6184, step time: 1.0809\n",
"132/388, train_loss: 0.3860, step time: 1.1567\n",
"133/388, train_loss: 0.4801, step time: 1.1219\n",
"134/388, train_loss: 0.2310, step time: 1.1015\n",
"135/388, train_loss: 0.3155, step time: 1.1287\n",
"136/388, train_loss: 0.3986, step time: 1.0903\n",
"137/388, train_loss: 0.4467, step time: 1.1060\n",
"138/388, train_loss: 0.7635, step time: 1.1397\n",
"139/388, train_loss: 0.2498, step time: 1.1033\n",
"140/388, train_loss: 0.3663, step time: 1.0590\n",
"141/388, train_loss: 0.3464, step time: 1.1017\n",
"142/388, train_loss: 0.6876, step time: 1.1111\n",
"143/388, train_loss: 0.3987, step time: 1.1001\n",
"144/388, train_loss: 0.2377, step time: 1.0535\n",
"145/388, train_loss: 0.4107, step time: 1.1725\n",
"146/388, train_loss: 0.5587, step time: 1.1189\n",
"147/388, train_loss: 0.5106, step time: 1.1248\n",
"148/388, train_loss: 0.4450, step time: 1.0486\n",
"149/388, train_loss: 0.4219, step time: 1.0631\n",
"150/388, train_loss: 0.2734, step time: 1.1359\n",
"151/388, train_loss: 0.3695, step time: 1.2095\n",
"152/388, train_loss: 0.3035, step time: 1.0527\n",
"153/388, train_loss: 0.3827, step time: 1.0377\n",
"154/388, train_loss: 0.2510, step time: 1.1163\n",
"155/388, train_loss: 0.7291, step time: 1.1528\n",
"156/388, train_loss: 0.4382, step time: 1.0707\n",
"157/388, train_loss: 0.6789, step time: 1.1918\n",
"158/388, train_loss: 0.3901, step time: 1.1572\n",
"159/388, train_loss: 0.2944, step time: 1.0602\n",
"160/388, train_loss: 0.4928, step time: 1.1413\n",
"161/388, train_loss: 0.3146, step time: 1.1074\n",
"162/388, train_loss: 0.3566, step time: 1.0616\n",
"163/388, train_loss: 0.6008, step time: 1.2396\n",
"164/388, train_loss: 0.2813, step time: 1.0501\n",
"165/388, train_loss: 0.1425, step time: 1.1249\n",
"166/388, train_loss: 0.5734, step time: 1.0784\n",
"167/388, train_loss: 0.4335, step time: 1.1711\n",
"168/388, train_loss: 0.5115, step time: 1.0431\n",
"169/388, train_loss: 0.3364, step time: 1.2294\n",
"170/388, train_loss: 0.3698, step time: 1.1342\n",
"171/388, train_loss: 0.2976, step time: 1.1024\n",
"172/388, train_loss: 0.4003, step time: 1.1268\n",
"173/388, train_loss: 0.5487, step time: 1.0651\n",
"174/388, train_loss: 0.3556, step time: 1.1215\n",
"175/388, train_loss: 0.3251, step time: 1.2018\n",
"176/388, train_loss: 0.5668, step time: 1.1286\n",
"177/388, train_loss: 0.4948, step time: 1.1672\n",
"178/388, train_loss: 0.2851, step time: 1.0602\n",
"179/388, train_loss: 0.2773, step time: 1.0553\n",
"180/388, train_loss: 0.3355, step time: 1.0482\n",
"181/388, train_loss: 0.2103, step time: 1.2351\n",
"182/388, train_loss: 0.3825, step time: 1.1472\n",
"183/388, train_loss: 0.2706, step time: 1.0938\n",
"184/388, train_loss: 0.8641, step time: 1.0685\n",
"185/388, train_loss: 0.3673, step time: 1.0962\n",
"186/388, train_loss: 0.3069, step time: 1.0490\n",
"187/388, train_loss: 0.4998, step time: 1.1144\n",
"188/388, train_loss: 0.2936, step time: 1.1650\n",
"189/388, train_loss: 0.2890, step time: 1.1660\n",
"190/388, train_loss: 0.2262, step time: 1.1065\n",
"191/388, train_loss: 0.8274, step time: 1.0411\n",
"192/388, train_loss: 0.3860, step time: 1.1195\n",
"193/388, train_loss: 0.3788, step time: 1.0740\n",
"194/388, train_loss: 0.8112, step time: 1.1509\n",
"195/388, train_loss: 0.5955, step time: 1.1824\n",
"196/388, train_loss: 0.3423, step time: 1.0802\n",
"197/388, train_loss: 0.3315, step time: 1.1008\n",
"198/388, train_loss: 0.4077, step time: 1.0453\n",
"199/388, train_loss: 0.2779, step time: 1.1016\n",
"200/388, train_loss: 0.5976, step time: 1.1325\n",
"201/388, train_loss: 0.2841, step time: 1.1596\n",
"202/388, train_loss: 0.1506, step time: 1.0386\n",
"203/388, train_loss: 0.4484, step time: 1.1076\n",
"204/388, train_loss: 0.5314, step time: 1.1135\n",
"205/388, train_loss: 0.1488, step time: 1.1602\n",
"206/388, train_loss: 0.4992, step time: 1.1457\n",
"207/388, train_loss: 0.1943, step time: 1.1612\n",
"208/388, train_loss: 0.4067, step time: 1.1421\n",
"209/388, train_loss: 0.5492, step time: 1.0394\n",
"210/388, train_loss: 0.4698, step time: 1.1162\n",
"211/388, train_loss: 0.4278, step time: 1.0677\n",
"212/388, train_loss: 0.3460, step time: 1.0503\n",
"213/388, train_loss: 0.4401, step time: 1.1650\n",
"214/388, train_loss: 0.3329, step time: 1.1735\n",
"215/388, train_loss: 0.4439, step time: 1.1589\n",
"216/388, train_loss: 0.2837, step time: 1.0744\n",
"217/388, train_loss: 0.5104, step time: 1.1044\n",
"218/388, train_loss: 0.3391, step time: 1.1241\n",
"219/388, train_loss: 0.3505, step time: 1.1361\n",
"220/388, train_loss: 0.3344, step time: 1.1856\n",
"221/388, train_loss: 0.4892, step time: 1.1710\n",
"222/388, train_loss: 0.1875, step time: 1.0389\n",
"223/388, train_loss: 0.2593, step time: 1.1021\n",
"224/388, train_loss: 0.3293, step time: 1.1013\n",
"225/388, train_loss: 0.3108, step time: 1.1757\n",
"226/388, train_loss: 0.3312, step time: 1.1449\n",
"227/388, train_loss: 0.2999, step time: 1.1418\n",
"228/388, train_loss: 0.3816, step time: 1.0558\n",
"229/388, train_loss: 0.3743, step time: 1.1132\n",
"230/388, train_loss: 0.3898, step time: 1.0534\n",
"231/388, train_loss: 0.4065, step time: 1.1860\n",
"232/388, train_loss: 0.4957, step time: 1.0712\n",
"233/388, train_loss: 0.4344, step time: 1.1079\n",
"234/388, train_loss: 0.5041, step time: 1.1328\n",
"235/388, train_loss: 0.3739, step time: 1.0471\n",
"236/388, train_loss: 0.3321, step time: 1.0557\n",
"237/388, train_loss: 0.3598, step time: 1.0455\n",
"238/388, train_loss: 0.2948, step time: 1.0576\n",
"239/388, train_loss: 0.3402, step time: 1.1123\n",
"240/388, train_loss: 0.4223, step time: 1.1132\n",
"241/388, train_loss: 0.3242, step time: 1.1021\n",
"242/388, train_loss: 0.3757, step time: 1.0804\n",
"243/388, train_loss: 0.3002, step time: 1.0928\n",
"244/388, train_loss: 0.3744, step time: 1.0619\n",
"245/388, train_loss: 0.2071, step time: 1.2242\n",
"246/388, train_loss: 0.3833, step time: 1.1033\n",
"247/388, train_loss: 0.4384, step time: 1.0439\n",
"248/388, train_loss: 0.4470, step time: 1.0727\n",
"249/388, train_loss: 0.3409, step time: 1.1056\n",
"250/388, train_loss: 0.3929, step time: 1.0945\n",
"251/388, train_loss: 0.3284, step time: 1.1598\n",
"252/388, train_loss: 0.4893, step time: 1.1335\n",
"253/388, train_loss: 0.1968, step time: 1.1241\n",
"254/388, train_loss: 0.4021, step time: 1.0608\n",
"255/388, train_loss: 0.3288, step time: 1.0365\n",
"256/388, train_loss: 0.4078, step time: 1.0773\n",
"257/388, train_loss: 0.1404, step time: 1.2034\n",
"258/388, train_loss: 0.2858, step time: 1.1479\n",
"259/388, train_loss: 0.6288, step time: 1.1637\n",
"260/388, train_loss: 0.4602, step time: 1.1093\n",
"261/388, train_loss: 0.4790, step time: 1.0317\n",
"262/388, train_loss: 0.2849, step time: 1.0804\n",
"263/388, train_loss: 0.3383, step time: 1.0498\n",
"264/388, train_loss: 0.9523, step time: 1.1425\n",
"265/388, train_loss: 0.8392, step time: 1.2063\n",
"266/388, train_loss: 0.2155, step time: 1.0530\n",
"267/388, train_loss: 0.4044, step time: 1.1038\n",
"268/388, train_loss: 0.7157, step time: 1.0566\n",
"269/388, train_loss: 0.4723, step time: 1.1903\n",
"270/388, train_loss: 0.8044, step time: 1.1779\n",
"271/388, train_loss: 0.2633, step time: 1.1725\n",
"272/388, train_loss: 0.5994, step time: 1.0738\n",
"273/388, train_loss: 0.3306, step time: 1.0953\n",
"274/388, train_loss: 0.1841, step time: 1.0576\n",
"275/388, train_loss: 0.1164, step time: 1.1167\n",
"276/388, train_loss: 0.2142, step time: 1.1272\n",
"277/388, train_loss: 0.2835, step time: 1.0879\n",
"278/388, train_loss: 0.7890, step time: 1.1406\n",
"279/388, train_loss: 0.2999, step time: 1.0984\n",
"280/388, train_loss: 0.3594, step time: 1.1313\n",
"281/388, train_loss: 0.1685, step time: 1.0447\n",
"282/388, train_loss: 0.5593, step time: 1.1921\n",
"283/388, train_loss: 0.3583, step time: 1.1715\n",
"284/388, train_loss: 0.3341, step time: 1.1248\n",
"285/388, train_loss: 0.5273, step time: 1.0750\n",
"286/388, train_loss: 0.7848, step time: 1.0719\n",
"287/388, train_loss: 0.2983, step time: 1.0348\n",
"288/388, train_loss: 0.4017, step time: 1.1220\n",
"289/388, train_loss: 0.3657, step time: 1.1652\n",
"290/388, train_loss: 0.4372, step time: 1.0644\n",
"291/388, train_loss: 0.2449, step time: 1.1036\n",
"292/388, train_loss: 0.3546, step time: 1.0445\n",
"293/388, train_loss: 0.3705, step time: 1.1032\n",
"294/388, train_loss: 0.2544, step time: 1.1272\n",
"295/388, train_loss: 0.5962, step time: 1.2206\n",
"296/388, train_loss: 0.4861, step time: 1.1370\n",
"297/388, train_loss: 0.2906, step time: 1.1603\n",
"298/388, train_loss: 0.3896, step time: 1.0666\n",
"299/388, train_loss: 0.5669, step time: 1.0501\n",
"300/388, train_loss: 0.2147, step time: 1.0694\n",
"301/388, train_loss: 0.3954, step time: 1.1943\n",
"302/388, train_loss: 0.4412, step time: 1.1367\n",
"303/388, train_loss: 0.6821, step time: 1.1010\n",
"304/388, train_loss: 0.5705, step time: 1.1286\n",
"305/388, train_loss: 0.3249, step time: 1.1070\n",
"306/388, train_loss: 0.3712, step time: 1.0693\n",
"307/388, train_loss: 0.5594, step time: 1.2354\n",
"308/388, train_loss: 0.1838, step time: 1.1232\n",
"309/388, train_loss: 0.6288, step time: 1.1297\n",
"310/388, train_loss: 0.2614, step time: 1.1325\n",
"311/388, train_loss: 0.8486, step time: 1.1039\n",
"312/388, train_loss: 0.8365, step time: 1.0595\n",
"313/388, train_loss: 0.6322, step time: 1.1372\n",
"314/388, train_loss: 0.4602, step time: 1.1675\n",
"315/388, train_loss: 0.5226, step time: 1.1680\n",
"316/388, train_loss: 0.7425, step time: 1.0832\n",
"317/388, train_loss: 0.3947, step time: 1.0902\n",
"318/388, train_loss: 0.2829, step time: 1.1210\n",
"319/388, train_loss: 0.2941, step time: 1.0440\n",
"320/388, train_loss: 0.2943, step time: 1.1265\n",
"321/388, train_loss: 0.5788, step time: 1.1696\n",
"322/388, train_loss: 0.2717, step time: 1.0519\n",
"323/388, train_loss: 0.2204, step time: 1.1041\n",
"324/388, train_loss: 0.4202, step time: 1.1137\n",
"325/388, train_loss: 0.2716, step time: 1.1868\n",
"326/388, train_loss: 0.7275, step time: 1.2147\n",
"327/388, train_loss: 0.3504, step time: 1.1385\n",
"328/388, train_loss: 0.3327, step time: 1.0578\n",
"329/388, train_loss: 0.3686, step time: 1.1638\n",
"330/388, train_loss: 0.3170, step time: 1.1108\n",
"331/388, train_loss: 0.2412, step time: 1.1074\n",
"332/388, train_loss: 0.3975, step time: 1.0715\n",
"333/388, train_loss: 0.3675, step time: 1.1546\n",
"334/388, train_loss: 0.3429, step time: 1.1278\n",
"335/388, train_loss: 0.2259, step time: 1.0982\n",
"336/388, train_loss: 0.4096, step time: 1.0966\n",
"337/388, train_loss: 0.4407, step time: 1.1002\n",
"338/388, train_loss: 0.3553, step time: 1.0551\n",
"339/388, train_loss: 0.2217, step time: 1.1961\n",
"340/388, train_loss: 0.2553, step time: 1.0839\n",
"341/388, train_loss: 0.2556, step time: 1.0449\n",
"342/388, train_loss: 0.3135, step time: 1.1541\n",
"343/388, train_loss: 0.3328, step time: 1.0905\n",
"344/388, train_loss: 0.2980, step time: 1.1111\n",
"345/388, train_loss: 0.4918, step time: 1.2210\n",
"346/388, train_loss: 0.2327, step time: 1.1797\n",
"347/388, train_loss: 0.4639, step time: 1.1412\n",
"348/388, train_loss: 0.1453, step time: 1.0521\n",
"349/388, train_loss: 0.3604, step time: 1.1572\n",
"350/388, train_loss: 0.4654, step time: 1.1407\n",
"351/388, train_loss: 0.3984, step time: 1.2042\n",
"352/388, train_loss: 0.3598, step time: 1.1188\n",
"353/388, train_loss: 0.2578, step time: 1.1619\n",
"354/388, train_loss: 0.2499, step time: 1.0442\n",
"355/388, train_loss: 0.3005, step time: 1.0337\n",
"356/388, train_loss: 0.5364, step time: 1.1314\n",
"357/388, train_loss: 0.5907, step time: 1.2065\n",
"358/388, train_loss: 0.3227, step time: 1.1572\n",
"359/388, train_loss: 0.3610, step time: 1.1385\n",
"360/388, train_loss: 0.1367, step time: 1.0834\n",
"361/388, train_loss: 0.6420, step time: 1.1017\n",
"362/388, train_loss: 0.5668, step time: 1.1136\n",
"363/388, train_loss: 0.2995, step time: 1.0362\n",
"364/388, train_loss: 0.5558, step time: 1.1679\n",
"365/388, train_loss: 0.2057, step time: 1.0846\n",
"366/388, train_loss: 0.3094, step time: 1.0904\n",
"367/388, train_loss: 0.4921, step time: 1.1147\n",
"368/388, train_loss: 0.3386, step time: 1.1390\n",
"369/388, train_loss: 0.2990, step time: 1.0959\n",
"370/388, train_loss: 0.3881, step time: 1.1740\n",
"371/388, train_loss: 0.3848, step time: 1.1778\n",
"372/388, train_loss: 0.3485, step time: 1.0530\n",
"373/388, train_loss: 0.3699, step time: 1.0485\n",
"374/388, train_loss: 0.3222, step time: 1.0877\n",
"375/388, train_loss: 0.5066, step time: 1.0669\n",
"376/388, train_loss: 0.4200, step time: 1.0572\n",
"377/388, train_loss: 0.2605, step time: 1.1435\n",
"378/388, train_loss: 0.3379, step time: 1.1136\n",
"379/388, train_loss: 0.1320, step time: 1.1777\n",
"380/388, train_loss: 0.2924, step time: 1.0450\n",
"381/388, train_loss: 0.5100, step time: 1.1026\n",
"382/388, train_loss: 0.2670, step time: 1.0448\n",
"383/388, train_loss: 0.3470, step time: 1.1737\n",
"384/388, train_loss: 0.3450, step time: 1.0478\n",
"385/388, train_loss: 0.2723, step time: 1.0898\n",
"386/388, train_loss: 0.3619, step time: 1.0439\n",
"387/388, train_loss: 0.5004, step time: 1.0300\n",
"388/388, train_loss: 0.3302, step time: 1.0308\n",
"epoch 12 average loss: 0.4068\n",
"saved new best metric model\n",
"current epoch: 12 current mean dice: 0.6601 tc: 0.7203 wt: 0.8571 et: 0.4028\n",
"best mean dice: 0.6601 at epoch: 12\n",
"time consuming of epoch 12 is: 966.8360\n",
"----------\n",
"epoch 13/16\n",
"1/388, train_loss: 0.2987, step time: 1.1670\n",
"2/388, train_loss: 0.2184, step time: 1.0517\n",
"3/388, train_loss: 0.2518, step time: 1.0940\n",
"4/388, train_loss: 0.2699, step time: 1.0652\n",
"5/388, train_loss: 0.5159, step time: 1.0744\n",
"6/388, train_loss: 0.8346, step time: 1.1391\n",
"7/388, train_loss: 0.3226, step time: 1.1492\n",
"8/388, train_loss: 0.2896, step time: 1.1891\n",
"9/388, train_loss: 0.3939, step time: 1.0684\n",
"10/388, train_loss: 0.2072, step time: 1.0954\n",
"11/388, train_loss: 0.3765, step time: 1.0422\n",
"12/388, train_loss: 0.5166, step time: 1.0811\n",
"13/388, train_loss: 0.4513, step time: 1.1895\n",
"14/388, train_loss: 0.4516, step time: 1.1298\n",
"15/388, train_loss: 0.3251, step time: 1.1093\n",
"16/388, train_loss: 0.6246, step time: 1.1278\n",
"17/388, train_loss: 0.5261, step time: 1.0562\n",
"18/388, train_loss: 0.2423, step time: 1.1344\n",
"19/388, train_loss: 0.5613, step time: 1.2131\n",
"20/388, train_loss: 0.3338, step time: 1.0724\n",
"21/388, train_loss: 0.2293, step time: 1.1158\n",
"22/388, train_loss: 0.3471, step time: 1.1105\n",
"23/388, train_loss: 0.3776, step time: 1.1235\n",
"24/388, train_loss: 0.5999, step time: 1.0884\n",
"25/388, train_loss: 0.4119, step time: 1.0388\n",
"26/388, train_loss: 0.4736, step time: 1.1090\n",
"27/388, train_loss: 0.4792, step time: 1.1112\n",
"28/388, train_loss: 0.1160, step time: 1.1060\n",
"29/388, train_loss: 0.3384, step time: 1.0990\n",
"30/388, train_loss: 0.4496, step time: 1.0585\n",
"31/388, train_loss: 0.3040, step time: 1.0618\n",
"32/388, train_loss: 0.4104, step time: 1.1166\n",
"33/388, train_loss: 0.3450, step time: 1.1796\n",
"34/388, train_loss: 0.3242, step time: 1.0843\n",
"35/388, train_loss: 0.7344, step time: 1.0986\n",
"36/388, train_loss: 0.2315, step time: 1.0986\n",
"37/388, train_loss: 0.2514, step time: 1.1071\n",
"38/388, train_loss: 0.5899, step time: 1.1186\n",
"39/388, train_loss: 0.4785, step time: 1.1581\n",
"40/388, train_loss: 0.2650, step time: 1.0512\n",
"41/388, train_loss: 0.3074, step time: 1.1026\n",
"42/388, train_loss: 0.2853, step time: 1.0855\n",
"43/388, train_loss: 0.5958, step time: 1.0773\n",
"44/388, train_loss: 0.2076, step time: 1.0459\n",
"45/388, train_loss: 0.3766, step time: 1.2249\n",
"46/388, train_loss: 0.5329, step time: 1.0412\n",
"47/388, train_loss: 0.3079, step time: 1.0940\n",
"48/388, train_loss: 0.5787, step time: 1.1039\n",
"49/388, train_loss: 0.3497, step time: 1.1988\n",
"50/388, train_loss: 0.4604, step time: 1.1162\n",
"51/388, train_loss: 0.2727, step time: 1.1605\n",
"52/388, train_loss: 0.1667, step time: 1.1319\n",
"53/388, train_loss: 0.4500, step time: 1.1839\n",
"54/388, train_loss: 0.4770, step time: 1.0864\n",
"55/388, train_loss: 0.3322, step time: 1.1689\n",
"56/388, train_loss: 0.2068, step time: 1.0783\n",
"57/388, train_loss: 0.5041, step time: 1.1792\n",
"58/388, train_loss: 0.5351, step time: 1.0883\n",
"59/388, train_loss: 0.3929, step time: 1.1665\n",
"60/388, train_loss: 0.3350, step time: 1.0735\n",
"61/388, train_loss: 0.2960, step time: 1.1118\n",
"62/388, train_loss: 0.2674, step time: 1.1078\n",
"63/388, train_loss: 0.4870, step time: 1.1972\n",
"64/388, train_loss: 0.3470, step time: 1.0610\n",
"65/388, train_loss: 0.5317, step time: 1.0400\n",
"66/388, train_loss: 0.5954, step time: 1.1193\n",
"67/388, train_loss: 0.3973, step time: 1.1174\n",
"68/388, train_loss: 0.4756, step time: 1.0621\n",
"69/388, train_loss: 0.2584, step time: 1.0468\n",
"70/388, train_loss: 0.3709, step time: 1.1254\n",
"71/388, train_loss: 0.2640, step time: 1.1650\n",
"72/388, train_loss: 0.3092, step time: 1.0683\n",
"73/388, train_loss: 0.3109, step time: 1.0498\n",
"74/388, train_loss: 0.4392, step time: 1.0514\n",
"75/388, train_loss: 0.3532, step time: 1.0879\n",
"76/388, train_loss: 0.3289, step time: 1.0589\n",
"77/388, train_loss: 0.7859, step time: 1.1076\n",
"78/388, train_loss: 0.2715, step time: 1.1234\n",
"79/388, train_loss: 0.4060, step time: 1.1121\n",
"80/388, train_loss: 0.2553, step time: 1.1193\n",
"81/388, train_loss: 0.3482, step time: 1.0410\n",
"82/388, train_loss: 0.8652, step time: 1.1344\n",
"83/388, train_loss: 0.3382, step time: 1.1837\n",
"84/388, train_loss: 0.2491, step time: 1.0664\n",
"85/388, train_loss: 0.2894, step time: 1.1031\n",
"86/388, train_loss: 0.2436, step time: 1.1188\n",
"87/388, train_loss: 0.4139, step time: 1.0531\n",
"88/388, train_loss: 0.6050, step time: 1.1074\n",
"89/388, train_loss: 0.3155, step time: 1.1777\n",
"90/388, train_loss: 0.3158, step time: 1.1119\n",
"91/388, train_loss: 0.4620, step time: 1.1875\n",
"92/388, train_loss: 0.2616, step time: 1.0509\n",
"93/388, train_loss: 0.3276, step time: 1.1266\n",
"94/388, train_loss: 0.2371, step time: 1.0413\n",
"95/388, train_loss: 0.4618, step time: 1.0755\n",
"96/388, train_loss: 0.3682, step time: 1.1304\n",
"97/388, train_loss: 0.2135, step time: 1.2046\n",
"98/388, train_loss: 0.3307, step time: 1.0332\n",
"99/388, train_loss: 0.3575, step time: 1.0424\n",
"100/388, train_loss: 0.2087, step time: 1.1246\n",
"101/388, train_loss: 0.3117, step time: 1.0376\n",
"102/388, train_loss: 0.2952, step time: 1.1192\n",
"103/388, train_loss: 0.3145, step time: 1.1640\n",
"104/388, train_loss: 0.3386, step time: 1.0595\n",
"105/388, train_loss: 0.3580, step time: 1.1073\n",
"106/388, train_loss: 0.2483, step time: 1.0449\n",
"107/388, train_loss: 0.2870, step time: 1.1073\n",
"108/388, train_loss: 0.3694, step time: 1.1317\n",
"109/388, train_loss: 0.3003, step time: 1.1368\n",
"110/388, train_loss: 0.4128, step time: 1.0416\n",
"111/388, train_loss: 0.2322, step time: 1.1249\n",
"112/388, train_loss: 0.6496, step time: 1.1016\n",
"113/388, train_loss: 0.6960, step time: 1.1243\n",
"114/388, train_loss: 0.1899, step time: 1.1251\n",
"115/388, train_loss: 0.3580, step time: 1.1139\n",
"116/388, train_loss: 0.2878, step time: 1.1026\n",
"117/388, train_loss: 0.2762, step time: 1.1024\n",
"118/388, train_loss: 0.5900, step time: 1.0507\n",
"119/388, train_loss: 0.3629, step time: 1.0461\n",
"120/388, train_loss: 0.6920, step time: 1.1160\n",
"121/388, train_loss: 0.2038, step time: 1.1037\n",
"122/388, train_loss: 0.2841, step time: 1.1198\n",
"123/388, train_loss: 0.2092, step time: 1.1504\n",
"124/388, train_loss: 0.3447, step time: 1.0583\n",
"125/388, train_loss: 0.3762, step time: 1.1177\n",
"126/388, train_loss: 0.2116, step time: 1.0481\n",
"127/388, train_loss: 0.2372, step time: 1.1372\n",
"128/388, train_loss: 0.4022, step time: 1.1198\n",
"129/388, train_loss: 0.1295, step time: 1.0972\n",
"130/388, train_loss: 0.6550, step time: 1.1225\n",
"131/388, train_loss: 0.3287, step time: 1.1091\n",
"132/388, train_loss: 0.2838, step time: 1.1259\n",
"133/388, train_loss: 0.2296, step time: 1.0687\n",
"134/388, train_loss: 0.5334, step time: 1.1542\n",
"135/388, train_loss: 0.2726, step time: 1.1043\n",
"136/388, train_loss: 0.1455, step time: 1.1161\n",
"137/388, train_loss: 0.5699, step time: 1.0918\n",
"138/388, train_loss: 0.2626, step time: 1.1136\n",
"139/388, train_loss: 0.7082, step time: 1.0644\n",
"140/388, train_loss: 0.3186, step time: 1.0829\n",
"141/388, train_loss: 0.2949, step time: 1.0961\n",
"142/388, train_loss: 0.5045, step time: 1.1087\n",
"143/388, train_loss: 0.2124, step time: 1.0579\n",
"144/388, train_loss: 0.3561, step time: 1.0499\n",
"145/388, train_loss: 0.3260, step time: 1.1056\n",
"146/388, train_loss: 0.2661, step time: 1.0732\n",
"147/388, train_loss: 0.4072, step time: 1.2043\n",
"148/388, train_loss: 0.3006, step time: 1.1149\n",
"149/388, train_loss: 0.4183, step time: 1.1292\n",
"150/388, train_loss: 0.3948, step time: 1.0654\n",
"151/388, train_loss: 0.2275, step time: 1.0793\n",
"152/388, train_loss: 0.3979, step time: 1.0813\n",
"153/388, train_loss: 0.3673, step time: 1.1950\n",
"154/388, train_loss: 0.5437, step time: 1.1166\n",
"155/388, train_loss: 0.1775, step time: 1.0854\n",
"156/388, train_loss: 0.4129, step time: 1.1219\n",
"157/388, train_loss: 0.2571, step time: 1.0466\n",
"158/388, train_loss: 0.5286, step time: 1.0602\n",
"159/388, train_loss: 0.3868, step time: 1.1544\n",
"160/388, train_loss: 0.7780, step time: 1.1095\n",
"161/388, train_loss: 0.1911, step time: 1.0559\n",
"162/388, train_loss: 0.3472, step time: 1.1170\n",
"163/388, train_loss: 0.3315, step time: 1.1040\n",
"164/388, train_loss: 0.3419, step time: 1.0576\n",
"165/388, train_loss: 0.2376, step time: 1.0465\n",
"166/388, train_loss: 0.5855, step time: 1.1950\n",
"167/388, train_loss: 0.3370, step time: 1.1141\n",
"168/388, train_loss: 0.4004, step time: 1.0395\n",
"169/388, train_loss: 0.3006, step time: 1.1085\n",
"170/388, train_loss: 0.3093, step time: 1.0367\n",
"171/388, train_loss: 0.2298, step time: 1.0602\n",
"172/388, train_loss: 0.1809, step time: 1.1044\n",
"173/388, train_loss: 0.2090, step time: 1.2100\n",
"174/388, train_loss: 0.2131, step time: 1.0495\n",
"175/388, train_loss: 0.2884, step time: 1.0329\n",
"176/388, train_loss: 0.3776, step time: 1.1206\n",
"177/388, train_loss: 0.3398, step time: 1.0658\n",
"178/388, train_loss: 0.2735, step time: 1.0832\n",
"179/388, train_loss: 0.3098, step time: 1.1971\n",
"180/388, train_loss: 0.4473, step time: 1.1411\n",
"181/388, train_loss: 0.3531, step time: 1.0781\n",
"182/388, train_loss: 0.1883, step time: 1.1278\n",
"183/388, train_loss: 0.5426, step time: 1.0715\n",
"184/388, train_loss: 0.2897, step time: 1.0931\n",
"185/388, train_loss: 0.3362, step time: 1.2270\n",
"186/388, train_loss: 0.3521, step time: 1.1140\n",
"187/388, train_loss: 0.7279, step time: 1.0752\n",
"188/388, train_loss: 0.3045, step time: 1.1249\n",
"189/388, train_loss: 0.3438, step time: 1.0466\n",
"190/388, train_loss: 0.1868, step time: 1.1276\n",
"191/388, train_loss: 0.7790, step time: 1.1578\n",
"192/388, train_loss: 0.2585, step time: 1.1182\n",
"193/388, train_loss: 0.5498, step time: 1.0523\n",
"194/388, train_loss: 0.2523, step time: 1.1392\n",
"195/388, train_loss: 0.1671, step time: 1.0373\n",
"196/388, train_loss: 0.3395, step time: 1.1196\n",
"197/388, train_loss: 0.1853, step time: 1.1126\n",
"198/388, train_loss: 0.3424, step time: 1.1183\n",
"199/388, train_loss: 0.5818, step time: 1.1327\n",
"200/388, train_loss: 0.2146, step time: 1.0839\n",
"201/388, train_loss: 0.2654, step time: 1.1238\n",
"202/388, train_loss: 0.6169, step time: 1.0548\n",
"203/388, train_loss: 0.2735, step time: 1.0911\n",
"204/388, train_loss: 0.3179, step time: 1.1558\n",
"205/388, train_loss: 0.3056, step time: 1.1533\n",
"206/388, train_loss: 0.1906, step time: 1.1375\n",
"207/388, train_loss: 0.5672, step time: 1.0782\n",
"208/388, train_loss: 0.5997, step time: 1.0962\n",
"209/388, train_loss: 0.3694, step time: 1.1190\n",
"210/388, train_loss: 0.3023, step time: 1.1425\n",
"211/388, train_loss: 0.3993, step time: 1.1905\n",
"212/388, train_loss: 0.3656, step time: 1.1128\n",
"213/388, train_loss: 0.2884, step time: 1.0757\n",
"214/388, train_loss: 0.3669, step time: 1.1044\n",
"215/388, train_loss: 0.3530, step time: 1.0477\n",
"216/388, train_loss: 0.3035, step time: 1.1036\n",
"217/388, train_loss: 0.2934, step time: 1.1352\n",
"218/388, train_loss: 0.3111, step time: 1.0476\n",
"219/388, train_loss: 0.2332, step time: 1.1030\n",
"220/388, train_loss: 0.3067, step time: 1.0435\n",
"221/388, train_loss: 0.4059, step time: 1.0402\n",
"222/388, train_loss: 0.2260, step time: 1.0750\n",
"223/388, train_loss: 0.2086, step time: 1.1739\n",
"224/388, train_loss: 0.1218, step time: 1.1810\n",
"225/388, train_loss: 0.2602, step time: 1.1034\n",
"226/388, train_loss: 0.3446, step time: 1.0730\n",
"227/388, train_loss: 0.5214, step time: 1.0766\n",
"228/388, train_loss: 0.4177, step time: 1.0971\n",
"229/388, train_loss: 0.3651, step time: 1.1351\n",
"230/388, train_loss: 0.3212, step time: 1.1892\n",
"231/388, train_loss: 0.2997, step time: 1.1018\n",
"232/388, train_loss: 0.1802, step time: 1.1070\n",
"233/388, train_loss: 0.2921, step time: 1.0993\n",
"234/388, train_loss: 0.2090, step time: 1.1247\n",
"235/388, train_loss: 0.3591, step time: 1.1139\n",
"236/388, train_loss: 0.3412, step time: 1.2259\n",
"237/388, train_loss: 0.2778, step time: 1.1194\n",
"238/388, train_loss: 0.5073, step time: 1.1212\n",
"239/388, train_loss: 0.4473, step time: 1.1356\n",
"240/388, train_loss: 0.6545, step time: 1.1453\n",
"241/388, train_loss: 0.3284, step time: 1.1134\n",
"242/388, train_loss: 0.5539, step time: 1.1770\n",
"243/388, train_loss: 0.2674, step time: 1.1587\n",
"244/388, train_loss: 0.2870, step time: 1.1120\n",
"245/388, train_loss: 0.3143, step time: 1.1300\n",
"246/388, train_loss: 0.4141, step time: 1.1014\n",
"247/388, train_loss: 0.2527, step time: 1.1398\n",
"248/388, train_loss: 0.1587, step time: 1.1564\n",
"249/388, train_loss: 0.2868, step time: 1.0388\n",
"250/388, train_loss: 0.2911, step time: 1.0569\n",
"251/388, train_loss: 0.1204, step time: 1.0526\n",
"252/388, train_loss: 0.3673, step time: 1.0984\n",
"253/388, train_loss: 0.3215, step time: 1.1083\n",
"254/388, train_loss: 0.3087, step time: 1.2068\n",
"255/388, train_loss: 0.4124, step time: 1.1557\n",
"256/388, train_loss: 0.1147, step time: 1.0716\n",
"257/388, train_loss: 0.2858, step time: 1.1192\n",
"258/388, train_loss: 0.4462, step time: 1.0976\n",
"259/388, train_loss: 0.5530, step time: 1.0665\n",
"260/388, train_loss: 0.5815, step time: 1.1499\n",
"261/388, train_loss: 0.3396, step time: 1.1343\n",
"262/388, train_loss: 0.3748, step time: 1.1150\n",
"263/388, train_loss: 0.5227, step time: 1.0977\n",
"264/388, train_loss: 0.1444, step time: 1.1106\n",
"265/388, train_loss: 0.2735, step time: 1.0530\n",
"266/388, train_loss: 0.2758, step time: 1.0696\n",
"267/388, train_loss: 0.2126, step time: 1.1831\n",
"268/388, train_loss: 0.2178, step time: 1.0931\n",
"269/388, train_loss: 0.3378, step time: 1.1490\n",
"270/388, train_loss: 0.2745, step time: 1.1141\n",
"271/388, train_loss: 0.2610, step time: 1.1213\n",
"272/388, train_loss: 0.4485, step time: 1.0524\n",
"273/388, train_loss: 0.3842, step time: 1.0645\n",
"274/388, train_loss: 0.2710, step time: 1.1173\n",
"275/388, train_loss: 0.4383, step time: 1.1277\n",
"276/388, train_loss: 0.1999, step time: 1.0957\n",
"277/388, train_loss: 0.5288, step time: 1.1171\n",
"278/388, train_loss: 0.4584, step time: 1.0993\n",
"279/388, train_loss: 0.4103, step time: 1.0447\n",
"280/388, train_loss: 0.3372, step time: 1.1352\n",
"281/388, train_loss: 0.6533, step time: 1.0820\n",
"282/388, train_loss: 0.2384, step time: 1.1583\n",
"283/388, train_loss: 0.3114, step time: 1.0609\n",
"284/388, train_loss: 0.3290, step time: 1.0978\n",
"285/388, train_loss: 0.5142, step time: 1.1282\n",
"286/388, train_loss: 0.3231, step time: 1.2126\n",
"287/388, train_loss: 0.1347, step time: 1.0505\n",
"288/388, train_loss: 0.4307, step time: 1.0673\n",
"289/388, train_loss: 0.2288, step time: 1.1176\n",
"290/388, train_loss: 0.2004, step time: 1.1045\n",
"291/388, train_loss: 0.1465, step time: 1.1024\n",
"292/388, train_loss: 0.2419, step time: 1.2362\n",
"293/388, train_loss: 0.7780, step time: 1.1273\n",
"294/388, train_loss: 0.3701, step time: 1.1822\n",
"295/388, train_loss: 0.3829, step time: 1.1216\n",
"296/388, train_loss: 0.1405, step time: 1.0989\n",
"297/388, train_loss: 0.3633, step time: 1.1039\n",
"298/388, train_loss: 0.3427, step time: 1.1466\n",
"299/388, train_loss: 0.3433, step time: 1.0977\n",
"300/388, train_loss: 0.2540, step time: 1.1064\n",
"301/388, train_loss: 0.1645, step time: 1.1351\n",
"302/388, train_loss: 0.4636, step time: 1.0461\n",
"303/388, train_loss: 0.3419, step time: 1.1009\n",
"304/388, train_loss: 0.3720, step time: 1.1469\n",
"305/388, train_loss: 0.3555, step time: 1.1848\n",
"306/388, train_loss: 0.1955, step time: 1.1572\n",
"307/388, train_loss: 0.3478, step time: 1.0574\n",
"308/388, train_loss: 0.2827, step time: 1.0381\n",
"309/388, train_loss: 0.2918, step time: 1.0578\n",
"310/388, train_loss: 0.3018, step time: 1.0435\n",
"311/388, train_loss: 0.3331, step time: 1.1149\n",
"312/388, train_loss: 0.5201, step time: 1.0981\n",
"313/388, train_loss: 0.2084, step time: 1.0520\n",
"314/388, train_loss: 0.5729, step time: 1.0470\n",
"315/388, train_loss: 0.2175, step time: 1.0501\n",
"316/388, train_loss: 0.2486, step time: 1.0908\n",
"317/388, train_loss: 0.4420, step time: 1.2152\n",
"318/388, train_loss: 0.2308, step time: 1.1750\n",
"319/388, train_loss: 0.1835, step time: 1.1753\n",
"320/388, train_loss: 0.2563, step time: 1.0916\n",
"321/388, train_loss: 0.3236, step time: 1.0772\n",
"322/388, train_loss: 0.4272, step time: 1.0356\n",
"323/388, train_loss: 0.3836, step time: 1.0573\n",
"324/388, train_loss: 0.3740, step time: 1.1696\n",
"325/388, train_loss: 0.3456, step time: 1.0624\n",
"326/388, train_loss: 0.3556, step time: 1.0407\n",
"327/388, train_loss: 0.3239, step time: 1.0655\n",
"328/388, train_loss: 0.4148, step time: 1.1204\n",
"329/388, train_loss: 0.4484, step time: 1.1188\n",
"330/388, train_loss: 0.3126, step time: 1.1597\n",
"331/388, train_loss: 0.2945, step time: 1.0502\n",
"332/388, train_loss: 0.2772, step time: 1.1001\n",
"333/388, train_loss: 0.3193, step time: 1.0483\n",
"334/388, train_loss: 0.6947, step time: 1.0461\n",
"335/388, train_loss: 0.2556, step time: 1.0639\n",
"336/388, train_loss: 0.3152, step time: 1.2192\n",
"337/388, train_loss: 0.2743, step time: 1.1248\n",
"338/388, train_loss: 0.2986, step time: 1.0431\n",
"339/388, train_loss: 0.2918, step time: 1.0528\n",
"340/388, train_loss: 0.2056, step time: 1.1662\n",
"341/388, train_loss: 0.4220, step time: 1.1119\n",
"342/388, train_loss: 0.5868, step time: 1.1666\n",
"343/388, train_loss: 0.4150, step time: 1.0585\n",
"344/388, train_loss: 0.1635, step time: 1.1215\n",
"345/388, train_loss: 0.5813, step time: 1.1006\n",
"346/388, train_loss: 0.3931, step time: 1.0652\n",
"347/388, train_loss: 0.5413, step time: 1.0959\n",
"348/388, train_loss: 0.2825, step time: 1.1426\n",
"349/388, train_loss: 0.2944, step time: 1.0939\n",
"350/388, train_loss: 0.2423, step time: 1.1160\n",
"351/388, train_loss: 0.7774, step time: 1.0618\n",
"352/388, train_loss: 0.5184, step time: 1.1279\n",
"353/388, train_loss: 0.3142, step time: 1.0766\n",
"354/388, train_loss: 0.1294, step time: 1.1009\n",
"355/388, train_loss: 0.2589, step time: 1.1647\n",
"356/388, train_loss: 0.2788, step time: 1.0916\n",
"357/388, train_loss: 0.3641, step time: 1.1442\n",
"358/388, train_loss: 0.1651, step time: 1.0638\n",
"359/388, train_loss: 0.3503, step time: 1.1128\n",
"360/388, train_loss: 0.3735, step time: 1.1226\n",
"361/388, train_loss: 0.1002, step time: 1.0762\n",
"362/388, train_loss: 0.5051, step time: 1.1195\n",
"363/388, train_loss: 0.2539, step time: 1.0786\n",
"364/388, train_loss: 0.3405, step time: 1.0794\n",
"365/388, train_loss: 0.8774, step time: 1.1240\n",
"366/388, train_loss: 0.1365, step time: 1.0393\n",
"367/388, train_loss: 0.1982, step time: 1.1274\n",
"368/388, train_loss: 0.3280, step time: 1.1681\n",
"369/388, train_loss: 0.8327, step time: 1.1070\n",
"370/388, train_loss: 0.4089, step time: 1.0753\n",
"371/388, train_loss: 0.3434, step time: 1.1678\n",
"372/388, train_loss: 0.5857, step time: 1.0396\n",
"373/388, train_loss: 0.2941, step time: 1.0488\n",
"374/388, train_loss: 0.2722, step time: 1.1750\n",
"375/388, train_loss: 0.5932, step time: 1.0469\n",
"376/388, train_loss: 0.6413, step time: 1.1043\n",
"377/388, train_loss: 0.2976, step time: 1.0713\n",
"378/388, train_loss: 0.7912, step time: 1.1059\n",
"379/388, train_loss: 0.3409, step time: 1.1360\n",
"380/388, train_loss: 0.2519, step time: 1.1966\n",
"381/388, train_loss: 0.7088, step time: 1.1256\n",
"382/388, train_loss: 0.3049, step time: 1.1007\n",
"383/388, train_loss: 0.2204, step time: 1.0560\n",
"384/388, train_loss: 0.2634, step time: 1.0507\n",
"385/388, train_loss: 0.3248, step time: 1.0599\n",
"386/388, train_loss: 0.4312, step time: 1.1909\n",
"387/388, train_loss: 0.2766, step time: 1.0701\n",
"388/388, train_loss: 0.4499, step time: 1.0222\n",
"epoch 13 average loss: 0.3602\n",
"current epoch: 13 current mean dice: 0.6388 tc: 0.6731 wt: 0.8506 et: 0.3927\n",
"best mean dice: 0.6601 at epoch: 12\n",
"time consuming of epoch 13 is: 963.4692\n",
"----------\n",
"epoch 14/16\n",
"1/388, train_loss: 0.3425, step time: 1.1160\n",
"2/388, train_loss: 0.3442, step time: 1.1315\n",
"3/388, train_loss: 0.8922, step time: 1.2318\n",
"4/388, train_loss: 0.2503, step time: 1.0705\n",
"5/388, train_loss: 0.6771, step time: 1.1149\n",
"6/388, train_loss: 0.4669, step time: 1.0496\n",
"7/388, train_loss: 0.5085, step time: 1.0801\n",
"8/388, train_loss: 0.1353, step time: 1.0614\n",
"9/388, train_loss: 0.5716, step time: 1.2950\n",
"10/388, train_loss: 0.2558, step time: 1.1058\n",
"11/388, train_loss: 0.4533, step time: 1.0353\n",
"12/388, train_loss: 0.2819, step time: 1.1247\n",
"13/388, train_loss: 0.5279, step time: 1.1019\n",
"14/388, train_loss: 0.6867, step time: 1.0512\n",
"15/388, train_loss: 0.3029, step time: 1.1140\n",
"16/388, train_loss: 0.2521, step time: 1.1473\n",
"17/388, train_loss: 0.3139, step time: 1.1086\n",
"18/388, train_loss: 0.6787, step time: 1.1335\n",
"19/388, train_loss: 0.2871, step time: 1.0979\n",
"20/388, train_loss: 0.5909, step time: 1.0589\n",
"21/388, train_loss: 0.8235, step time: 1.1047\n",
"22/388, train_loss: 0.4269, step time: 1.1274\n",
"23/388, train_loss: 0.2384, step time: 1.1422\n",
"24/388, train_loss: 0.3044, step time: 1.0820\n",
"25/388, train_loss: 0.2375, step time: 1.0992\n",
"26/388, train_loss: 0.8531, step time: 1.1212\n",
"27/388, train_loss: 0.8209, step time: 1.0382\n",
"28/388, train_loss: 0.5055, step time: 1.1292\n",
"29/388, train_loss: 0.1543, step time: 1.0867\n",
"30/388, train_loss: 0.3578, step time: 1.0485\n",
"31/388, train_loss: 0.1686, step time: 1.0569\n",
"32/388, train_loss: 0.6005, step time: 1.0856\n",
"33/388, train_loss: 0.3782, step time: 1.1680\n",
"34/388, train_loss: 0.1839, step time: 1.1262\n",
"35/388, train_loss: 0.1420, step time: 1.1702\n",
"36/388, train_loss: 0.3014, step time: 1.1207\n",
"37/388, train_loss: 0.3666, step time: 1.1021\n",
"38/388, train_loss: 0.2854, step time: 1.1048\n",
"39/388, train_loss: 0.4093, step time: 1.0605\n",
"40/388, train_loss: 0.3126, step time: 1.1203\n",
"41/388, train_loss: 0.2046, step time: 1.2450\n",
"42/388, train_loss: 0.2658, step time: 1.1220\n",
"43/388, train_loss: 0.2696, step time: 1.1024\n",
"44/388, train_loss: 0.3828, step time: 1.0654\n",
"45/388, train_loss: 0.3838, step time: 1.1021\n",
"46/388, train_loss: 0.2407, step time: 1.0946\n",
"47/388, train_loss: 0.5572, step time: 1.2592\n",
"48/388, train_loss: 0.3615, step time: 1.1305\n",
"49/388, train_loss: 0.4018, step time: 1.1850\n",
"50/388, train_loss: 0.3114, step time: 1.0443\n",
"51/388, train_loss: 0.4389, step time: 1.0498\n",
"52/388, train_loss: 0.1691, step time: 1.0484\n",
"53/388, train_loss: 0.5440, step time: 1.1042\n",
"54/388, train_loss: 0.1386, step time: 1.1508\n",
"55/388, train_loss: 0.2944, step time: 1.1836\n",
"56/388, train_loss: 0.2400, step time: 1.0826\n",
"57/388, train_loss: 0.4167, step time: 1.0525\n",
"58/388, train_loss: 0.3112, step time: 1.1235\n",
"59/388, train_loss: 0.1303, step time: 1.2426\n",
"60/388, train_loss: 0.2731, step time: 1.1121\n",
"61/388, train_loss: 0.5290, step time: 1.1038\n",
"62/388, train_loss: 0.1515, step time: 1.0438\n",
"63/388, train_loss: 0.2758, step time: 1.1044\n",
"64/388, train_loss: 0.3589, step time: 1.0815\n",
"65/388, train_loss: 0.2672, step time: 1.2318\n",
"66/388, train_loss: 0.2646, step time: 1.1153\n",
"67/388, train_loss: 0.4543, step time: 1.1389\n",
"68/388, train_loss: 0.4134, step time: 1.1306\n",
"69/388, train_loss: 0.3172, step time: 1.1416\n",
"70/388, train_loss: 0.3565, step time: 1.0548\n",
"71/388, train_loss: 0.3705, step time: 1.0729\n",
"72/388, train_loss: 0.4421, step time: 1.1386\n",
"73/388, train_loss: 0.4346, step time: 1.1455\n",
"74/388, train_loss: 0.2790, step time: 1.1132\n",
"75/388, train_loss: 0.3666, step time: 1.0873\n",
"76/388, train_loss: 0.2926, step time: 1.0568\n",
"77/388, train_loss: 0.3816, step time: 1.0405\n",
"78/388, train_loss: 0.2001, step time: 1.1267\n",
"79/388, train_loss: 0.2772, step time: 1.2123\n",
"80/388, train_loss: 0.4776, step time: 1.1215\n",
"81/388, train_loss: 0.1801, step time: 1.0684\n",
"82/388, train_loss: 0.2979, step time: 1.0939\n",
"83/388, train_loss: 0.3927, step time: 1.1667\n",
"84/388, train_loss: 0.2313, step time: 1.0981\n",
"85/388, train_loss: 0.3090, step time: 1.0924\n",
"86/388, train_loss: 0.5941, step time: 1.0643\n",
"87/388, train_loss: 0.4243, step time: 1.1006\n",
"88/388, train_loss: 0.3323, step time: 1.0500\n",
"89/388, train_loss: 0.1653, step time: 1.1015\n",
"90/388, train_loss: 0.3038, step time: 1.0508\n",
"91/388, train_loss: 0.4243, step time: 1.1770\n",
"92/388, train_loss: 0.7680, step time: 1.0734\n",
"93/388, train_loss: 0.2792, step time: 1.0538\n",
"94/388, train_loss: 0.2092, step time: 1.1000\n",
"95/388, train_loss: 0.4203, step time: 1.1452\n",
"96/388, train_loss: 0.2611, step time: 1.0434\n",
"97/388, train_loss: 0.3045, step time: 1.1965\n",
"98/388, train_loss: 0.2239, step time: 1.1253\n",
"99/388, train_loss: 0.3391, step time: 1.1175\n",
"100/388, train_loss: 0.2717, step time: 1.0861\n",
"101/388, train_loss: 0.6955, step time: 1.0384\n",
"102/388, train_loss: 0.3592, step time: 1.1302\n",
"103/388, train_loss: 0.2799, step time: 1.2228\n",
"104/388, train_loss: 0.5206, step time: 1.1060\n",
"105/388, train_loss: 0.2675, step time: 1.1673\n",
"106/388, train_loss: 0.1599, step time: 1.0745\n",
"107/388, train_loss: 0.2145, step time: 1.0406\n",
"108/388, train_loss: 0.2305, step time: 1.0548\n",
"109/388, train_loss: 0.7060, step time: 1.1995\n",
"110/388, train_loss: 0.2084, step time: 1.1520\n",
"111/388, train_loss: 0.2888, step time: 1.1078\n",
"112/388, train_loss: 0.2572, step time: 1.1303\n",
"113/388, train_loss: 0.2122, step time: 1.0589\n",
"114/388, train_loss: 0.4017, step time: 1.1318\n",
"115/388, train_loss: 0.2063, step time: 1.1921\n",
"116/388, train_loss: 0.3147, step time: 1.1252\n",
"117/388, train_loss: 0.2671, step time: 1.1265\n",
"118/388, train_loss: 0.3145, step time: 1.0512\n",
"119/388, train_loss: 0.1849, step time: 1.1047\n",
"120/388, train_loss: 0.2270, step time: 1.1015\n",
"121/388, train_loss: 0.3493, step time: 1.1770\n",
"122/388, train_loss: 0.2492, step time: 1.1385\n",
"123/388, train_loss: 0.4237, step time: 1.1023\n",
"124/388, train_loss: 0.3112, step time: 1.0992\n",
"125/388, train_loss: 0.1444, step time: 1.0980\n",
"126/388, train_loss: 0.1263, step time: 1.1288\n",
"127/388, train_loss: 0.3574, step time: 1.0496\n",
"128/388, train_loss: 0.3724, step time: 1.1368\n",
"129/388, train_loss: 0.3561, step time: 1.1569\n",
"130/388, train_loss: 0.4456, step time: 1.0716\n",
"131/388, train_loss: 0.4589, step time: 1.0388\n",
"132/388, train_loss: 0.1970, step time: 1.0787\n",
"133/388, train_loss: 0.4822, step time: 1.1449\n",
"134/388, train_loss: 0.3187, step time: 1.0803\n",
"135/388, train_loss: 0.5609, step time: 1.1859\n",
"136/388, train_loss: 0.6172, step time: 1.0492\n",
"137/388, train_loss: 0.3915, step time: 1.0907\n",
"138/388, train_loss: 0.3724, step time: 1.0808\n",
"139/388, train_loss: 0.2885, step time: 1.0502\n",
"140/388, train_loss: 0.3641, step time: 1.1426\n",
"141/388, train_loss: 0.2047, step time: 1.1785\n",
"142/388, train_loss: 0.2667, step time: 1.1597\n",
"143/388, train_loss: 0.1434, step time: 1.1155\n",
"144/388, train_loss: 0.1940, step time: 1.1204\n",
"145/388, train_loss: 0.3425, step time: 1.1788\n",
"146/388, train_loss: 0.4484, step time: 1.0538\n",
"147/388, train_loss: 0.3281, step time: 1.2216\n",
"148/388, train_loss: 0.3284, step time: 1.1121\n",
"149/388, train_loss: 0.2726, step time: 1.0974\n",
"150/388, train_loss: 0.2612, step time: 1.1257\n",
"151/388, train_loss: 0.3213, step time: 1.1662\n",
"152/388, train_loss: 0.3098, step time: 1.0767\n",
"153/388, train_loss: 0.5024, step time: 1.2253\n",
"154/388, train_loss: 0.3985, step time: 1.1760\n",
"155/388, train_loss: 0.2369, step time: 1.1165\n",
"156/388, train_loss: 0.3510, step time: 1.0775\n",
"157/388, train_loss: 0.1398, step time: 1.0675\n",
"158/388, train_loss: 0.3483, step time: 1.0475\n",
"159/388, train_loss: 0.2973, step time: 1.1064\n",
"160/388, train_loss: 0.2598, step time: 1.1259\n",
"161/388, train_loss: 0.2311, step time: 1.1488\n",
"162/388, train_loss: 0.3124, step time: 1.0528\n",
"163/388, train_loss: 0.1879, step time: 1.1344\n",
"164/388, train_loss: 0.3179, step time: 1.0848\n",
"165/388, train_loss: 0.2617, step time: 1.0980\n",
"166/388, train_loss: 0.2300, step time: 1.0709\n",
"167/388, train_loss: 0.1463, step time: 1.1873\n",
"168/388, train_loss: 0.2934, step time: 1.0649\n",
"169/388, train_loss: 0.3093, step time: 1.1198\n",
"170/388, train_loss: 0.2768, step time: 1.0932\n",
"171/388, train_loss: 0.2907, step time: 1.0964\n",
"172/388, train_loss: 0.2774, step time: 1.1274\n",
"173/388, train_loss: 0.1674, step time: 1.1683\n",
"174/388, train_loss: 0.4260, step time: 1.1305\n",
"175/388, train_loss: 0.5606, step time: 1.0985\n",
"176/388, train_loss: 0.2361, step time: 1.0628\n",
"177/388, train_loss: 0.3015, step time: 1.0863\n",
"178/388, train_loss: 0.3241, step time: 1.0333\n",
"179/388, train_loss: 0.5375, step time: 1.1439\n",
"180/388, train_loss: 0.2512, step time: 1.1528\n",
"181/388, train_loss: 0.2269, step time: 1.0987\n",
"182/388, train_loss: 0.1103, step time: 1.0812\n",
"183/388, train_loss: 0.6180, step time: 1.0516\n",
"184/388, train_loss: 0.1417, step time: 1.1147\n",
"185/388, train_loss: 0.1725, step time: 1.0436\n",
"186/388, train_loss: 0.1331, step time: 1.1305\n",
"187/388, train_loss: 0.2623, step time: 1.1306\n",
"188/388, train_loss: 0.3068, step time: 1.0832\n",
"189/388, train_loss: 0.3691, step time: 1.0473\n",
"190/388, train_loss: 0.2176, step time: 1.1168\n",
"191/388, train_loss: 0.5122, step time: 1.1036\n",
"192/388, train_loss: 0.1856, step time: 1.0877\n",
"193/388, train_loss: 0.2348, step time: 1.1427\n",
"194/388, train_loss: 0.2410, step time: 1.1220\n",
"195/388, train_loss: 0.2494, step time: 1.1093\n",
"196/388, train_loss: 0.4567, step time: 1.0475\n",
"197/388, train_loss: 0.2013, step time: 1.1528\n",
"198/388, train_loss: 0.2202, step time: 1.0533\n",
"199/388, train_loss: 0.2794, step time: 1.1548\n",
"200/388, train_loss: 0.2615, step time: 1.0502\n",
"201/388, train_loss: 0.2256, step time: 1.0503\n",
"202/388, train_loss: 0.5608, step time: 1.1002\n",
"203/388, train_loss: 0.3446, step time: 1.0985\n",
"204/388, train_loss: 0.2485, step time: 1.0503\n",
"205/388, train_loss: 0.2568, step time: 1.1981\n",
"206/388, train_loss: 0.1550, step time: 1.0607\n",
"207/388, train_loss: 0.2171, step time: 1.1024\n",
"208/388, train_loss: 0.5635, step time: 1.0635\n",
"209/388, train_loss: 0.1582, step time: 1.0752\n",
"210/388, train_loss: 0.2297, step time: 1.0535\n",
"211/388, train_loss: 0.3464, step time: 1.2778\n",
"212/388, train_loss: 0.5561, step time: 1.1078\n",
"213/388, train_loss: 0.2179, step time: 1.0850\n",
"214/388, train_loss: 0.1411, step time: 1.1193\n",
"215/388, train_loss: 0.2764, step time: 1.1136\n",
"216/388, train_loss: 0.1864, step time: 1.1322\n",
"217/388, train_loss: 0.1661, step time: 1.1291\n",
"218/388, train_loss: 0.1652, step time: 1.1491\n",
"219/388, train_loss: 0.2701, step time: 1.1813\n",
"220/388, train_loss: 0.3621, step time: 1.0506\n",
"221/388, train_loss: 0.2043, step time: 1.1249\n",
"222/388, train_loss: 0.4418, step time: 1.1307\n",
"223/388, train_loss: 0.2923, step time: 1.1242\n",
"224/388, train_loss: 0.3714, step time: 1.1548\n",
"225/388, train_loss: 0.3884, step time: 1.1507\n",
"226/388, train_loss: 0.1992, step time: 1.1226\n",
"227/388, train_loss: 0.2854, step time: 1.1056\n",
"228/388, train_loss: 0.3618, step time: 1.1293\n",
"229/388, train_loss: 0.2559, step time: 1.1076\n",
"230/388, train_loss: 0.1892, step time: 1.1078\n",
"231/388, train_loss: 0.7057, step time: 1.2382\n",
"232/388, train_loss: 0.4714, step time: 1.0437\n",
"233/388, train_loss: 0.2991, step time: 1.0410\n",
"234/388, train_loss: 0.2086, step time: 1.0431\n",
"235/388, train_loss: 0.1515, step time: 1.1234\n",
"236/388, train_loss: 0.2130, step time: 1.1317\n",
"237/388, train_loss: 0.3462, step time: 1.1184\n",
"238/388, train_loss: 0.2226, step time: 1.0947\n",
"239/388, train_loss: 0.4891, step time: 1.1646\n",
"240/388, train_loss: 0.4335, step time: 1.0740\n",
"241/388, train_loss: 0.3080, step time: 1.0401\n",
"242/388, train_loss: 0.1633, step time: 1.1057\n",
"243/388, train_loss: 0.2311, step time: 1.1649\n",
"244/388, train_loss: 0.2235, step time: 1.1366\n",
"245/388, train_loss: 0.4500, step time: 1.1775\n",
"246/388, train_loss: 0.2979, step time: 1.1119\n",
"247/388, train_loss: 0.6698, step time: 1.0514\n",
"248/388, train_loss: 0.2363, step time: 1.0644\n",
"249/388, train_loss: 0.1941, step time: 1.1104\n",
"250/388, train_loss: 0.2635, step time: 1.1311\n",
"251/388, train_loss: 0.4494, step time: 1.1321\n",
"252/388, train_loss: 0.2557, step time: 1.1224\n",
"253/388, train_loss: 0.2834, step time: 1.0560\n",
"254/388, train_loss: 0.2068, step time: 1.1049\n",
"255/388, train_loss: 0.5536, step time: 1.0525\n",
"256/388, train_loss: 0.2240, step time: 1.1622\n",
"257/388, train_loss: 0.2163, step time: 1.1313\n",
"258/388, train_loss: 0.4969, step time: 1.1026\n",
"259/388, train_loss: 0.2976, step time: 1.1043\n",
"260/388, train_loss: 0.2857, step time: 1.1327\n",
"261/388, train_loss: 0.3738, step time: 1.0530\n",
"262/388, train_loss: 0.2507, step time: 1.0519\n",
"263/388, train_loss: 0.3175, step time: 1.1787\n",
"264/388, train_loss: 0.2810, step time: 1.1241\n",
"265/388, train_loss: 0.2580, step time: 1.1078\n",
"266/388, train_loss: 0.2766, step time: 1.1008\n",
"267/388, train_loss: 0.3393, step time: 1.0393\n",
"268/388, train_loss: 0.1183, step time: 1.1359\n",
"269/388, train_loss: 0.3590, step time: 1.1971\n",
"270/388, train_loss: 0.3192, step time: 1.0471\n",
"271/388, train_loss: 0.2483, step time: 1.1778\n",
"272/388, train_loss: 0.4482, step time: 1.0623\n",
"273/388, train_loss: 0.5448, step time: 1.0403\n",
"274/388, train_loss: 0.3067, step time: 1.1258\n",
"275/388, train_loss: 0.2563, step time: 1.1665\n",
"276/388, train_loss: 0.7538, step time: 1.1560\n",
"277/388, train_loss: 0.2512, step time: 1.0983\n",
"278/388, train_loss: 0.4857, step time: 1.0773\n",
"279/388, train_loss: 0.4324, step time: 1.0640\n",
"280/388, train_loss: 0.3687, step time: 1.1113\n",
"281/388, train_loss: 0.2551, step time: 1.1066\n",
"282/388, train_loss: 0.4401, step time: 1.1356\n",
"283/388, train_loss: 0.6918, step time: 1.0565\n",
"284/388, train_loss: 0.4044, step time: 1.0489\n",
"285/388, train_loss: 0.2067, step time: 1.1026\n",
"286/388, train_loss: 0.4757, step time: 1.1099\n",
"287/388, train_loss: 0.3258, step time: 1.0996\n",
"288/388, train_loss: 0.7272, step time: 1.1722\n",
"289/388, train_loss: 0.2368, step time: 1.0447\n",
"290/388, train_loss: 0.4522, step time: 1.0815\n",
"291/388, train_loss: 0.5085, step time: 1.0855\n",
"292/388, train_loss: 0.5658, step time: 1.0416\n",
"293/388, train_loss: 0.2305, step time: 1.1204\n",
"294/388, train_loss: 0.2645, step time: 1.1297\n",
"295/388, train_loss: 0.7248, step time: 1.1261\n",
"296/388, train_loss: 0.2550, step time: 1.0842\n",
"297/388, train_loss: 0.3634, step time: 1.1099\n",
"298/388, train_loss: 0.3201, step time: 1.1028\n",
"299/388, train_loss: 0.3915, step time: 1.1051\n",
"300/388, train_loss: 0.2737, step time: 1.1263\n",
"301/388, train_loss: 0.2064, step time: 1.1189\n",
"302/388, train_loss: 0.1285, step time: 1.0938\n",
"303/388, train_loss: 0.1655, step time: 1.1158\n",
"304/388, train_loss: 0.1887, step time: 1.0429\n",
"305/388, train_loss: 0.3452, step time: 1.0655\n",
"306/388, train_loss: 0.2187, step time: 1.1311\n",
"307/388, train_loss: 0.2985, step time: 1.1636\n",
"308/388, train_loss: 0.2149, step time: 1.1915\n",
"309/388, train_loss: 0.4690, step time: 1.0512\n",
"310/388, train_loss: 0.5348, step time: 1.0491\n",
"311/388, train_loss: 0.2439, step time: 1.1345\n",
"312/388, train_loss: 0.2290, step time: 1.0485\n",
"313/388, train_loss: 0.5242, step time: 1.1149\n",
"314/388, train_loss: 0.2113, step time: 1.1937\n",
"315/388, train_loss: 0.1368, step time: 1.0561\n",
"316/388, train_loss: 0.3476, step time: 1.1126\n",
"317/388, train_loss: 0.1964, step time: 1.0674\n",
"318/388, train_loss: 0.1462, step time: 1.0722\n",
"319/388, train_loss: 0.2476, step time: 1.1584\n",
"320/388, train_loss: 0.3309, step time: 1.1661\n",
"321/388, train_loss: 0.5176, step time: 1.0939\n",
"322/388, train_loss: 0.2688, step time: 1.0426\n",
"323/388, train_loss: 0.1586, step time: 1.0678\n",
"324/388, train_loss: 0.2289, step time: 1.1182\n",
"325/388, train_loss: 0.3895, step time: 1.1409\n",
"326/388, train_loss: 0.3409, step time: 1.1095\n",
"327/388, train_loss: 0.1897, step time: 1.1183\n",
"328/388, train_loss: 0.1886, step time: 1.1109\n",
"329/388, train_loss: 0.2742, step time: 1.0784\n",
"330/388, train_loss: 0.2836, step time: 1.1026\n",
"331/388, train_loss: 0.3627, step time: 1.1172\n",
"332/388, train_loss: 0.3462, step time: 1.1396\n",
"333/388, train_loss: 0.3272, step time: 1.0466\n",
"334/388, train_loss: 0.2436, step time: 1.1076\n",
"335/388, train_loss: 0.1676, step time: 1.1393\n",
"336/388, train_loss: 0.2566, step time: 1.0498\n",
"337/388, train_loss: 0.2378, step time: 1.0517\n",
"338/388, train_loss: 0.1710, step time: 1.1673\n",
"339/388, train_loss: 0.2004, step time: 1.1168\n",
"340/388, train_loss: 0.1978, step time: 1.1036\n",
"341/388, train_loss: 0.2475, step time: 1.1316\n",
"342/388, train_loss: 0.1586, step time: 1.0619\n",
"343/388, train_loss: 0.7081, step time: 1.1270\n",
"344/388, train_loss: 0.1909, step time: 1.2239\n",
"345/388, train_loss: 0.2904, step time: 1.1318\n",
"346/388, train_loss: 0.2613, step time: 1.1260\n",
"347/388, train_loss: 0.2958, step time: 1.1485\n",
"348/388, train_loss: 0.1734, step time: 1.2035\n",
"349/388, train_loss: 0.3254, step time: 1.0896\n",
"350/388, train_loss: 0.2832, step time: 1.2512\n",
"351/388, train_loss: 0.2252, step time: 1.1284\n",
"352/388, train_loss: 0.3326, step time: 1.1267\n",
"353/388, train_loss: 0.2821, step time: 1.0644\n",
"354/388, train_loss: 0.2821, step time: 1.0560\n",
"355/388, train_loss: 0.1982, step time: 1.1145\n",
"356/388, train_loss: 0.6350, step time: 1.2127\n",
"357/388, train_loss: 0.5654, step time: 1.1031\n",
"358/388, train_loss: 0.2616, step time: 1.1598\n",
"359/388, train_loss: 0.2042, step time: 1.1101\n",
"360/388, train_loss: 0.4978, step time: 1.1726\n",
"361/388, train_loss: 0.1040, step time: 1.0465\n",
"362/388, train_loss: 0.2046, step time: 1.2484\n",
"363/388, train_loss: 0.3934, step time: 1.1579\n",
"364/388, train_loss: 0.2028, step time: 1.0913\n",
"365/388, train_loss: 0.3656, step time: 1.0458\n",
"366/388, train_loss: 0.1786, step time: 1.1108\n",
"367/388, train_loss: 0.3107, step time: 1.1126\n",
"368/388, train_loss: 0.3623, step time: 1.2206\n",
"369/388, train_loss: 0.2612, step time: 1.1270\n",
"370/388, train_loss: 0.4022, step time: 1.1133\n",
"371/388, train_loss: 0.2076, step time: 1.0585\n",
"372/388, train_loss: 0.2696, step time: 1.0984\n",
"373/388, train_loss: 0.3518, step time: 1.1299\n",
"374/388, train_loss: 0.3542, step time: 1.1844\n",
"375/388, train_loss: 0.4332, step time: 1.1704\n",
"376/388, train_loss: 0.1698, step time: 1.1009\n",
"377/388, train_loss: 0.4642, step time: 1.1520\n",
"378/388, train_loss: 0.5366, step time: 1.1111\n",
"379/388, train_loss: 0.3011, step time: 1.1117\n",
"380/388, train_loss: 0.4542, step time: 1.1084\n",
"381/388, train_loss: 0.2265, step time: 1.1340\n",
"382/388, train_loss: 0.2390, step time: 1.2104\n",
"383/388, train_loss: 0.4655, step time: 1.0777\n",
"384/388, train_loss: 0.6325, step time: 1.0400\n",
"385/388, train_loss: 0.2656, step time: 1.0464\n",
"386/388, train_loss: 0.3066, step time: 1.0326\n",
"387/388, train_loss: 0.3727, step time: 1.0369\n",
"388/388, train_loss: 0.3981, step time: 1.0311\n",
"epoch 14 average loss: 0.3285\n",
"saved new best metric model\n",
"current epoch: 14 current mean dice: 0.6800 tc: 0.7203 wt: 0.8625 et: 0.4573\n",
"best mean dice: 0.6800 at epoch: 14\n",
"time consuming of epoch 14 is: 960.6520\n",
"----------\n",
"epoch 15/16\n",
"1/388, train_loss: 0.3106, step time: 1.1663\n",
"2/388, train_loss: 0.3389, step time: 1.0792\n",
"3/388, train_loss: 0.2033, step time: 1.0918\n",
"4/388, train_loss: 0.1913, step time: 1.0966\n",
"5/388, train_loss: 0.4834, step time: 1.1806\n",
"6/388, train_loss: 0.2882, step time: 1.1242\n",
"7/388, train_loss: 0.4913, step time: 1.1130\n",
"8/388, train_loss: 0.2014, step time: 1.1257\n",
"9/388, train_loss: 0.2205, step time: 1.0788\n",
"10/388, train_loss: 0.3305, step time: 1.1125\n",
"11/388, train_loss: 0.5203, step time: 1.0597\n",
"12/388, train_loss: 0.1511, step time: 1.1771\n",
"13/388, train_loss: 0.2736, step time: 1.1480\n",
"14/388, train_loss: 0.2176, step time: 1.1343\n",
"15/388, train_loss: 0.4451, step time: 1.0396\n",
"16/388, train_loss: 0.2396, step time: 1.0936\n",
"17/388, train_loss: 0.3675, step time: 1.0401\n",
"18/388, train_loss: 0.3613, step time: 1.0639\n",
"19/388, train_loss: 0.2625, step time: 1.2021\n",
"20/388, train_loss: 0.2303, step time: 1.0366\n",
"21/388, train_loss: 0.3865, step time: 1.0669\n",
"22/388, train_loss: 0.5283, step time: 1.0394\n",
"23/388, train_loss: 0.3505, step time: 1.0870\n",
"24/388, train_loss: 0.1950, step time: 1.0768\n",
"25/388, train_loss: 0.4525, step time: 1.2393\n",
"26/388, train_loss: 0.3685, step time: 1.1139\n",
"27/388, train_loss: 0.6613, step time: 1.0922\n",
"28/388, train_loss: 0.3146, step time: 1.1191\n",
"29/388, train_loss: 0.2197, step time: 1.0582\n",
"30/388, train_loss: 0.2277, step time: 1.1184\n",
"31/388, train_loss: 0.2739, step time: 1.0723\n",
"32/388, train_loss: 0.6844, step time: 1.1622\n",
"33/388, train_loss: 0.3656, step time: 1.1079\n",
"34/388, train_loss: 0.3574, step time: 1.1482\n",
"35/388, train_loss: 0.1880, step time: 1.1068\n",
"36/388, train_loss: 0.4876, step time: 1.0652\n",
"37/388, train_loss: 0.1562, step time: 1.0886\n",
"38/388, train_loss: 0.4965, step time: 1.1580\n",
"39/388, train_loss: 0.3290, step time: 1.1652\n",
"40/388, train_loss: 0.1549, step time: 1.1027\n",
"41/388, train_loss: 0.2184, step time: 1.0861\n",
"42/388, train_loss: 0.5060, step time: 1.0919\n",
"43/388, train_loss: 0.2663, step time: 1.0595\n",
"44/388, train_loss: 0.4393, step time: 1.1427\n",
"45/388, train_loss: 0.1749, step time: 1.1421\n",
"46/388, train_loss: 0.2066, step time: 1.1275\n",
"47/388, train_loss: 0.2624, step time: 1.0516\n",
"48/388, train_loss: 0.3328, step time: 1.0983\n",
"49/388, train_loss: 0.2870, step time: 1.1364\n",
"50/388, train_loss: 0.2714, step time: 1.0501\n",
"51/388, train_loss: 0.3070, step time: 1.2105\n",
"52/388, train_loss: 0.1552, step time: 1.0718\n",
"53/388, train_loss: 0.7283, step time: 1.0322\n",
"54/388, train_loss: 0.2276, step time: 1.1050\n",
"55/388, train_loss: 0.6264, step time: 1.1108\n",
"56/388, train_loss: 0.2889, step time: 1.1132\n",
"57/388, train_loss: 0.1654, step time: 1.1959\n",
"58/388, train_loss: 0.8695, step time: 1.1166\n",
"59/388, train_loss: 0.2256, step time: 1.0898\n",
"60/388, train_loss: 0.2163, step time: 1.0725\n",
"61/388, train_loss: 0.4654, step time: 1.0710\n",
"62/388, train_loss: 0.3792, step time: 1.0862\n",
"63/388, train_loss: 0.2885, step time: 1.1295\n",
"64/388, train_loss: 0.4954, step time: 1.1121\n",
"65/388, train_loss: 0.1980, step time: 1.0391\n",
"66/388, train_loss: 0.1499, step time: 1.0668\n",
"67/388, train_loss: 0.2521, step time: 1.0620\n",
"68/388, train_loss: 0.2184, step time: 1.0602\n",
"69/388, train_loss: 0.1880, step time: 1.1107\n",
"70/388, train_loss: 0.2027, step time: 1.1583\n",
"71/388, train_loss: 0.2732, step time: 1.2118\n",
"72/388, train_loss: 0.2239, step time: 1.1152\n",
"73/388, train_loss: 0.2666, step time: 1.1044\n",
"74/388, train_loss: 0.2116, step time: 1.0555\n",
"75/388, train_loss: 0.2224, step time: 1.1277\n",
"76/388, train_loss: 0.2443, step time: 1.0489\n",
"77/388, train_loss: 0.3279, step time: 1.1730\n",
"78/388, train_loss: 0.2928, step time: 1.0715\n",
"79/388, train_loss: 0.4248, step time: 1.1050\n",
"80/388, train_loss: 0.2861, step time: 1.1313\n",
"81/388, train_loss: 0.4864, step time: 1.0425\n",
"82/388, train_loss: 0.5972, step time: 1.0571\n",
"83/388, train_loss: 0.4025, step time: 1.1618\n",
"84/388, train_loss: 0.1651, step time: 1.1157\n",
"85/388, train_loss: 0.2292, step time: 1.0304\n",
"86/388, train_loss: 0.2443, step time: 1.1184\n",
"87/388, train_loss: 0.2496, step time: 1.0593\n",
"88/388, train_loss: 0.3199, step time: 1.1126\n",
"89/388, train_loss: 0.2913, step time: 1.2173\n",
"90/388, train_loss: 0.1783, step time: 1.1085\n",
"91/388, train_loss: 0.1994, step time: 1.1121\n",
"92/388, train_loss: 0.4447, step time: 1.1225\n",
"93/388, train_loss: 0.2956, step time: 1.1274\n",
"94/388, train_loss: 0.4025, step time: 1.1231\n",
"95/388, train_loss: 0.3721, step time: 1.2246\n",
"96/388, train_loss: 0.2378, step time: 1.1424\n",
"97/388, train_loss: 0.1623, step time: 1.1575\n",
"98/388, train_loss: 0.2596, step time: 1.1121\n",
"99/388, train_loss: 0.1607, step time: 1.1117\n",
"100/388, train_loss: 0.1587, step time: 1.0500\n",
"101/388, train_loss: 0.2664, step time: 1.1796\n",
"102/388, train_loss: 0.3898, step time: 1.1718\n",
"103/388, train_loss: 0.1803, step time: 1.1998\n",
"104/388, train_loss: 0.7243, step time: 1.0356\n",
"105/388, train_loss: 0.4124, step time: 1.1811\n",
"106/388, train_loss: 0.2655, step time: 1.0447\n",
"107/388, train_loss: 0.1925, step time: 1.0424\n",
"108/388, train_loss: 0.2806, step time: 1.1633\n",
"109/388, train_loss: 0.1291, step time: 1.1426\n",
"110/388, train_loss: 0.2366, step time: 1.0434\n",
"111/388, train_loss: 0.1544, step time: 1.1116\n",
"112/388, train_loss: 0.2948, step time: 1.0602\n",
"113/388, train_loss: 0.2728, step time: 1.1572\n",
"114/388, train_loss: 0.2238, step time: 1.1861\n",
"115/388, train_loss: 0.2313, step time: 1.1490\n",
"116/388, train_loss: 0.3189, step time: 1.1169\n",
"117/388, train_loss: 0.2939, step time: 1.0553\n",
"118/388, train_loss: 0.3315, step time: 1.0853\n",
"119/388, train_loss: 0.2130, step time: 1.0990\n",
"120/388, train_loss: 0.2789, step time: 1.1679\n",
"121/388, train_loss: 0.3638, step time: 1.1055\n",
"122/388, train_loss: 0.2245, step time: 1.1360\n",
"123/388, train_loss: 0.3089, step time: 1.0975\n",
"124/388, train_loss: 0.2434, step time: 1.0628\n",
"125/388, train_loss: 0.3126, step time: 1.0433\n",
"126/388, train_loss: 0.2664, step time: 1.1133\n",
"127/388, train_loss: 0.2429, step time: 1.0981\n",
"128/388, train_loss: 0.2217, step time: 1.1178\n",
"129/388, train_loss: 0.0989, step time: 1.0920\n",
"130/388, train_loss: 0.2723, step time: 1.0780\n",
"131/388, train_loss: 0.2868, step time: 1.0838\n",
"132/388, train_loss: 0.4087, step time: 1.1185\n",
"133/388, train_loss: 0.3427, step time: 1.1531\n",
"134/388, train_loss: 0.2226, step time: 1.1028\n",
"135/388, train_loss: 0.3066, step time: 1.0497\n",
"136/388, train_loss: 0.5826, step time: 1.0399\n",
"137/388, train_loss: 0.4524, step time: 1.1812\n",
"138/388, train_loss: 0.1524, step time: 1.0712\n",
"139/388, train_loss: 0.1746, step time: 1.1206\n",
"140/388, train_loss: 0.2364, step time: 1.1406\n",
"141/388, train_loss: 0.4744, step time: 1.1164\n",
"142/388, train_loss: 0.1858, step time: 1.1226\n",
"143/388, train_loss: 0.2987, step time: 1.0943\n",
"144/388, train_loss: 0.1599, step time: 1.0707\n",
"145/388, train_loss: 0.2675, step time: 1.1826\n",
"146/388, train_loss: 0.2036, step time: 1.1860\n",
"147/388, train_loss: 0.2645, step time: 1.1837\n",
"148/388, train_loss: 0.1735, step time: 1.0587\n",
"149/388, train_loss: 0.3151, step time: 1.0435\n",
"150/388, train_loss: 0.2312, step time: 1.1308\n",
"151/388, train_loss: 0.0927, step time: 1.1056\n",
"152/388, train_loss: 0.2398, step time: 1.1221\n",
"153/388, train_loss: 0.1986, step time: 1.1247\n",
"154/388, train_loss: 0.4188, step time: 1.0525\n",
"155/388, train_loss: 0.3160, step time: 1.0369\n",
"156/388, train_loss: 0.3259, step time: 1.0897\n",
"157/388, train_loss: 0.2863, step time: 1.0691\n",
"158/388, train_loss: 0.4519, step time: 1.0646\n",
"159/388, train_loss: 0.3174, step time: 1.1098\n",
"160/388, train_loss: 0.3006, step time: 1.0892\n",
"161/388, train_loss: 0.4452, step time: 1.0722\n",
"162/388, train_loss: 0.1176, step time: 1.0483\n",
"163/388, train_loss: 0.2591, step time: 1.0547\n",
"164/388, train_loss: 0.3365, step time: 1.0585\n",
"165/388, train_loss: 0.4440, step time: 1.1712\n",
"166/388, train_loss: 0.3552, step time: 1.1306\n",
"167/388, train_loss: 0.3898, step time: 1.1173\n",
"168/388, train_loss: 0.3063, step time: 1.0463\n",
"169/388, train_loss: 0.2103, step time: 1.1676\n",
"170/388, train_loss: 0.3434, step time: 1.1599\n",
"171/388, train_loss: 0.5822, step time: 1.1498\n",
"172/388, train_loss: 0.1185, step time: 1.1351\n",
"173/388, train_loss: 0.4961, step time: 1.1587\n",
"174/388, train_loss: 0.2480, step time: 1.1139\n",
"175/388, train_loss: 0.3736, step time: 1.0979\n",
"176/388, train_loss: 0.1501, step time: 1.0890\n",
"177/388, train_loss: 0.2112, step time: 1.0362\n",
"178/388, train_loss: 0.1805, step time: 1.1313\n",
"179/388, train_loss: 0.2591, step time: 1.1550\n",
"180/388, train_loss: 0.3402, step time: 1.1504\n",
"181/388, train_loss: 0.2143, step time: 1.0478\n",
"182/388, train_loss: 0.2548, step time: 1.0803\n",
"183/388, train_loss: 0.1993, step time: 1.1026\n",
"184/388, train_loss: 0.2652, step time: 1.0515\n",
"185/388, train_loss: 0.2342, step time: 1.1728\n",
"186/388, train_loss: 0.2907, step time: 1.0471\n",
"187/388, train_loss: 0.3333, step time: 1.0911\n",
"188/388, train_loss: 0.2425, step time: 1.1054\n",
"189/388, train_loss: 0.5606, step time: 1.0660\n",
"190/388, train_loss: 0.3721, step time: 1.0655\n",
"191/388, train_loss: 0.2455, step time: 1.1367\n",
"192/388, train_loss: 0.2407, step time: 1.1160\n",
"193/388, train_loss: 0.1868, step time: 1.0939\n",
"194/388, train_loss: 0.1489, step time: 1.0853\n",
"195/388, train_loss: 0.1515, step time: 1.1834\n",
"196/388, train_loss: 0.2980, step time: 1.0492\n",
"197/388, train_loss: 0.3354, step time: 1.2043\n",
"198/388, train_loss: 0.2350, step time: 1.0964\n",
"199/388, train_loss: 0.4940, step time: 1.1425\n",
"200/388, train_loss: 0.5707, step time: 1.0559\n",
"201/388, train_loss: 0.1797, step time: 1.0362\n",
"202/388, train_loss: 0.1821, step time: 1.0535\n",
"203/388, train_loss: 0.1983, step time: 1.1073\n",
"204/388, train_loss: 0.2923, step time: 1.1317\n",
"205/388, train_loss: 0.2352, step time: 1.1456\n",
"206/388, train_loss: 0.2519, step time: 1.0835\n",
"207/388, train_loss: 0.5027, step time: 1.1640\n",
"208/388, train_loss: 0.2185, step time: 1.0432\n",
"209/388, train_loss: 0.2719, step time: 1.1543\n",
"210/388, train_loss: 0.1408, step time: 1.1589\n",
"211/388, train_loss: 0.3826, step time: 1.1693\n",
"212/388, train_loss: 0.1871, step time: 1.0441\n",
"213/388, train_loss: 0.2716, step time: 1.0387\n",
"214/388, train_loss: 0.3027, step time: 1.0823\n",
"215/388, train_loss: 0.3938, step time: 1.1773\n",
"216/388, train_loss: 0.2858, step time: 1.1519\n",
"217/388, train_loss: 0.2002, step time: 1.1256\n",
"218/388, train_loss: 0.2310, step time: 1.0945\n",
"219/388, train_loss: 0.2685, step time: 1.0961\n",
"220/388, train_loss: 0.1868, step time: 1.1205\n",
"221/388, train_loss: 0.1652, step time: 1.1002\n",
"222/388, train_loss: 0.2144, step time: 1.1388\n",
"223/388, train_loss: 0.2897, step time: 1.1738\n",
"224/388, train_loss: 0.2415, step time: 1.1080\n",
"225/388, train_loss: 0.1399, step time: 1.0570\n",
"226/388, train_loss: 0.1334, step time: 1.0560\n",
"227/388, train_loss: 0.5050, step time: 1.0523\n",
"228/388, train_loss: 0.1614, step time: 1.0395\n",
"229/388, train_loss: 0.1950, step time: 1.2243\n",
"230/388, train_loss: 0.6044, step time: 1.0724\n",
"231/388, train_loss: 0.4198, step time: 1.1201\n",
"232/388, train_loss: 0.1292, step time: 1.0476\n",
"233/388, train_loss: 0.2263, step time: 1.1025\n",
"234/388, train_loss: 0.3742, step time: 1.1109\n",
"235/388, train_loss: 0.4292, step time: 1.1804\n",
"236/388, train_loss: 0.2880, step time: 1.0626\n",
"237/388, train_loss: 0.3121, step time: 1.1208\n",
"238/388, train_loss: 0.2408, step time: 1.0932\n",
"239/388, train_loss: 0.2096, step time: 1.1217\n",
"240/388, train_loss: 0.1526, step time: 1.0484\n",
"241/388, train_loss: 0.2258, step time: 1.1694\n",
"242/388, train_loss: 0.4105, step time: 1.1529\n",
"243/388, train_loss: 0.2847, step time: 1.1523\n",
"244/388, train_loss: 0.3248, step time: 1.0754\n",
"245/388, train_loss: 0.6508, step time: 1.1058\n",
"246/388, train_loss: 0.2251, step time: 1.1075\n",
"247/388, train_loss: 0.2622, step time: 1.1419\n",
"248/388, train_loss: 0.1677, step time: 1.1400\n",
"249/388, train_loss: 0.5145, step time: 1.1226\n",
"250/388, train_loss: 0.1720, step time: 1.1211\n",
"251/388, train_loss: 0.3850, step time: 1.1070\n",
"252/388, train_loss: 0.2227, step time: 1.0651\n",
"253/388, train_loss: 0.8094, step time: 1.2550\n",
"254/388, train_loss: 0.2558, step time: 1.1520\n",
"255/388, train_loss: 0.1922, step time: 1.1436\n",
"256/388, train_loss: 0.4009, step time: 1.0527\n",
"257/388, train_loss: 0.1965, step time: 1.0686\n",
"258/388, train_loss: 0.3264, step time: 1.0633\n",
"259/388, train_loss: 0.1729, step time: 1.1969\n",
"260/388, train_loss: 0.8268, step time: 1.1847\n",
"261/388, train_loss: 0.2289, step time: 1.1557\n",
"262/388, train_loss: 0.2224, step time: 1.0630\n",
"263/388, train_loss: 0.3432, step time: 1.1033\n",
"264/388, train_loss: 0.1781, step time: 1.0454\n",
"265/388, train_loss: 0.1470, step time: 1.0811\n",
"266/388, train_loss: 0.2071, step time: 1.1190\n",
"267/388, train_loss: 0.3489, step time: 1.1673\n",
"268/388, train_loss: 0.2016, step time: 1.0577\n",
"269/388, train_loss: 0.1735, step time: 1.0868\n",
"270/388, train_loss: 0.4015, step time: 1.1256\n",
"271/388, train_loss: 0.2333, step time: 1.1580\n",
"272/388, train_loss: 0.3291, step time: 1.1262\n",
"273/388, train_loss: 0.2714, step time: 1.1695\n",
"274/388, train_loss: 0.2773, step time: 1.0773\n",
"275/388, train_loss: 0.4961, step time: 1.0370\n",
"276/388, train_loss: 0.6477, step time: 1.1364\n",
"277/388, train_loss: 0.4037, step time: 1.0872\n",
"278/388, train_loss: 0.4606, step time: 1.0997\n",
"279/388, train_loss: 0.2230, step time: 1.1824\n",
"280/388, train_loss: 0.1008, step time: 1.1143\n",
"281/388, train_loss: 0.2340, step time: 1.1037\n",
"282/388, train_loss: 0.2178, step time: 1.0800\n",
"283/388, train_loss: 0.2184, step time: 1.0394\n",
"284/388, train_loss: 0.3610, step time: 1.1242\n",
"285/388, train_loss: 0.1374, step time: 1.1918\n",
"286/388, train_loss: 0.3322, step time: 1.0606\n",
"287/388, train_loss: 0.2547, step time: 1.1023\n",
"288/388, train_loss: 0.0981, step time: 1.1006\n",
"289/388, train_loss: 0.7879, step time: 1.1076\n",
"290/388, train_loss: 0.1211, step time: 1.1108\n",
"291/388, train_loss: 0.5174, step time: 1.0537\n",
"292/388, train_loss: 0.6993, step time: 1.1352\n",
"293/388, train_loss: 0.3143, step time: 1.1941\n",
"294/388, train_loss: 0.2710, step time: 1.0473\n",
"295/388, train_loss: 0.2278, step time: 1.0882\n",
"296/388, train_loss: 0.2137, step time: 1.1522\n",
"297/388, train_loss: 0.6825, step time: 1.1969\n",
"298/388, train_loss: 0.7091, step time: 1.1784\n",
"299/388, train_loss: 0.1088, step time: 1.1241\n",
"300/388, train_loss: 0.2512, step time: 1.1147\n",
"301/388, train_loss: 0.3993, step time: 1.1180\n",
"302/388, train_loss: 0.1803, step time: 1.1228\n",
"303/388, train_loss: 0.3656, step time: 1.1544\n",
"304/388, train_loss: 0.2296, step time: 1.1170\n",
"305/388, train_loss: 0.4066, step time: 1.1059\n",
"306/388, train_loss: 0.1043, step time: 1.1106\n",
"307/388, train_loss: 0.3176, step time: 1.0253\n",
"308/388, train_loss: 0.5087, step time: 1.0449\n",
"309/388, train_loss: 0.2593, step time: 1.0528\n",
"310/388, train_loss: 0.1490, step time: 1.1070\n",
"311/388, train_loss: 0.2639, step time: 1.1555\n",
"312/388, train_loss: 0.2890, step time: 1.1172\n",
"313/388, train_loss: 0.1761, step time: 1.0933\n",
"314/388, train_loss: 0.5423, step time: 1.1039\n",
"315/388, train_loss: 0.2394, step time: 1.1125\n",
"316/388, train_loss: 0.3649, step time: 1.0442\n",
"317/388, train_loss: 0.2841, step time: 1.2130\n",
"318/388, train_loss: 0.6489, step time: 1.0834\n",
"319/388, train_loss: 0.2325, step time: 1.0929\n",
"320/388, train_loss: 0.2886, step time: 1.0610\n",
"321/388, train_loss: 0.1692, step time: 1.0362\n",
"322/388, train_loss: 0.2918, step time: 1.1063\n",
"323/388, train_loss: 0.5815, step time: 1.2434\n",
"324/388, train_loss: 0.1351, step time: 1.1674\n",
"325/388, train_loss: 0.5624, step time: 1.1465\n",
"326/388, train_loss: 0.5472, step time: 1.0911\n",
"327/388, train_loss: 0.1086, step time: 1.0487\n",
"328/388, train_loss: 0.2737, step time: 1.0946\n",
"329/388, train_loss: 0.4316, step time: 1.0942\n",
"330/388, train_loss: 0.3529, step time: 1.1496\n",
"331/388, train_loss: 0.0876, step time: 1.1897\n",
"332/388, train_loss: 0.2075, step time: 1.0664\n",
"333/388, train_loss: 0.1390, step time: 1.0329\n",
"334/388, train_loss: 0.3643, step time: 1.0559\n",
"335/388, train_loss: 0.1640, step time: 1.0462\n",
"336/388, train_loss: 0.4531, step time: 1.1167\n",
"337/388, train_loss: 0.4963, step time: 1.1597\n",
"338/388, train_loss: 0.2439, step time: 1.0590\n",
"339/388, train_loss: 0.3368, step time: 1.1850\n",
"340/388, train_loss: 0.3401, step time: 1.0690\n",
"341/388, train_loss: 0.1773, step time: 1.0335\n",
"342/388, train_loss: 0.1258, step time: 1.0546\n",
"343/388, train_loss: 0.3835, step time: 1.1670\n",
"344/388, train_loss: 0.2227, step time: 1.0629\n",
"345/388, train_loss: 0.3208, step time: 1.0983\n",
"346/388, train_loss: 0.2607, step time: 1.0562\n",
"347/388, train_loss: 0.1672, step time: 1.1484\n",
"348/388, train_loss: 0.5528, step time: 1.0781\n",
"349/388, train_loss: 0.2620, step time: 1.1380\n",
"350/388, train_loss: 0.2227, step time: 1.0847\n",
"351/388, train_loss: 0.2008, step time: 1.0420\n",
"352/388, train_loss: 0.3124, step time: 1.1360\n",
"353/388, train_loss: 0.2191, step time: 1.0556\n",
"354/388, train_loss: 0.6665, step time: 1.1404\n",
"355/388, train_loss: 0.3689, step time: 1.1838\n",
"356/388, train_loss: 0.4527, step time: 1.1463\n",
"357/388, train_loss: 0.3116, step time: 1.0994\n",
"358/388, train_loss: 0.1346, step time: 1.1254\n",
"359/388, train_loss: 0.2467, step time: 1.2001\n",
"360/388, train_loss: 0.1068, step time: 1.1716\n",
"361/388, train_loss: 0.2782, step time: 1.1752\n",
"362/388, train_loss: 0.2011, step time: 1.0528\n",
"363/388, train_loss: 0.3582, step time: 1.1137\n",
"364/388, train_loss: 0.2032, step time: 1.0447\n",
"365/388, train_loss: 0.5099, step time: 1.0942\n",
"366/388, train_loss: 0.2469, step time: 1.1106\n",
"367/388, train_loss: 0.4719, step time: 1.2119\n",
"368/388, train_loss: 0.1783, step time: 1.1170\n",
"369/388, train_loss: 0.1827, step time: 1.1074\n",
"370/388, train_loss: 0.4331, step time: 1.0492\n",
"371/388, train_loss: 0.2319, step time: 1.0817\n",
"372/388, train_loss: 0.1716, step time: 1.0740\n",
"373/388, train_loss: 0.3131, step time: 1.1242\n",
"374/388, train_loss: 0.2334, step time: 1.1603\n",
"375/388, train_loss: 0.3252, step time: 1.0356\n",
"376/388, train_loss: 0.2279, step time: 1.1123\n",
"377/388, train_loss: 0.2977, step time: 1.0972\n",
"378/388, train_loss: 0.4469, step time: 1.1238\n",
"379/388, train_loss: 0.1273, step time: 1.0622\n",
"380/388, train_loss: 0.2107, step time: 1.1217\n",
"381/388, train_loss: 0.2086, step time: 1.1607\n",
"382/388, train_loss: 0.2005, step time: 1.0483\n",
"383/388, train_loss: 0.4131, step time: 1.1042\n",
"384/388, train_loss: 0.4993, step time: 1.1112\n",
"385/388, train_loss: 0.1793, step time: 1.1520\n",
"386/388, train_loss: 0.2713, step time: 1.1218\n",
"387/388, train_loss: 0.2216, step time: 1.1465\n",
"388/388, train_loss: 0.1042, step time: 1.0257\n",
"epoch 15 average loss: 0.3011\n",
"saved new best metric model\n",
"current epoch: 15 current mean dice: 0.7006 tc: 0.7415 wt: 0.8777 et: 0.4828\n",
"best mean dice: 0.7006 at epoch: 15\n",
"time consuming of epoch 15 is: 955.3864\n",
"----------\n",
"epoch 16/16\n",
"1/388, train_loss: 0.3175, step time: 1.1943\n",
"2/388, train_loss: 0.7291, step time: 1.1414\n",
"3/388, train_loss: 0.2435, step time: 1.0528\n",
"4/388, train_loss: 0.3963, step time: 1.1241\n",
"5/388, train_loss: 0.4210, step time: 1.2011\n",
"6/388, train_loss: 0.5345, step time: 1.1582\n",
"7/388, train_loss: 0.3449, step time: 1.1783\n",
"8/388, train_loss: 0.3347, step time: 1.1296\n",
"9/388, train_loss: 0.2242, step time: 1.1171\n",
"10/388, train_loss: 0.5355, step time: 1.0627\n",
"11/388, train_loss: 0.2623, step time: 1.0997\n",
"12/388, train_loss: 0.1748, step time: 1.1337\n",
"13/388, train_loss: 0.4101, step time: 1.1065\n",
"14/388, train_loss: 0.1763, step time: 1.0513\n",
"15/388, train_loss: 0.1555, step time: 1.0941\n",
"16/388, train_loss: 0.5659, step time: 1.1306\n",
"17/388, train_loss: 0.2885, step time: 1.0519\n",
"18/388, train_loss: 0.4322, step time: 1.1950\n",
"19/388, train_loss: 0.7853, step time: 1.1559\n",
"20/388, train_loss: 0.3297, step time: 1.1261\n",
"21/388, train_loss: 0.3043, step time: 1.0836\n",
"22/388, train_loss: 0.2145, step time: 1.1256\n",
"23/388, train_loss: 0.2125, step time: 1.0648\n",
"24/388, train_loss: 0.2962, step time: 1.1670\n",
"25/388, train_loss: 0.2487, step time: 1.1600\n",
"26/388, train_loss: 0.1976, step time: 1.0462\n",
"27/388, train_loss: 0.2089, step time: 1.0941\n",
"28/388, train_loss: 0.2955, step time: 1.1167\n",
"29/388, train_loss: 0.2598, step time: 1.1172\n",
"30/388, train_loss: 0.2101, step time: 1.1266\n",
"31/388, train_loss: 0.1165, step time: 1.2064\n",
"32/388, train_loss: 0.2887, step time: 1.0967\n",
"33/388, train_loss: 0.1313, step time: 1.0874\n",
"34/388, train_loss: 0.2624, step time: 1.1255\n",
"35/388, train_loss: 0.4323, step time: 1.0913\n",
"36/388, train_loss: 0.2225, step time: 1.1985\n",
"37/388, train_loss: 0.6103, step time: 1.1591\n",
"38/388, train_loss: 0.2093, step time: 1.1190\n",
"39/388, train_loss: 0.3317, step time: 1.0537\n",
"40/388, train_loss: 0.7404, step time: 1.1336\n",
"41/388, train_loss: 0.4582, step time: 1.0428\n",
"42/388, train_loss: 0.2161, step time: 1.1280\n",
"43/388, train_loss: 0.3534, step time: 1.2182\n",
"44/388, train_loss: 0.6703, step time: 1.1576\n",
"45/388, train_loss: 0.2004, step time: 1.1781\n",
"46/388, train_loss: 0.3049, step time: 1.1217\n",
"47/388, train_loss: 0.3241, step time: 1.0512\n",
"48/388, train_loss: 0.3099, step time: 1.0687\n",
"49/388, train_loss: 0.3795, step time: 1.2750\n",
"50/388, train_loss: 0.2831, step time: 1.1486\n",
"51/388, train_loss: 0.1358, step time: 1.1784\n",
"52/388, train_loss: 0.3860, step time: 1.1175\n",
"53/388, train_loss: 0.6840, step time: 1.0834\n",
"54/388, train_loss: 0.3756, step time: 1.1138\n",
"55/388, train_loss: 0.2031, step time: 1.1822\n",
"56/388, train_loss: 0.3716, step time: 1.1316\n",
"57/388, train_loss: 0.2649, step time: 1.1655\n",
"58/388, train_loss: 0.1424, step time: 1.1119\n",
"59/388, train_loss: 0.2324, step time: 1.1568\n",
"60/388, train_loss: 0.1986, step time: 1.1337\n",
"61/388, train_loss: 0.3209, step time: 1.0726\n",
"62/388, train_loss: 0.2493, step time: 1.0465\n",
"63/388, train_loss: 0.2709, step time: 1.1788\n",
"64/388, train_loss: 0.4464, step time: 1.0904\n",
"65/388, train_loss: 0.2987, step time: 1.0970\n",
"66/388, train_loss: 0.2536, step time: 1.0602\n",
"67/388, train_loss: 0.3179, step time: 1.0405\n",
"68/388, train_loss: 0.2204, step time: 1.1301\n",
"69/388, train_loss: 0.1686, step time: 1.1573\n",
"70/388, train_loss: 0.4774, step time: 1.0737\n",
"71/388, train_loss: 0.2933, step time: 1.0888\n",
"72/388, train_loss: 0.3277, step time: 1.0524\n",
"73/388, train_loss: 0.2688, step time: 1.0384\n",
"74/388, train_loss: 0.2760, step time: 1.0780\n",
"75/388, train_loss: 0.3570, step time: 1.2168\n",
"76/388, train_loss: 0.6934, step time: 1.0537\n",
"77/388, train_loss: 0.2584, step time: 1.0793\n",
"78/388, train_loss: 0.2377, step time: 1.0485\n",
"79/388, train_loss: 0.2236, step time: 1.0868\n",
"80/388, train_loss: 0.1872, step time: 1.0542\n",
"81/388, train_loss: 0.3725, step time: 1.1869\n",
"82/388, train_loss: 0.3887, step time: 1.0728\n",
"83/388, train_loss: 0.3479, step time: 1.1015\n",
"84/388, train_loss: 0.4540, step time: 1.0660\n",
"85/388, train_loss: 0.1543, step time: 1.0810\n",
"86/388, train_loss: 0.2729, step time: 1.1107\n",
"87/388, train_loss: 0.1163, step time: 1.2135\n",
"88/388, train_loss: 0.1242, step time: 1.1184\n",
"89/388, train_loss: 0.5917, step time: 1.1291\n",
"90/388, train_loss: 0.5371, step time: 1.1135\n",
"91/388, train_loss: 0.2608, step time: 1.0322\n",
"92/388, train_loss: 0.2303, step time: 1.1111\n",
"93/388, train_loss: 0.3183, step time: 1.1750\n",
"94/388, train_loss: 0.0856, step time: 1.1447\n",
"95/388, train_loss: 0.4070, step time: 1.1519\n",
"96/388, train_loss: 0.5968, step time: 1.0728\n",
"97/388, train_loss: 0.8005, step time: 1.1003\n",
"98/388, train_loss: 0.3577, step time: 1.0700\n",
"99/388, train_loss: 0.0913, step time: 1.0389\n",
"100/388, train_loss: 0.1710, step time: 1.1839\n",
"101/388, train_loss: 0.3733, step time: 1.0937\n",
"102/388, train_loss: 0.1528, step time: 1.1132\n",
"103/388, train_loss: 0.1523, step time: 1.0973\n",
"104/388, train_loss: 0.4535, step time: 1.1126\n",
"105/388, train_loss: 0.2500, step time: 1.1008\n",
"106/388, train_loss: 0.1522, step time: 1.0638\n",
"107/388, train_loss: 0.2852, step time: 1.1773\n",
"108/388, train_loss: 0.2204, step time: 1.1181\n",
"109/388, train_loss: 0.2029, step time: 1.0972\n",
"110/388, train_loss: 0.1197, step time: 1.0911\n",
"111/388, train_loss: 0.1776, step time: 1.0315\n",
"112/388, train_loss: 0.2733, step time: 1.0824\n",
"113/388, train_loss: 0.3743, step time: 1.1667\n",
"114/388, train_loss: 0.3405, step time: 1.0488\n",
"115/388, train_loss: 0.1675, step time: 1.1761\n",
"116/388, train_loss: 0.2430, step time: 1.0936\n",
"117/388, train_loss: 0.1184, step time: 1.0971\n",
"118/388, train_loss: 0.3990, step time: 1.0467\n",
"119/388, train_loss: 0.4955, step time: 1.1913\n",
"120/388, train_loss: 0.1719, step time: 1.1102\n",
"121/388, train_loss: 0.1748, step time: 1.1074\n",
"122/388, train_loss: 0.3338, step time: 1.0334\n",
"123/388, train_loss: 0.1527, step time: 1.1007\n",
"124/388, train_loss: 0.3519, step time: 1.0462\n",
"125/388, train_loss: 0.5513, step time: 1.2218\n",
"126/388, train_loss: 0.6512, step time: 1.1723\n",
"127/388, train_loss: 0.2090, step time: 1.0721\n",
"128/388, train_loss: 0.1287, step time: 1.0484\n",
"129/388, train_loss: 0.0853, step time: 1.0374\n",
"130/388, train_loss: 0.5196, step time: 1.0572\n",
"131/388, train_loss: 0.3857, step time: 1.1708\n",
"132/388, train_loss: 0.2433, step time: 1.1117\n",
"133/388, train_loss: 0.1361, step time: 1.1604\n",
"134/388, train_loss: 0.2028, step time: 1.0395\n",
"135/388, train_loss: 0.1529, step time: 1.0379\n",
"136/388, train_loss: 0.3529, step time: 1.0525\n",
"137/388, train_loss: 0.1589, step time: 1.1272\n",
"138/388, train_loss: 0.4420, step time: 1.1583\n",
"139/388, train_loss: 0.1592, step time: 1.1646\n",
"140/388, train_loss: 0.3777, step time: 1.0897\n",
"141/388, train_loss: 0.2942, step time: 1.0945\n",
"142/388, train_loss: 0.3141, step time: 1.0931\n",
"143/388, train_loss: 0.1279, step time: 1.0555\n",
"144/388, train_loss: 0.2816, step time: 1.1634\n",
"145/388, train_loss: 0.4874, step time: 1.1544\n",
"146/388, train_loss: 0.1632, step time: 1.0429\n",
"147/388, train_loss: 0.2372, step time: 1.1772\n",
"148/388, train_loss: 0.6670, step time: 1.0687\n",
"149/388, train_loss: 0.1526, step time: 1.0638\n",
"150/388, train_loss: 0.3555, step time: 1.0573\n",
"151/388, train_loss: 0.1831, step time: 1.1632\n",
"152/388, train_loss: 0.2857, step time: 1.0681\n",
"153/388, train_loss: 0.6805, step time: 1.1047\n",
"154/388, train_loss: 0.1034, step time: 1.1143\n",
"155/388, train_loss: 0.2384, step time: 1.0354\n",
"156/388, train_loss: 0.2177, step time: 1.1315\n",
"157/388, train_loss: 0.2647, step time: 1.1247\n",
"158/388, train_loss: 0.2421, step time: 1.1029\n",
"159/388, train_loss: 0.1813, step time: 1.0535\n",
"160/388, train_loss: 0.1973, step time: 1.0510\n",
"161/388, train_loss: 0.2475, step time: 1.0979\n",
"162/388, train_loss: 0.1596, step time: 1.0487\n",
"163/388, train_loss: 0.2935, step time: 1.1875\n",
"164/388, train_loss: 0.1926, step time: 1.1078\n",
"165/388, train_loss: 0.1225, step time: 1.1558\n",
"166/388, train_loss: 0.2521, step time: 1.0531\n",
"167/388, train_loss: 0.2076, step time: 1.0999\n",
"168/388, train_loss: 0.1343, step time: 1.1085\n",
"169/388, train_loss: 0.5020, step time: 1.2327\n",
"170/388, train_loss: 0.2919, step time: 1.0722\n",
"171/388, train_loss: 0.2164, step time: 1.1218\n",
"172/388, train_loss: 0.1885, step time: 1.1356\n",
"173/388, train_loss: 0.1795, step time: 1.0463\n",
"174/388, train_loss: 0.2039, step time: 1.0540\n",
"175/388, train_loss: 0.4128, step time: 1.2360\n",
"176/388, train_loss: 0.3320, step time: 1.1371\n",
"177/388, train_loss: 0.1266, step time: 1.0948\n",
"178/388, train_loss: 0.3370, step time: 1.1252\n",
"179/388, train_loss: 0.2428, step time: 1.1268\n",
"180/388, train_loss: 0.3609, step time: 1.0872\n",
"181/388, train_loss: 0.2582, step time: 1.2132\n",
"182/388, train_loss: 0.2079, step time: 1.1573\n",
"183/388, train_loss: 0.2280, step time: 1.1482\n",
"184/388, train_loss: 0.1318, step time: 1.0584\n",
"185/388, train_loss: 0.2819, step time: 1.0865\n",
"186/388, train_loss: 0.2468, step time: 1.0927\n",
"187/388, train_loss: 0.2987, step time: 1.1776\n",
"188/388, train_loss: 0.3721, step time: 1.1828\n",
"189/388, train_loss: 0.6608, step time: 1.2238\n",
"190/388, train_loss: 0.4215, step time: 1.0490\n",
"191/388, train_loss: 0.1300, step time: 1.1091\n",
"192/388, train_loss: 0.2712, step time: 1.0518\n",
"193/388, train_loss: 0.5186, step time: 1.1122\n",
"194/388, train_loss: 0.4154, step time: 1.1680\n",
"195/388, train_loss: 0.2680, step time: 1.1770\n",
"196/388, train_loss: 0.6306, step time: 1.0603\n",
"197/388, train_loss: 0.2460, step time: 1.0546\n",
"198/388, train_loss: 0.1504, step time: 1.1090\n",
"199/388, train_loss: 0.1802, step time: 1.0364\n",
"200/388, train_loss: 0.4993, step time: 1.1143\n",
"201/388, train_loss: 0.5635, step time: 1.1518\n",
"202/388, train_loss: 0.1641, step time: 1.0528\n",
"203/388, train_loss: 0.5085, step time: 1.0985\n",
"204/388, train_loss: 0.2452, step time: 1.1205\n",
"205/388, train_loss: 0.2464, step time: 1.0931\n",
"206/388, train_loss: 0.2516, step time: 1.0905\n",
"207/388, train_loss: 0.2772, step time: 1.1895\n",
"208/388, train_loss: 0.2207, step time: 1.0820\n",
"209/388, train_loss: 0.3416, step time: 1.0328\n",
"210/388, train_loss: 0.2613, step time: 1.0656\n",
"211/388, train_loss: 0.3187, step time: 1.0984\n",
"212/388, train_loss: 0.3695, step time: 1.0762\n",
"213/388, train_loss: 0.3283, step time: 1.2553\n",
"214/388, train_loss: 0.4948, step time: 1.1332\n",
"215/388, train_loss: 0.4739, step time: 1.1614\n",
"216/388, train_loss: 0.2418, step time: 1.1031\n",
"217/388, train_loss: 0.4068, step time: 1.1008\n",
"218/388, train_loss: 0.5064, step time: 1.1241\n",
"219/388, train_loss: 0.2504, step time: 1.2532\n",
"220/388, train_loss: 0.2262, step time: 1.1368\n",
"221/388, train_loss: 0.2318, step time: 1.2074\n",
"222/388, train_loss: 0.2762, step time: 1.1083\n",
"223/388, train_loss: 0.2014, step time: 1.1204\n",
"224/388, train_loss: 0.1741, step time: 1.0981\n",
"225/388, train_loss: 0.2153, step time: 1.0537\n",
"226/388, train_loss: 0.1582, step time: 1.1745\n",
"227/388, train_loss: 0.2131, step time: 1.1762\n",
"228/388, train_loss: 0.1536, step time: 1.0450\n",
"229/388, train_loss: 0.1668, step time: 1.1173\n",
"230/388, train_loss: 0.1433, step time: 1.1260\n",
"231/388, train_loss: 0.1769, step time: 1.0404\n",
"232/388, train_loss: 0.2995, step time: 1.0891\n",
"233/388, train_loss: 0.1612, step time: 1.1585\n",
"234/388, train_loss: 0.4088, step time: 1.0600\n",
"235/388, train_loss: 0.3011, step time: 1.0372\n",
"236/388, train_loss: 0.4797, step time: 1.0562\n",
"237/388, train_loss: 0.4450, step time: 1.0946\n",
"238/388, train_loss: 0.7366, step time: 1.1266\n",
"239/388, train_loss: 0.3283, step time: 1.2051\n",
"240/388, train_loss: 0.1494, step time: 1.1253\n",
"241/388, train_loss: 0.1732, step time: 1.1481\n",
"242/388, train_loss: 0.1965, step time: 1.1162\n",
"243/388, train_loss: 0.4437, step time: 1.1114\n",
"244/388, train_loss: 0.3807, step time: 1.1080\n",
"245/388, train_loss: 0.1834, step time: 1.1579\n",
"246/388, train_loss: 0.2438, step time: 1.1130\n",
"247/388, train_loss: 0.0918, step time: 1.0455\n",
"248/388, train_loss: 0.3808, step time: 1.0499\n",
"249/388, train_loss: 0.2304, step time: 1.1008\n",
"250/388, train_loss: 0.3074, step time: 1.0484\n",
"251/388, train_loss: 0.2881, step time: 1.1781\n",
"252/388, train_loss: 0.3340, step time: 1.0539\n",
"253/388, train_loss: 0.4918, step time: 1.0924\n",
"254/388, train_loss: 0.2121, step time: 1.0673\n",
"255/388, train_loss: 0.2075, step time: 1.0950\n",
"256/388, train_loss: 0.3161, step time: 1.0984\n",
"257/388, train_loss: 0.2064, step time: 1.1380\n",
"258/388, train_loss: 0.5714, step time: 1.1300\n",
"259/388, train_loss: 0.2219, step time: 1.0656\n",
"260/388, train_loss: 0.1325, step time: 1.1186\n",
"261/388, train_loss: 0.1226, step time: 1.0666\n",
"262/388, train_loss: 0.2376, step time: 1.1010\n",
"263/388, train_loss: 0.2394, step time: 1.0435\n",
"264/388, train_loss: 0.2797, step time: 1.1814\n",
"265/388, train_loss: 0.3874, step time: 1.1641\n",
"266/388, train_loss: 0.3865, step time: 1.1093\n",
"267/388, train_loss: 0.2519, step time: 1.0965\n",
"268/388, train_loss: 0.1507, step time: 1.1089\n",
"269/388, train_loss: 0.4058, step time: 1.0570\n",
"270/388, train_loss: 0.2579, step time: 1.2265\n",
"271/388, train_loss: 0.3471, step time: 1.0941\n",
"272/388, train_loss: 0.2320, step time: 1.1065\n",
"273/388, train_loss: 0.2263, step time: 1.0443\n",
"274/388, train_loss: 0.2739, step time: 1.0727\n",
"275/388, train_loss: 0.2579, step time: 1.0914\n",
"276/388, train_loss: 0.4110, step time: 1.0578\n",
"277/388, train_loss: 0.1525, step time: 1.1863\n",
"278/388, train_loss: 0.1937, step time: 1.1220\n",
"279/388, train_loss: 0.4447, step time: 1.1060\n",
"280/388, train_loss: 0.5535, step time: 1.0643\n",
"281/388, train_loss: 0.4146, step time: 1.1021\n",
"282/388, train_loss: 0.1388, step time: 1.0929\n",
"283/388, train_loss: 0.2458, step time: 1.1577\n",
"284/388, train_loss: 0.2146, step time: 1.0909\n",
"285/388, train_loss: 0.3028, step time: 1.1318\n",
"286/388, train_loss: 0.2233, step time: 1.1196\n",
"287/388, train_loss: 0.1316, step time: 1.0306\n",
"288/388, train_loss: 0.4943, step time: 1.0994\n",
"289/388, train_loss: 0.2305, step time: 1.1979\n",
"290/388, train_loss: 0.2102, step time: 1.1224\n",
"291/388, train_loss: 0.3402, step time: 1.1698\n",
"292/388, train_loss: 0.1893, step time: 1.0444\n",
"293/388, train_loss: 0.2560, step time: 1.0438\n",
"294/388, train_loss: 0.1483, step time: 1.0618\n",
"295/388, train_loss: 0.2783, step time: 1.1813\n",
"296/388, train_loss: 0.3041, step time: 1.1654\n",
"297/388, train_loss: 0.2170, step time: 1.0994\n",
"298/388, train_loss: 0.2290, step time: 1.0740\n",
"299/388, train_loss: 0.1997, step time: 1.0449\n",
"300/388, train_loss: 0.2074, step time: 1.0528\n",
"301/388, train_loss: 0.2593, step time: 1.0460\n",
"302/388, train_loss: 0.1859, step time: 1.1047\n",
"303/388, train_loss: 0.4111, step time: 1.1483\n",
"304/388, train_loss: 0.1237, step time: 1.1188\n",
"305/388, train_loss: 0.2742, step time: 1.1005\n",
"306/388, train_loss: 0.1855, step time: 1.0854\n",
"307/388, train_loss: 0.5203, step time: 1.1230\n",
"308/388, train_loss: 0.5272, step time: 1.1054\n",
"309/388, train_loss: 0.1679, step time: 1.1768\n",
"310/388, train_loss: 0.1705, step time: 1.0348\n",
"311/388, train_loss: 0.1773, step time: 1.0974\n",
"312/388, train_loss: 0.2377, step time: 1.0560\n",
"313/388, train_loss: 0.2592, step time: 1.0444\n",
"314/388, train_loss: 0.4591, step time: 1.1330\n",
"315/388, train_loss: 0.6215, step time: 1.1288\n",
"316/388, train_loss: 0.2749, step time: 1.1229\n",
"317/388, train_loss: 0.2814, step time: 1.0883\n",
"318/388, train_loss: 0.2693, step time: 1.0945\n",
"319/388, train_loss: 0.2394, step time: 1.0355\n",
"320/388, train_loss: 0.2784, step time: 1.1138\n",
"321/388, train_loss: 0.5045, step time: 1.1427\n",
"322/388, train_loss: 0.1177, step time: 1.0934\n",
"323/388, train_loss: 0.1660, step time: 1.1384\n",
"324/388, train_loss: 0.1729, step time: 1.0552\n",
"325/388, train_loss: 0.5955, step time: 1.0352\n",
"326/388, train_loss: 0.3229, step time: 1.0508\n",
"327/388, train_loss: 0.2927, step time: 1.1159\n",
"328/388, train_loss: 0.2235, step time: 1.1346\n",
"329/388, train_loss: 0.5138, step time: 1.1660\n",
"330/388, train_loss: 0.1993, step time: 1.1175\n",
"331/388, train_loss: 0.1832, step time: 1.0547\n",
"332/388, train_loss: 0.2891, step time: 1.0770\n",
"333/388, train_loss: 0.2726, step time: 1.1086\n",
"334/388, train_loss: 0.1457, step time: 1.1625\n",
"335/388, train_loss: 0.4035, step time: 1.1274\n",
"336/388, train_loss: 0.5427, step time: 1.0723\n",
"337/388, train_loss: 0.1804, step time: 1.0918\n",
"338/388, train_loss: 0.2007, step time: 1.1311\n",
"339/388, train_loss: 0.2419, step time: 1.1105\n",
"340/388, train_loss: 0.3226, step time: 1.1089\n",
"341/388, train_loss: 0.3098, step time: 1.1812\n",
"342/388, train_loss: 0.2651, step time: 1.1099\n",
"343/388, train_loss: 0.2988, step time: 1.0819\n",
"344/388, train_loss: 0.3136, step time: 1.0475\n",
"345/388, train_loss: 0.1138, step time: 1.1073\n",
"346/388, train_loss: 0.1654, step time: 1.0484\n",
"347/388, train_loss: 0.4338, step time: 1.1981\n",
"348/388, train_loss: 0.5601, step time: 1.0771\n",
"349/388, train_loss: 0.2243, step time: 1.1692\n",
"350/388, train_loss: 0.2730, step time: 1.1189\n",
"351/388, train_loss: 0.4359, step time: 1.0385\n",
"352/388, train_loss: 0.4172, step time: 1.1358\n",
"353/388, train_loss: 0.2704, step time: 1.2403\n",
"354/388, train_loss: 0.2148, step time: 1.1243\n",
"355/388, train_loss: 0.1625, step time: 1.1263\n",
"356/388, train_loss: 0.3017, step time: 1.0566\n",
"357/388, train_loss: 0.4057, step time: 1.0998\n",
"358/388, train_loss: 0.2382, step time: 1.1254\n",
"359/388, train_loss: 0.1706, step time: 1.1662\n",
"360/388, train_loss: 0.3436, step time: 1.1171\n",
"361/388, train_loss: 0.2562, step time: 1.1814\n",
"362/388, train_loss: 0.1950, step time: 1.0477\n",
"363/388, train_loss: 0.4870, step time: 1.1155\n",
"364/388, train_loss: 0.1882, step time: 1.1086\n",
"365/388, train_loss: 0.2606, step time: 1.1187\n",
"366/388, train_loss: 0.2052, step time: 1.1404\n",
"367/388, train_loss: 0.2496, step time: 1.1354\n",
"368/388, train_loss: 0.2757, step time: 1.1536\n",
"369/388, train_loss: 0.3240, step time: 1.1041\n",
"370/388, train_loss: 0.3304, step time: 1.0810\n",
"371/388, train_loss: 0.6376, step time: 1.0424\n",
"372/388, train_loss: 0.2814, step time: 1.1694\n",
"373/388, train_loss: 0.3125, step time: 1.1488\n",
"374/388, train_loss: 0.1018, step time: 1.1395\n",
"375/388, train_loss: 0.2530, step time: 1.0505\n",
"376/388, train_loss: 0.5617, step time: 1.1251\n",
"377/388, train_loss: 0.2982, step time: 1.0907\n",
"378/388, train_loss: 0.3254, step time: 1.1447\n",
"379/388, train_loss: 0.1889, step time: 1.1802\n",
"380/388, train_loss: 0.7208, step time: 1.0963\n",
"381/388, train_loss: 0.2906, step time: 1.0442\n",
"382/388, train_loss: 0.1661, step time: 1.0869\n",
"383/388, train_loss: 0.1624, step time: 1.1745\n",
"384/388, train_loss: 0.4159, step time: 1.0878\n",
"385/388, train_loss: 0.2160, step time: 1.1293\n",
"386/388, train_loss: 0.2155, step time: 1.1393\n",
"387/388, train_loss: 0.2350, step time: 1.0368\n",
"388/388, train_loss: 0.4847, step time: 1.0280\n",
"epoch 16 average loss: 0.2981\n",
"current epoch: 16 current mean dice: 0.7006 tc: 0.7415 wt: 0.8777 et: 0.4828\n",
"best mean dice: 0.7006 at epoch: 15\n",
"time consuming of epoch 16 is: 960.7444\n"
