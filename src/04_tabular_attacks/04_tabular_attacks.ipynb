{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYmdL3l-2eOF"
      },
      "source": [
        "# Tabular attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F73P8q4h2h7_"
      },
      "source": [
        "The objective of this practical is to adapt a powerful attack from image classification to tabular data. As shown in the class, the main challenge is to respect domain constraints.\n",
        "\n",
        "The translation table of constraints can be found here: https://arxiv.org/pdf/2112.01156, Table 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuMG_5ltBlgA"
      },
      "source": [
        "## Import package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEx8c5BPBozE"
      },
      "source": [
        "It is good practice to import all necessary packages at the top of Python files or in the first code cell of a Python notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8J0mnBAqB34I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import mlc\n",
        "from mlc.datasets.dataset_factory import get_dataset\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS2FtrBvB7mO"
      },
      "source": [
        "We check the correct version are installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqYf3po7B-nl",
        "outputId": "d75d71ae-ae99-4838-add7-610a0b2555ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: mlc==0.1.0.\n"
          ]
        }
      ],
      "source": [
        "for pkg, version in [(mlc, \"0.1.0\")]:\n",
        "    if version in pkg.__version__:\n",
        "        print(f\"OK: {pkg.__name__}=={pkg.__version__}.\")\n",
        "    else:\n",
        "        print(f\"Version mismatch: expected version {version} for package {pkg.__name__} but is currently {pkg.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THKn2-vlGC2H"
      },
      "source": [
        "## Retrieve data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUc64rNzGCZi"
      },
      "source": [
        "In this section we will download and load a feature engineered version of the URL dataset. The ojective is to classify URL as legitimate or potential phishing attack.\n",
        "We only consider type, boundary and relationship constraints. All features are mutable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WIhkHObkDkEY"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset(\"lcld_v2_iid\")\n",
        "x, y = dataset.get_x_y()\n",
        "metadata = dataset.get_metadata(only_x=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplify the problem to non categorical\n",
        "# Select only non-categorical features\n",
        "non_cat_features = metadata[metadata[\"type\"] != \"cat\"][\"feature\"].tolist()\n",
        "x = x[non_cat_features]\n",
        "metadata = metadata[metadata[\"feature\"].isin(non_cat_features)].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r9xERyggHHiN"
      },
      "outputs": [],
      "source": [
        "# Splitting the data\n",
        "splits = dataset.get_splits()\n",
        "x_train, x_val, x_test = x.iloc[splits[\"train\"]].to_numpy(), x.iloc[splits[\"val\"]].to_numpy(), x.iloc[splits[\"test\"]].to_numpy()\n",
        "y_train, y_val, y_test = y[splits[\"train\"]], y[splits[\"val\"]], y[splits[\"test\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFTBxjroN2iG"
      },
      "source": [
        "As you can see below, the dataset only contains numerical values: 13 continous and 10 discretes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "wbwRCrEHNgmO",
        "outputId": "f534c637-5965-48f1-d117-74b55737086b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "type\n",
              "real    13\n",
              "int     10\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata[\"type\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AYkCwBROQ8K"
      },
      "source": [
        "Neural networks needs scaled data to obtain the best performance.\n",
        "We usually use min/max or standard scaling.\n",
        "Attacks from image classification also suppose min/max scaling in the [0 , 1] range.\n",
        "For simplicity we will use min/max scaling in this notebook.\n",
        "However, constraints penalty function evaluations need to be perform in the unscaled/original domain.\n",
        "Hence we will use extensively the following transform / inverse transform functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VZ17vgkDPCU7"
      },
      "outputs": [],
      "source": [
        "class Scaler:\n",
        "    def __init__(self, x_min, x_max):\n",
        "        self.x_min = x_min\n",
        "        self.x_max = x_max\n",
        "\n",
        "        # Define the scale and set to 1 if equals to 0.\n",
        "        scale = x_max - x_min\n",
        "        constant_mask = scale < 10 * torch.finfo(torch.from_numpy(scale).dtype).eps\n",
        "        scale = scale.copy()\n",
        "        scale[constant_mask] = 1.0\n",
        "        self.scale = scale\n",
        "\n",
        "    def transform(self, x):\n",
        "        x_min = self.x_min\n",
        "        scale = self.scale\n",
        "\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x_min = torch.from_numpy(x_min).float()\n",
        "            scale = torch.from_numpy(scale).float()\n",
        "\n",
        "        return (x - x_min) / scale\n",
        "\n",
        "    def inverse_transform(self, x):\n",
        "        x_min = self.x_min\n",
        "        scale = self.scale\n",
        "\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x_min = torch.from_numpy(x_min).float()\n",
        "            scale = torch.from_numpy(scale).float()\n",
        "\n",
        "        return x * scale + x_min\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9XnrhswFR9jo"
      },
      "outputs": [],
      "source": [
        "x_min = metadata[\"min\"].to_numpy().astype(\"float\")\n",
        "x_max = metadata[\"max\"].to_numpy().astype(\"float\")\n",
        "\n",
        "scaler = Scaler(x_min, x_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FnwGkL23WJgh"
      },
      "outputs": [],
      "source": [
        "x_t = scaler.transform(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LpHBUxhY2SS",
        "outputId": "419cb6f4-4371-41dd-b321-d640440d1f27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_t.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0yKAJk0XrTiE"
      },
      "outputs": [],
      "source": [
        "x_it = scaler.inverse_transform(x_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os6wJ7c6rXKO",
        "outputId": "6730f4c3-1394-4bb0-98e1-80228165c9ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.3283064365386963e-10"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.max((x_train - x_it))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgqtA3jgSKHD"
      },
      "source": [
        "## Fit a Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUjMT5KLTNxZ"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlXwmVBFSpXd"
      },
      "source": [
        "We define a simple neural network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeZD1iDQSOop"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.l1 = nn.Linear(x_train.shape[1], 128)\n",
        "        self.l2 = nn.Linear(128, 128)\n",
        "        self.l3 = nn.Linear(128, 128)\n",
        "        self.l4 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.l4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wSjpDOmSsxE"
      },
      "source": [
        "We create a scaler module that will scale the input based on a scaler before feeding the results to the neural network.\n",
        "To chain two such nn.Module (Net and ScalerModule), we can use the nn.Sequential nn.Module: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nlA6zcT5SPgy"
      },
      "outputs": [],
      "source": [
        "class ScalerModule(nn.Module):\n",
        "    def __init__(self, scaler):\n",
        "        super(ScalerModule, self).__init__()\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = scaler.transform(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuQaXUGWTasF"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lQZpvDoTeOt"
      },
      "source": [
        "We use the class weight to give importance to the underrepresented class during training. Here, the class are balanced but it is not always the case. For instance, in fraud detection we observe a huge imbalance with a few frauds for a large number of legitimate transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1l_W-v-NAh6",
        "outputId": "aa73e506-d1dc-4b75-cf48-ab34adafa5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weight tensor([0.2009, 0.7991])\n"
          ]
        }
      ],
      "source": [
        "class_weight = torch.Tensor(\n",
        "    1 - torch.unique(torch.tensor(y_train), return_counts=True)[1] / len(y_train)\n",
        ")\n",
        "print(f\"Class weight {class_weight}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXiWdIoeT5qH"
      },
      "source": [
        "Here we use the aforementioned nn.Sequential module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rnlkZp7jNT66"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(ScalerModule(scaler), Net()).float()\n",
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=0.001,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jcAdcVkNsYF"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in tqdm(enumerate(dataloader), total=int(size/batch_size)):\n",
        "        # if batch % 10 == 0:\n",
        "        #     print(f\"Batch {batch}.\")\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def val_loop(dataloader, model, loss_fn, epoch_i):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y[:, 1]).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Epoch {epoch_i}, Val Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, x_train, y_train, x_val, y_val, optimizer, batch_size, loss_func, epochs):\n",
        "    # Data processing\n",
        "    train_dataset = TensorDataset(x_train, y_train)\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,m_num\n",
        "        num_workers=2,\n",
        "    )\n",
        "    val_dataset = TensorDataset(x_val, y_val)\n",
        "    val_loader = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=2000,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    # Main train loop\n",
        "    for epoch in range(epochs):\n",
        "        train_loop(train_loader, model, loss_func, optimizer, batch_size)\n",
        "        val_loop(val_loader, model, loss_func, epoch)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_YHuRh7ONOR",
        "outputId": "91eb7b19-96ee-42c6-ce82-6d0b76457ec3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7721it [00:13, 587.63it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Val Error: Accuracy: 67.0%, Avg loss: 0.200277\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7721it [00:12, 597.86it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Val Error: Accuracy: 67.6%, Avg loss: 0.201033\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7721it [00:12, 595.39it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Val Error: Accuracy: 66.9%, Avg loss: 0.199877\n"
          ]
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss(weight=class_weight)\n",
        "train_model(\n",
        "    model,\n",
        "    torch.from_numpy(x_train).float(),\n",
        "    torch.from_numpy(np.array([1 - y_train, y_train]).T).float(),\n",
        "    torch.from_numpy(x_val).float(),\n",
        "    torch.from_numpy(np.array([1 - y_val, y_val]).T).float(),\n",
        "    optimizer,\n",
        "    64,\n",
        "    loss,\n",
        "    3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9EZlIo6JO92l"
      },
      "outputs": [],
      "source": [
        "# Model prediction\n",
        "y_score = model(torch.from_numpy(x_test).float()).detach().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEd7pTvsQ7-b",
        "outputId": "3e934c32-8431-42bb-9a47-bd9177aec75a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The AUROC score of the model is 0.7120895698942986\n"
          ]
        }
      ],
      "source": [
        "# Model scoring\n",
        "auc = roc_auc_score(y_test, y_score[:, 1])\n",
        "print(f\"The AUROC score of the model is {auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HQTBUdhZtvj"
      },
      "source": [
        "## Generating adversarial examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89MBe7oqhWlu"
      },
      "source": [
        "### PGD Attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaUPq60bhLgP"
      },
      "source": [
        "Bellow is the PGD attack for image classification.\n",
        "The perturbation is bounded by a maximum L2 norm, called epsilon (eps).\n",
        "We initialy set the maximum perturbation to eps = 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "v9SwUhe3hRUp"
      },
      "outputs": [],
      "source": [
        "n_examples = 1000\n",
        "eps = 5\n",
        "n_iter = 100\n",
        "alpha = 2*eps\n",
        "eps_for_division=1e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mCelBfCul7S4"
      },
      "outputs": [],
      "source": [
        "def perturb(scaler, x_origin, x_adv, grad, eps, alpha, n_iter, iter):\n",
        "    \n",
        "    x_origin = scaler.transform(x_origin)\n",
        "    x_adv = scaler.transform(x_adv)\n",
        "\n",
        "    # Compute L2 pertubation\n",
        "    grad_norms = (\n",
        "        torch.norm(grad.view(x_adv.shape[0], -1), p=2, dim=1)\n",
        "        + eps_for_division\n",
        "    )  # nopep8\n",
        "    grad = grad / grad_norms.view(x_adv.shape[0], 1)\n",
        "    \n",
        "    \n",
        "    decay_steps = max(n_iter // 10, 1)\n",
        "    decay_factor = iter // decay_steps\n",
        "    l_alpha = alpha / (2 ** decay_factor)\n",
        "\n",
        "    # Apply L2 perturbation\n",
        "    x_adv = x_adv + l_alpha * grad\n",
        "\n",
        "    # Project on L2\n",
        "    delta = x_origin - x_adv\n",
        "    delta_norms = torch.norm(delta.view(x_adv.shape[0], -1), p=2, dim=1)\n",
        "    factor = eps / delta_norms\n",
        "    factor = torch.min(factor, torch.ones_like(delta_norms))\n",
        "    delta = delta * factor.view(\n",
        "        -1,\n",
        "        1,\n",
        "    )\n",
        "    x_adv = x_origin + delta\n",
        "\n",
        "    # Clamp\n",
        "    x_adv = torch.clamp(x_adv, 0, 1)\n",
        "    \n",
        "    x_adv = scaler.inverse_transform(x_adv)\n",
        "\n",
        "    return x_adv.detach()\n",
        "\n",
        "\n",
        "\n",
        "def generate_adversarial2(model,  x, y, eps, alpha, iter, scaler, verbose=1):\n",
        "    x_adv = x.clone().detach()\n",
        "\n",
        "    iterable = range(iter)\n",
        "    if verbose >0:\n",
        "        iterable = tqdm(iterable)\n",
        "    for i in iterable:\n",
        "        x_adv.requires_grad = True\n",
        "        output = model(x_adv)\n",
        "        loss = F.cross_entropy(output, y)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        data_grad =  x_adv.grad.data\n",
        "        x_adv = perturb(scaler, x, x_adv, data_grad, eps, alpha, n_iter, i)\n",
        "    return x_adv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35YJcfqihoDy"
      },
      "source": [
        "1. Write a `is_constrained_adversarial` function that, for a set of examples x and their correct labels y, determines if:\n",
        "- x is adversarial,\n",
        "- x respects the boundary constraints,\n",
        "- x respects the type constraints,\n",
        "- x respects the feature relation constraints,\n",
        "- all of the above.\n",
        "\n",
        "For boundary, you can tolerate 10 * torch.finfo((x).dtype).eps difference, due to float precision.\n",
        "\n",
        "Type constraints can be access with:\n",
        "```\n",
        "metadata[\"type\"]\n",
        "```\n",
        "\n",
        "Feature relation constraints are:\n",
        "\n",
        "```\n",
        "int_rate = Feature(\"int_rate\") / Constant(1200)\n",
        "term = Feature(\"term\")\n",
        "installment = Feature(\"loan_amnt\") * (\n",
        "    (int_rate * ((Constant(1) + int_rate) ** term))\n",
        "    / ((Constant(1) + int_rate) ** term - Constant(1))\n",
        ")\n",
        "g1 = ABS(Feature(\"installment\") - installment) <= 0.1\n",
        "\n",
        "g2 = Feature(\"open_acc\") <= Feature(\"total_acc\")\n",
        "\n",
        "g3 = Feature(\"pub_rec_bankruptcies\") <= Feature(\"pub_rec\")\n",
        "```\n",
        "\n",
        "Use the feat_to_idx function bellow to retrieve the feature in x, e.g.\n",
        "```\n",
        "int_rate = x[:, feat_to_ix(\"int_rate\")]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def feat_to_idx(feature:str) -> int:\n",
        "    return metadata[metadata[\"feature\"] == feature].index[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "qgOUoGdikAq0",
        "outputId": "baab8f25-d3fc-4f81-88ab-0e4efbd06030"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 0.3652173913043478, 0.0, 0.0, 0.0)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "model(torch.from_numpy(x_test).float()[:5])\n",
        "\n",
        "def is_constrained_adversarial(x, y, model, metadata) -> Tuple[float,float,float,float,float]:\n",
        "    features = torch.from_numpy(x).float()\n",
        "    y_pred = model(features).detach().numpy()\n",
        "    y_class = np.argmax(y, axis=1)\n",
        "\n",
        "    correct = (y_pred == y).astype(int)\n",
        "    percent_correct = correct.mean()\n",
        "    percent_adv = 1 - percent_correct\n",
        "\n",
        "    respects_boundary = features <= (1 + 10 * torch.finfo(features.dtype).eps)\n",
        "    percent_respects_boundary = respects_boundary.sum().item() / (respects_boundary.shape[0] * respects_boundary.shape[1])\n",
        "\n",
        "    return percent_adv, percent_respects_boundary, 0.0, 0.0, 0.0\n",
        "\n",
        "is_constrained_adversarial(x_test[:5], np.array([1 - y_test, y_test]).T[:5], model, metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Verify your is constrained function by running it on the test set instead of adversarial set. Few examples should be adversarial, but all should pass the constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muF4B56ziW2-"
      },
      "source": [
        "3. Run PGD and evaluate the success rate of the attack based on the `is_constrained_adversarial` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKEx8ACCkBLa"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlErihj-ksW0"
      },
      "source": [
        "4. Comment your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkVYmwm7k2xn"
      },
      "source": [
        "YOUR TEXT HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8_BoZm3cYkm"
      },
      "source": [
        "5. Adapt PGD to respect type constraints.\n",
        "\n",
        "PGD is implemented for continuous numerical values only, hence it generates real values.\n",
        "Write a function that converts reals to integer and guarantees that it does not break boundaries and epsilon constraints.\n",
        "Integrates this function into PGD.\n",
        "\n",
        "DO NOT remove/modify the cell with the original implementation of PGD, you will need it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0tEMGdlkBx7"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLSa5q3oio5Z"
      },
      "source": [
        "7. Compare the  success rate with the original implementation of PGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kClXPoykCMD"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4XdHYfVk_Y_"
      },
      "source": [
        "8. Comment your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKK3dR4Dk_7U"
      },
      "source": [
        "YOUR TEXT HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrrXqGrzfsSD"
      },
      "source": [
        "9. Write a function that for a sample X returns the constraints penalty function of the constraints above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa_2CLP_kCwo"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CigBEqbgi5P"
      },
      "source": [
        "10. Integrates the constraints penalty function in the loss of the PGD attack as in CPGD (shown in class).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIY6M2LpkDIi"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep-z95PHi0kO"
      },
      "source": [
        "11. Compare the success rate with previous implemenations of PGD.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk3z4u0jkDr7"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZthtlIVjGHr"
      },
      "source": [
        "12. Comment your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owjHy9rwkEcl"
      },
      "source": [
        "YOUR TEXT HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "04-tabular-attacks",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
