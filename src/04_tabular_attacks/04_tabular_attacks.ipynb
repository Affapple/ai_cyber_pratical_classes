{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYmdL3l-2eOF"
      },
      "source": [
        "# Tabular attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F73P8q4h2h7_"
      },
      "source": [
        "The objective of this practical is to adapt a powerful attack from image classification to tabular data. As shown in the class, the main challenge is to respect domain constraints.\n",
        "\n",
        "The translation table of constraints can be found here: https://arxiv.org/pdf/2112.01156, Table 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuMG_5ltBlgA"
      },
      "source": [
        "## Import package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEx8c5BPBozE"
      },
      "source": [
        "It is good practice to import all necessary packages at the top of Python files or in the first code cell of a Python notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8J0mnBAqB34I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import mlc\n",
        "from mlc.datasets.dataset_factory import get_dataset\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS2FtrBvB7mO"
      },
      "source": [
        "We check the correct version are installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqYf3po7B-nl",
        "outputId": "d75d71ae-ae99-4838-add7-610a0b2555ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: mlc==0.1.0.\n"
          ]
        }
      ],
      "source": [
        "for pkg, version in [(mlc, \"0.1.0\")]:\n",
        "    if version in pkg.__version__:\n",
        "        print(f\"OK: {pkg.__name__}=={pkg.__version__}.\")\n",
        "    else:\n",
        "        print(f\"Version mismatch: expected version {version} for package {pkg.__name__} but is currently {pkg.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THKn2-vlGC2H"
      },
      "source": [
        "## Retrieve data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUc64rNzGCZi"
      },
      "source": [
        "In this section we will download and load a feature engineered version of the URL dataset. The ojective is to classify URL as legitimate or potential phishing attack.\n",
        "We only consider type, boundary and relationship constraints. All features are mutable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WIhkHObkDkEY"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset(\"lcld_v2_iid\")\n",
        "x, y = dataset.get_x_y()\n",
        "metadata = dataset.get_metadata(only_x=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              feature                 min  \\\n",
            "0                                           loan_amnt              1000.0   \n",
            "1                                                term                  36   \n",
            "2                                            int_rate                5.31   \n",
            "3                                         installment                4.93   \n",
            "4                                           sub_grade                   0   \n",
            "5                                          emp_length                   0   \n",
            "6                                          annual_inc                33.0   \n",
            "7                                                 dti                -1.0   \n",
            "8                                            open_acc                   1   \n",
            "9                                             pub_rec                 0.0   \n",
            "10                                          revol_bal                 0.0   \n",
            "11                                         revol_util                 0.0   \n",
            "12                                          total_acc                   2   \n",
            "13                                           mort_acc                   0   \n",
            "14                               pub_rec_bankruptcies                   0   \n",
            "15                                         fico_score               662.0   \n",
            "16                                      month_of_year                   0   \n",
            "17                         ratio_loan_amnt_annual_inc  0.0001714285714285   \n",
            "18                           ratio_open_acc_total_acc   0.024390243902439   \n",
            "19                       month_since_earliest_cr_line                  36   \n",
            "20         ratio_pub_rec_month_since_earliest_cr_line                 0.0   \n",
            "21  ratio_pub_rec_bankruptcies_month_since_earlies...                 0.0   \n",
            "22                 ratio_pub_rec_bankruptcies_pub_rec                -1.0   \n",
            "\n",
            "                   max  mutable  type  \n",
            "0              40000.0     True  real  \n",
            "1                   60    False   int  \n",
            "2                30.99    False  real  \n",
            "3              1719.83     True  real  \n",
            "4                   34    False   int  \n",
            "5                   10     True   int  \n",
            "6           10999200.0     True  real  \n",
            "7                999.0     True  real  \n",
            "8                   90     True   int  \n",
            "9                 86.0    False   int  \n",
            "10           2904836.0     True  real  \n",
            "11               892.3     True  real  \n",
            "12                 176     True   int  \n",
            "13                  51     True   int  \n",
            "14                  12    False   int  \n",
            "15               847.5    False  real  \n",
            "16                  11    False   int  \n",
            "17   310.6060606060606     True  real  \n",
            "18                 1.0     True  real  \n",
            "19                 999    False   int  \n",
            "20  0.4959349593495935    False  real  \n",
            "21  0.0909090909090909    False  real  \n",
            "22                 1.0    False  real  \n"
          ]
        }
      ],
      "source": [
        "# Simplify the problem to non categorical\n",
        "# Select only non-categorical features\n",
        "non_cat_features = metadata[metadata[\"type\"] != \"cat\"][\"feature\"].tolist()\n",
        "x = x[non_cat_features]\n",
        "metadata = metadata[metadata[\"feature\"].isin(non_cat_features)].reset_index(drop=True)\n",
        "\n",
        "print(metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r9xERyggHHiN"
      },
      "outputs": [],
      "source": [
        "# Splitting the data\n",
        "splits = dataset.get_splits()\n",
        "x_train, x_val, x_test = x.iloc[splits[\"train\"]].to_numpy(), x.iloc[splits[\"val\"]].to_numpy(), x.iloc[splits[\"test\"]].to_numpy()\n",
        "y_train, y_val, y_test = y[splits[\"train\"]], y[splits[\"val\"]], y[splits[\"test\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFTBxjroN2iG"
      },
      "source": [
        "As you can see below, the dataset only contains numerical values: 13 continous and 10 discretes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "wbwRCrEHNgmO",
        "outputId": "f534c637-5965-48f1-d117-74b55737086b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "type\n",
              "real    13\n",
              "int     10\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata[\"type\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AYkCwBROQ8K"
      },
      "source": [
        "Neural networks needs scaled data to obtain the best performance.\n",
        "We usually use min/max or standard scaling.\n",
        "Attacks from image classification also suppose min/max scaling in the [0 , 1] range.\n",
        "For simplicity we will use min/max scaling in this notebook.\n",
        "However, constraints penalty function evaluations need to be perform in the unscaled/original domain.\n",
        "Hence we will use extensively the following transform / inverse transform functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VZ17vgkDPCU7"
      },
      "outputs": [],
      "source": [
        "class Scaler:\n",
        "    def __init__(self, x_min, x_max):\n",
        "        self.x_min = x_min\n",
        "        self.x_max = x_max\n",
        "\n",
        "        # Define the scale and set to 1 if equals to 0.\n",
        "        scale = x_max - x_min\n",
        "        constant_mask = scale < 10 * torch.finfo(torch.from_numpy(scale).dtype).eps\n",
        "        scale = scale.copy()\n",
        "        scale[constant_mask] = 1.0\n",
        "        self.scale = scale\n",
        "\n",
        "    def transform(self, x):\n",
        "        x_min = self.x_min\n",
        "        scale = self.scale\n",
        "\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x_min = torch.from_numpy(x_min).float()\n",
        "            scale = torch.from_numpy(scale).float()\n",
        "\n",
        "        return (x - x_min) / scale\n",
        "\n",
        "    def inverse_transform(self, x):\n",
        "        x_min = self.x_min\n",
        "        scale = self.scale\n",
        "\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x_min = torch.from_numpy(x_min).float()\n",
        "            scale = torch.from_numpy(scale).float()\n",
        "\n",
        "        return x * scale + x_min\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9XnrhswFR9jo"
      },
      "outputs": [],
      "source": [
        "x_min = metadata[\"min\"].to_numpy().astype(\"float\")\n",
        "x_max = metadata[\"max\"].to_numpy().astype(\"float\")\n",
        "\n",
        "scaler = Scaler(x_min, x_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FnwGkL23WJgh"
      },
      "outputs": [],
      "source": [
        "x_t = scaler.transform(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LpHBUxhY2SS",
        "outputId": "419cb6f4-4371-41dd-b321-d640440d1f27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_t.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0yKAJk0XrTiE"
      },
      "outputs": [],
      "source": [
        "x_it = scaler.inverse_transform(x_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os6wJ7c6rXKO",
        "outputId": "6730f4c3-1394-4bb0-98e1-80228165c9ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.3283064365386963e-10"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.max((x_train - x_it))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgqtA3jgSKHD"
      },
      "source": [
        "## Fit a Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUjMT5KLTNxZ"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlXwmVBFSpXd"
      },
      "source": [
        "We define a simple neural network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EeZD1iDQSOop"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.l1 = nn.Linear(x_train.shape[1], 128)\n",
        "        self.l2 = nn.Linear(128, 128)\n",
        "        self.l3 = nn.Linear(128, 128)\n",
        "        self.l4 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.l4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wSjpDOmSsxE"
      },
      "source": [
        "We create a scaler module that will scale the input based on a scaler before feeding the results to the neural network.\n",
        "To chain two such nn.Module (Net and ScalerModule), we can use the nn.Sequential nn.Module: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nlA6zcT5SPgy"
      },
      "outputs": [],
      "source": [
        "class ScalerModule(nn.Module):\n",
        "    def __init__(self, scaler):\n",
        "        super(ScalerModule, self).__init__()\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = scaler.transform(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuQaXUGWTasF"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lQZpvDoTeOt"
      },
      "source": [
        "We use the class weight to give importance to the underrepresented class during training. Here, the class are balanced but it is not always the case. For instance, in fraud detection we observe a huge imbalance with a few frauds for a large number of legitimate transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1l_W-v-NAh6",
        "outputId": "aa73e506-d1dc-4b75-cf48-ab34adafa5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weight tensor([0.2009, 0.7991])\n"
          ]
        }
      ],
      "source": [
        "class_weight = torch.Tensor(\n",
        "    1 - torch.unique(torch.tensor(y_train), return_counts=True)[1] / len(y_train)\n",
        ")\n",
        "print(f\"Class weight {class_weight}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXiWdIoeT5qH"
      },
      "source": [
        "Here we use the aforementioned nn.Sequential module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rnlkZp7jNT66"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(ScalerModule(scaler), Net()).float()\n",
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=0.001,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6jcAdcVkNsYF"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in tqdm(enumerate(dataloader), total=int(size/batch_size)):\n",
        "        # if batch % 10 == 0:\n",
        "        #     print(f\"Batch {batch}.\")\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def val_loop(dataloader, model, loss_fn, epoch_i):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y[:, 1]).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Epoch {epoch_i}, Val Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, x_train, y_train, x_val, y_val, optimizer, batch_size, loss_func, epochs):\n",
        "    # Data processing\n",
        "    train_dataset = TensorDataset(x_train, y_train)\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "    val_dataset = TensorDataset(x_val, y_val)\n",
        "    val_loader = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=2000,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    # Main train loop\n",
        "    for epoch in range(epochs):\n",
        "        train_loop(train_loader, model, loss_func, optimizer, batch_size)\n",
        "        val_loop(val_loader, model, loss_func, epoch)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_YHuRh7ONOR",
        "outputId": "91eb7b19-96ee-42c6-ce82-6d0b76457ec3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7721it [00:13, 585.41it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Val Error: Accuracy: 64.8%, Avg loss: 0.200121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7721it [00:12, 605.64it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Val Error: Accuracy: 66.5%, Avg loss: 0.200017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7721it [00:13, 590.48it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Val Error: Accuracy: 61.0%, Avg loss: 0.200782\n"
          ]
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss(weight=class_weight)\n",
        "\n",
        "train_model(\n",
        "    model,\n",
        "    torch.from_numpy(x_train).float(),\n",
        "    torch.from_numpy(np.array([1 - y_train, y_train]).T).float(),\n",
        "    torch.from_numpy(x_val).float(),\n",
        "    torch.from_numpy(np.array([1 - y_val, y_val]).T).float(),\n",
        "    optimizer,\n",
        "    64,\n",
        "    loss,\n",
        "    3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9EZlIo6JO92l"
      },
      "outputs": [],
      "source": [
        "# Model prediction\n",
        "y_score = model(torch.from_numpy(x_test).float()).detach().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEd7pTvsQ7-b",
        "outputId": "3e934c32-8431-42bb-9a47-bd9177aec75a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The AUROC score of the model is 0.712450798254593\n"
          ]
        }
      ],
      "source": [
        "# Model scoring\n",
        "auc = roc_auc_score(y_test, y_score[:, 1])\n",
        "print(f\"The AUROC score of the model is {auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HQTBUdhZtvj"
      },
      "source": [
        "## Generating adversarial examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89MBe7oqhWlu"
      },
      "source": [
        "### PGD Attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaUPq60bhLgP"
      },
      "source": [
        "Bellow is the PGD attack for image classification.\n",
        "The perturbation is bounded by a maximum L2 norm, called epsilon (eps).\n",
        "We initialy set the maximum perturbation to eps = 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "v9SwUhe3hRUp"
      },
      "outputs": [],
      "source": [
        "n_examples = 1000\n",
        "eps = 5\n",
        "n_iter = 100\n",
        "alpha = 2*eps\n",
        "eps_for_division=1e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mCelBfCul7S4"
      },
      "outputs": [],
      "source": [
        "def perturb(scaler, x_origin, x_adv, grad, eps, alpha, n_iter, iter):\n",
        "    x_origin = scaler.transform(x_origin)\n",
        "    x_adv = scaler.transform(x_adv)\n",
        "\n",
        "    # Compute L2 pertubation\n",
        "    grad_norms = (\n",
        "        torch.norm(grad.view(x_adv.shape[0], -1), p=2, dim=1)\n",
        "        + eps_for_division\n",
        "    )  # nopep8\n",
        "    grad = grad / grad_norms.view(x_adv.shape[0], 1)\n",
        "    \n",
        "    \n",
        "    decay_steps = max(n_iter // 10, 1)\n",
        "    decay_factor = iter // decay_steps\n",
        "    l_alpha = alpha / (2 ** decay_factor)\n",
        "\n",
        "    # Apply L2 perturbation\n",
        "    x_adv = x_adv + l_alpha * grad\n",
        "\n",
        "    # Project on L2\n",
        "    delta = x_origin - x_adv\n",
        "    delta_norms = torch.norm(delta.view(x_adv.shape[0], -1), p=2, dim=1)\n",
        "    factor = eps / delta_norms\n",
        "    factor = torch.min(factor, torch.ones_like(delta_norms))\n",
        "    delta = delta * factor.view(\n",
        "        -1,\n",
        "        1,\n",
        "    )\n",
        "    x_adv = x_origin + delta\n",
        "\n",
        "    # Clamp\n",
        "    x_adv = torch.clamp(x_adv, 0, 1)\n",
        "    \n",
        "    x_adv = scaler.inverse_transform(x_adv)\n",
        "\n",
        "    return x_adv.detach()\n",
        "\n",
        "\n",
        "\n",
        "def generate_adversarial2(model,  x, y, eps, alpha, iter, scaler, verbose=1):\n",
        "    x_adv = x.clone().detach()\n",
        "\n",
        "    iterable = range(iter)\n",
        "    if verbose >0:\n",
        "        iterable = tqdm(iterable)\n",
        "    for i in iterable:\n",
        "        x_adv.requires_grad = True\n",
        "        output = model(x_adv)\n",
        "        loss = F.cross_entropy(output, y)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        data_grad =  x_adv.grad.data\n",
        "        x_adv = perturb(scaler, x, x_adv, data_grad, eps, alpha, n_iter, i)\n",
        "    return x_adv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35YJcfqihoDy"
      },
      "source": [
        "1. Write a `is_constrained_adversarial` function that, for a set of examples x and their correct labels y, determines if:\n",
        "- x is adversarial,\n",
        "- x respects the boundary constraints,\n",
        "- x respects the type constraints,\n",
        "- x respects the feature relation constraints,\n",
        "- all of the above.\n",
        "\n",
        "For boundary, you can tolerate 10 * torch.finfo((x).dtype).eps difference, due to float precision.\n",
        "\n",
        "Type constraints can be access with:\n",
        "```\n",
        "metadata[\"type\"]\n",
        "```\n",
        "\n",
        "Feature relation constraints are:\n",
        "\n",
        "```\n",
        "int_rate = Feature(\"int_rate\") / Constant(1200)\n",
        "term = Feature(\"term\")\n",
        "installment = Feature(\"loan_amnt\") * (\n",
        "    (int_rate * ((Constant(1) + int_rate) ** term))\n",
        "    / ((Constant(1) + int_rate) ** term - Constant(1))\n",
        ")\n",
        "g1 = ABS(Feature(\"installment\") - installment) <= 0.1\n",
        "\n",
        "g2 = Feature(\"open_acc\") <= Feature(\"total_acc\")\n",
        "\n",
        "g3 = Feature(\"pub_rec_bankruptcies\") <= Feature(\"pub_rec\")\n",
        "```\n",
        "\n",
        "Use the feat_to_idx function bellow to retrieve the feature in x, e.g.\n",
        "```\n",
        "int_rate = x[:, feat_to_ix(\"int_rate\")]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def feature_to_idx(feature:str) -> int:\n",
        "    return metadata[metadata[\"feature\"] == feature].index[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "qgOUoGdikAq0",
        "outputId": "baab8f25-d3fc-4f81-88ab-0e4efbd06030"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# model(torch.from_numpy(x_t).float()[:5])\n",
        "\n",
        "def verify_type_constraints(features: torch.Tensor, metadata) -> bool:\n",
        "    # This code possibly can be improved\n",
        "    for row in metadata.iterrows():\n",
        "        feature_name = row[1][\"feature\"]\n",
        "        feature_type = row[1][\"type\"]\n",
        "        for value in features[:, feature_to_idx(feature_name)]:\n",
        "            match feature_type:\n",
        "                case \"real\":\n",
        "                    if type(value.item()) not in (float, np.floating, torch.float):\n",
        "                        return False\n",
        "                case \"int\":\n",
        "                    # if type(value.item()) not in (int, np.integer, torch.int) and value.item() % 1 != 0: #not value.item().is_integer():\n",
        "                    if type(value.item()) not in (int, np.integer, torch.int) and not value.item().is_integer():\n",
        "                        return False\n",
        "    return True\n",
        "\n",
        "def verify_feature_relations(features: torch.Tensor) -> bool:\n",
        "    # Extract relevant features\n",
        "    int_rate = features[: , feature_to_idx(\"int_rate\")] / 1200\n",
        "    term = features[:, feature_to_idx(\"term\")]\n",
        "    loan_amnt = features[:, feature_to_idx(\"loan_amnt\")]\n",
        "    installment = features[:, feature_to_idx(\"installment\")]\n",
        "    open_acc = features[:, feature_to_idx(\"open_acc\")]\n",
        "    total_acc = features[:, feature_to_idx(\"total_acc\")]\n",
        "    pub_rec_bankruptcies = features[:, feature_to_idx(\"pub_rec_bankruptcies\")]\n",
        "    pub_rec = features[:, feature_to_idx(\"pub_rec\")]\n",
        "\n",
        "    if (int_rate * 1200).le(0.01).any():\n",
        "        print(\"Warning: int_rate is very low, check for possible division by zero.\")\n",
        "        \n",
        "    expected_installment = loan_amnt * (\n",
        "        (int_rate * ((1 + int_rate) ** term))\n",
        "        / ((1 + int_rate) ** term - 1)\n",
        "    )\n",
        "\n",
        "    # Constraints\n",
        "    g1 = (installment - expected_installment).abs() <= 0.1\n",
        "    respects_g1 = g1.sum().item() == len(g1)\n",
        "    print(f\"Max deviation was: {(installment - expected_installment).abs().max().item()}\")\n",
        "\n",
        "    g2 = open_acc <= total_acc\n",
        "    respects_g2 = g2.sum().item() == len(g2)\n",
        "    print(f\"Max difference in accs was: {(open_acc - total_acc).max().item()}\")\n",
        "    \n",
        "    g3 = pub_rec_bankruptcies <= pub_rec\n",
        "    respects_g3 = g3.sum().item() == len(g3)\n",
        "    print(f\"Max difference in pub rec was: {(pub_rec_bankruptcies - pub_rec).max().item()}\")\n",
        "\n",
        "    respects_all = respects_g1 and respects_g2 and respects_g3\n",
        "    print(f\"Respects feature relations: {respects_all} (g1: {respects_g1}, g2: {respects_g2}, g3: {respects_g3})\")\n",
        "    return respects_all\n",
        "\n",
        "def is_constrained_adversarial(x, y, model, metadata) -> Tuple[float,float,float,float,float]:\n",
        "    \"\"\"\n",
        "    - x is adversarial,\n",
        "    - x respects the boundary constraints,\n",
        "    - x respects the type constraints,\n",
        "    - x respects the feature relation constraints,\n",
        "    - all of the above.\n",
        "    \"\"\"\n",
        "    features = torch.from_numpy(x).float()\n",
        "    norm_features = scaler.transform(features)\n",
        "    y_pred = model(features).detach().numpy()\n",
        "    \n",
        "    y_pred_class = np.argmax(y_pred, axis=1)\n",
        "    y_class = np.argmax(y, axis=1)\n",
        "\n",
        "    correct = (y_pred_class == y_class).astype(int)\n",
        "    percent_correct = correct.mean()\n",
        "    \n",
        "    # - x is adversarial (Prediction different from true label)\n",
        "    percent_adv = 1 - percent_correct\n",
        "\n",
        "    # - x respects the boundary constraints,\n",
        "    respects_boundary = norm_features <= (1 + 10 * torch.finfo(features.dtype).eps)\n",
        "    percent_respects_boundary = respects_boundary.sum().item() / (respects_boundary.shape[0] * respects_boundary.shape[1])\n",
        "\n",
        "    # - x respects the type constraints\n",
        "    respects_type_constraint = verify_type_constraints(features, metadata)\n",
        "\n",
        "    # - x respects the feature relation constraints\n",
        "    respects_features_relations = verify_feature_relations(features)\n",
        "\n",
        "    constraints_met = (\n",
        "        percent_adv,\n",
        "        percent_respects_boundary,\n",
        "        respects_type_constraint, \n",
        "        respects_features_relations\n",
        "    )\n",
        "\n",
        "    return (*constraints_met, all(constraints_met))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Verify your is constrained function by running it on the test set instead of adversarial set. Few examples should be adversarial, but all should pass the constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max deviation was: 431.1004333496094\n",
            "Max difference in accs was: 0.0\n",
            "Max difference in pub rec was: 0.0\n",
            "Respects feature relations: False (g1: False, g2: True, g3: True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.3877889052049913, 1.0, False, False, False)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "# is_constrained_adversarial(x_test[:5], np.array([1 - y_test, y_test]).T[:5], model, metadata)\n",
        "is_constrained_adversarial(x_test, np.array([1 - y_test, y_test]).T, model, metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muF4B56ziW2-"
      },
      "source": [
        "3. Run PGD and evaluate the success rate of the attack based on the `is_constrained_adversarial` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CKEx8ACCkBLa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/media/jgaspar/7CCE3FA7CE3F589A1/MCS/1_Semester/ai/ai-security-labs/src/04_tabular_attacks/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "                                                                    \r"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from art.attacks.evasion import ProjectedGradientDescent\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "\n",
        "classifier = PyTorchClassifier(model=model, loss=loss, input_shape=(x_train.shape[1],), nb_classes=2, device_type=torch.device('cpu' if torch.cuda.is_available() else 'cpu').type)\n",
        "attack_projected = ProjectedGradientDescent(\n",
        "    estimator=classifier,\n",
        "    eps=16 / 255 * 784**0.5,\n",
        "    norm=2,\n",
        ")\n",
        "x_copy = x_test.astype(np.float32).copy()\n",
        "y_copy = y_test.astype(np.int64).copy()\n",
        "# By passing also the y copy into the adversarial examples generation function, an accuracy of 0% was achieved\n",
        "x_test_adv_projected = attack_projected.generate(x_copy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max deviation was: 431.1004333496094\n",
            "Max difference in accs was: 0.0\n",
            "Max difference in pub rec was: 0.0\n",
            "Respects feature relations: False (g1: False, g2: True, g3: True)\n",
            "Results obtained from real examples: (0.3877889052049913, 1.0, False, False, False)\n",
            "\n",
            "\n",
            "Max deviation was: 431.17205810546875\n",
            "Max difference in accs was: 0.0011386871337890625\n",
            "Max difference in pub rec was: 0.012365341186523438\n",
            "Respects feature relations: False (g1: False, g2: False, g3: False)\n",
            "Results obtained from adversarial examples: (0.6122157344449934, 0.9421065483515576, False, False, False)\n"
          ]
        }
      ],
      "source": [
        "adversarial_examples = torch.from_numpy(x_test_adv_projected)\n",
        "\n",
        "no_constraint_examples_results = is_constrained_adversarial(x_test, np.array([1 - y_test, y_test]).T, model, metadata)\n",
        "print(f\"Results obtained from real examples: {no_constraint_examples_results}\\n\\n\")\n",
        "\n",
        "post_convert_results = is_constrained_adversarial(x_test_adv_projected, np.array([1 - y_test, y_test]).T, model, metadata)\n",
        "print(f\"Results obtained from adversarial examples: {post_convert_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlErihj-ksW0"
      },
      "source": [
        "4. Comment your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Real dataset\n",
        "The real examples gave a % of adversarial examples of 0.32% (0.68% accuracy) which is not the best model per say.\\\n",
        "All the others constraints passed as expected except for G1, which has at least one element that does not respect the condition that the difference between installment and the expected installment value is wayy bigger than expected (431.1004 instead of a max of 0.1 due to two values mismatching: 483.3204(expected) and 52.200(real value))\n",
        "\n",
        "#### Adversarial dataset\n",
        "When we look at the adversarial examples set, the adversarial examples percentage is noticeably larger, which means that both the attack was successfull and that the metric evaluation looks correct.\\\n",
        "The remaining constraints all fail, as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8_BoZm3cYkm"
      },
      "source": [
        "5. Adapt PGD to respect type constraints.\n",
        "\n",
        "PGD is implemented for continuous numerical values only, hence it generates real values.\n",
        "Write a function that converts reals to integer and guarantees that it does not break boundaries and epsilon constraints.\n",
        "Integrates this function into PGD.\n",
        "\n",
        "DO NOT remove/modify the cell with the original implementation of PGD, you will need it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0tEMGdlkBx7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                    \r"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from art.attacks.evasion import ProjectedGradientDescent\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "\n",
        "classifier = PyTorchClassifier(model=model, loss=loss, input_shape=(x_train.shape[1],), nb_classes=2, device_type=torch.device('cpu' if torch.cuda.is_available() else 'cpu').type)\n",
        "attack_projected = ProjectedGradientDescent(\n",
        "    estimator=classifier,\n",
        "    eps=eps,\n",
        "    norm=2,\n",
        ")\n",
        "x_copy = x_test.astype(np.float32).copy()\n",
        "y_copy = y_test.astype(np.int64).copy()\n",
        "\n",
        "\n",
        "x_test_pre_convert = attack_projected.generate(x_copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PGD_convert_reals_to_integer(x_adversarial: np.ndarray, x_original: np.ndarray, metadata, eps) -> np.ndarray:\n",
        "    x_rounded = x_adversarial.copy()\n",
        "    int_features = metadata[metadata[\"type\"] == \"int\"].index.tolist()\n",
        "\n",
        "    x_rounded[:, int_features] = np.round(x_rounded[:, int_features])\n",
        "    delta = x_rounded - x_original\n",
        "    delta_norm = np.linalg.norm(delta, ord=np.inf, axis=1)\n",
        "\n",
        "    for i in range(len(x_rounded)):\n",
        "        if delta_norm[i] > eps:\n",
        "            scale = eps / delta_norm[i]\n",
        "            x_rounded[i] = x_original[i] + (x_rounded[i] - x_original[i]) * scale\n",
        "\n",
        "    x_min = metadata[\"min\"].to_numpy().astype(float)\n",
        "    x_max = metadata[\"max\"].to_numpy().astype(float)\n",
        "    x_rounded = np.clip(x_rounded, x_min, x_max)\n",
        "    \n",
        "    x_rounded[:, int_features] = x_rounded[:, int_features].astype(int)\n",
        "    return x_rounded\n",
        "\n",
        "x_test_post_convert = PGD_convert_reals_to_integer(x_test_pre_convert, x_copy, metadata, eps=eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLSa5q3oio5Z"
      },
      "source": [
        "7. Compare the  success rate with the original implementation of PGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "-kClXPoykCMD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max deviation was: 431.29803466796875\n",
            "Max difference in accs was: 0.00324249267578125\n",
            "Max difference in pub rec was: 0.03526002913713455\n",
            "Respects feature relations: False (g1: False, g2: False, g3: False)\n",
            "Results obtained before type restriction: (0.6122110947950087, 0.9393474695399414, False, False, False)\n",
            "\n",
            "\n",
            "Max deviation was: 431.1859130859375\n",
            "Max difference in accs was: 0.0\n",
            "Max difference in pub rec was: 0.0\n",
            "Respects feature relations: False (g1: False, g2: True, g3: True)\n",
            "Results obtained after type restriction: (0.7483685830740929, 1.0, True, False, False)\n"
          ]
        }
      ],
      "source": [
        "no_constraint_examples_results = is_constrained_adversarial(x_test_pre_convert, np.array([1 - y_test, y_test]).T, model, metadata)\n",
        "print(f\"Results obtained before type restriction: {no_constraint_examples_results}\\n\\n\")\n",
        "\n",
        "post_convert_results = is_constrained_adversarial(x_test_post_convert, np.array([1 - y_test, y_test]).T, model, metadata)\n",
        "print(f\"Results obtained after type restriction: {post_convert_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4XdHYfVk_Y_"
      },
      "source": [
        "8. Comment your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKK3dR4Dk_7U"
      },
      "source": [
        "While the first example g1, g2 and g3 constraints failed, g2 and g3 passed after the conversion. \n",
        "Moreover, it would be expected that the test for type constraint from the is_constrained_advesarial obtained after correcting the data would pass as True in the cleaned data but it would fail in the first data, and as seen in the \"Results obtained\" section, in the second example the third element is indeed true, therefore it passed the type constraint, whereas for the uncleaned data, it indeed failed the constraint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrrXqGrzfsSD"
      },
      "source": [
        "9. Write a function that for a sample X returns the constraints penalty function of the constraints above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Fa_2CLP_kCwo"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "def constraint_penalty(features: torch.Tensor) -> torch.Tensor:\n",
        "    # Unnormalize and extract relevant features\n",
        "    print(features)\n",
        "    model[0].scaler.inverse_transform(features)\n",
        "    int_rate: torch.Tensor             = features[:, feature_to_idx(\"int_rate\")]\n",
        "    term: torch.Tensor                 = features[:, feature_to_idx(\"term\")]\n",
        "    loan_amnt: torch.Tensor            = features[:, feature_to_idx(\"loan_amnt\")]\n",
        "    installment: torch.Tensor          = features[:, feature_to_idx(\"installment\")]\n",
        "    open_acc: torch.Tensor             = features[:, feature_to_idx(\"open_acc\")]\n",
        "    total_acc: torch.Tensor            = features[:, feature_to_idx(\"total_acc\")]\n",
        "    pub_rec_bankruptcies: torch.Tensor = features[:, feature_to_idx(\"pub_rec_bankruptcies\")]\n",
        "    pub_rec: torch.Tensor              = features[:, feature_to_idx(\"pub_rec\")]\n",
        "    \n",
        "    int_rate = int_rate / 1200\n",
        "    expected_installment = loan_amnt * (\n",
        "        (int_rate * ((1 + int_rate) ** term))\n",
        "        / ((1 + int_rate) ** term - 1)\n",
        "    )\n",
        "\n",
        "    print(f\"Expecter installment: {expected_installment}, installment: {installment}\")\n",
        "    g1_penalty = (expected_installment - installment).abs()\n",
        "    g2_penalty = (open_acc - total_acc).clamp_min(0)\n",
        "    g3_penalty = (pub_rec_bankruptcies - pub_rec).clamp_min(0)\n",
        "\n",
        "    return g1_penalty + g2_penalty + g3_penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CigBEqbgi5P"
      },
      "source": [
        "10. Integrates the constraints penalty function in the loss of the PGD attack as in CPGD (shown in class).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "vIY6M2LpkDIi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 2])\n",
            "torch.Size([32])\n",
            "tensor([[-0.1719,  0.1545],\n",
            "        [-0.0504,  0.0361],\n",
            "        [-0.4274,  0.4002],\n",
            "        [ 0.0802, -0.0938],\n",
            "        [ 0.5210, -0.5036],\n",
            "        [ 0.4330, -0.4225],\n",
            "        [ 0.1290, -0.1359],\n",
            "        [-0.5887,  0.5534],\n",
            "        [-0.1173,  0.0931],\n",
            "        [ 0.5733, -0.5492],\n",
            "        [ 0.1978, -0.2070],\n",
            "        [-0.7872,  0.7342],\n",
            "        [ 0.1500, -0.1644],\n",
            "        [-0.0495,  0.0338],\n",
            "        [-0.3402,  0.3039],\n",
            "        [ 0.0715, -0.0818],\n",
            "        [ 0.0223, -0.0380],\n",
            "        [ 0.0241, -0.0387],\n",
            "        [-0.0094, -0.0078],\n",
            "        [-0.2594,  0.2276],\n",
            "        [ 0.6017, -0.5856],\n",
            "        [ 0.0355, -0.0546],\n",
            "        [ 0.3495, -0.3484],\n",
            "        [-0.5410,  0.5027],\n",
            "        [-1.2285,  1.1491],\n",
            "        [ 0.6748, -0.6515],\n",
            "        [ 0.2264, -0.2324],\n",
            "        [ 0.2501, -0.2546],\n",
            "        [-0.3273,  0.3050],\n",
            "        [-0.1651,  0.1509],\n",
            "        [ 0.0292, -0.0430],\n",
            "        [-0.4640,  0.4253]], grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (2) must match the size of tensor b (23) at non-singleton dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m x_copy \u001b[38;5;241m=\u001b[39m x_test\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     25\u001b[0m y_copy \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 27\u001b[0m x_test_constraint_l \u001b[38;5;241m=\u001b[39m \u001b[43mattack_projected\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_copy\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/media/jgaspar/7CCE3FA7CE3F589A1/MCS/1_Semester/ai/ai-security-labs/src/04_tabular_attacks/.venv/lib/python3.10/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent.py:202\u001b[0m, in \u001b[0;36mProjectedGradientDescent.generate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03mGenerate adversarial samples and return them in an array.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m:return: An array holding the adversarial examples.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating adversarial samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/media/jgaspar/7CCE3FA7CE3F589A1/MCS/1_Semester/ai/ai-security-labs/src/04_tabular_attacks/.venv/lib/python3.10/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:223\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch.generate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rand_init_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_random_init)):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rand_init_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;66;03m# first iteration: use the adversarial examples as they are the only ones we have now\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m         adv_x[batch_index_1:batch_index_2] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_eps_step\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m         adversarial_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_batch(\n\u001b[1;32m    228\u001b[0m             x\u001b[38;5;241m=\u001b[39mbatch, targets\u001b[38;5;241m=\u001b[39mbatch_labels, mask\u001b[38;5;241m=\u001b[39mmask_batch, eps\u001b[38;5;241m=\u001b[39mbatch_eps, eps_step\u001b[38;5;241m=\u001b[39mbatch_eps_step\n\u001b[1;32m    229\u001b[0m         )\n",
            "File \u001b[0;32m/media/jgaspar/7CCE3FA7CE3F589A1/MCS/1_Semester/ai/ai-security-labs/src/04_tabular_attacks/.venv/lib/python3.10/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:284\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch._generate_batch\u001b[0;34m(self, x, targets, mask, eps, eps_step)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_max_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_max_iter \u001b[38;5;241m=\u001b[39m i_max_iter\n\u001b[0;32m--> 284\u001b[0m     adv_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_pytorch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43madv_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_random_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi_max_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adv_x\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
            "File \u001b[0;32m/media/jgaspar/7CCE3FA7CE3F589A1/MCS/1_Semester/ai/ai-security-labs/src/04_tabular_attacks/.venv/lib/python3.10/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:445\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch._compute_pytorch\u001b[0;34m(self, x, x_init, y, mask, eps, eps_step, random_init, momentum)\u001b[0m\n\u001b[1;32m    442\u001b[0m     x_adv \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Get perturbation\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m perturbation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_perturbation_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_adv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# Apply perturbation and clip\u001b[39;00m\n\u001b[1;32m    448\u001b[0m x_adv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_perturbation_pytorch(x_adv, perturbation, eps_step)\n",
            "File \u001b[0;32m/media/jgaspar/7CCE3FA7CE3F589A1/MCS/1_Semester/ai/ai-security-labs/src/04_tabular_attacks/.venv/lib/python3.10/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:309\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch._compute_perturbation_pytorch\u001b[0;34m(self, x, y, mask, momentum)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Get gradient wrt loss; invert it if attack is targeted\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargeted \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Write summary\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
            "File \u001b[0;32m/media/jgaspar/7CCE3FA7CE3F589A1/MCS/1_Semester/ai/ai-security-labs/src/04_tabular_attacks/.venv/lib/python3.10/site-packages/art/estimators/classification/pytorch.py:843\u001b[0m, in \u001b[0;36mPyTorchClassifier.loss_gradient\u001b[0;34m(self, x, y, training_mode, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# Compute the gradient and return\u001b[39;00m\n\u001b[1;32m    842\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model(inputs_t)\n\u001b[0;32m--> 843\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# Clean gradients\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m/media/jgaspar/7CCE3FA7CE3F589A1/MCS/1_Semester/ai/ai-security-labs/src/04_tabular_attacks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/media/jgaspar/7CCE3FA7CE3F589A1/MCS/1_Semester/ai/ai-security-labs/src/04_tabular_attacks/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[31], line 11\u001b[0m, in \u001b[0;36mConstraintLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m ce_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m---> 11\u001b[0m penalty \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint_penalty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m ce_loss \u001b[38;5;241m+\u001b[39m penalty\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "Cell \u001b[0;32mIn[30], line 5\u001b[0m, in \u001b[0;36mconstraint_penalty\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconstraint_penalty\u001b[39m(features: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Unnormalize and extract relevant features\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(features)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     int_rate: torch\u001b[38;5;241m.\u001b[39mTensor             \u001b[38;5;241m=\u001b[39m features[:, feature_to_idx(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      7\u001b[0m     term: torch\u001b[38;5;241m.\u001b[39mTensor                 \u001b[38;5;241m=\u001b[39m features[:, feature_to_idx(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterm\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
            "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mScaler.inverse_transform\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m     x_min \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x_min)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     29\u001b[0m     scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(scale)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m \u001b[38;5;241m+\u001b[39m x_min\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (23) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "from art.attacks.evasion import ProjectedGradientDescent\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "from torch import Tensor\n",
        "\n",
        "class ConstraintLoss(nn.CrossEntropyLoss):\n",
        "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
        "        print(input.shape)\n",
        "        print(target.shape)\n",
        "        ce_loss = super().forward(input, target)\n",
        "        penalty = constraint_penalty(input).mean()\n",
        "        loss = ce_loss + penalty\n",
        "        return loss\n",
        "\n",
        "constraint_loss = ConstraintLoss()\n",
        "\n",
        "classifier = PyTorchClassifier(model=model, loss=constraint_loss, input_shape=x_train.shape, nb_classes=2, optimizer=optimizer, device_type=\"cpu\")\n",
        "attack_projected = ProjectedGradientDescent(\n",
        "    estimator=classifier,\n",
        "    eps=eps,\n",
        "    norm=2,\n",
        "    max_iter=20\n",
        ")\n",
        "x_copy = x_test.astype(np.float32).copy()\n",
        "y_copy = y_test.astype(np.int64).copy()\n",
        "\n",
        "x_test_constraint_l = attack_projected.generate(x_copy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep-z95PHi0kO"
      },
      "source": [
        "11. Compare the success rate with previous implemenations of PGD.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk3z4u0jkDr7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max deviation was: 430.99298095703125\n",
            "Max difference in accs was: 0.0042877197265625\n",
            "Max difference in pub rec was: 0.0794677734375\n",
            "Respects feature relations: False (g1: False, g2: False, g3: False)\n",
            "Results obtained without constraints set: (0.7412281617474777, 0.9156183831404803, False, False, False)\n",
            "\n",
            "\n",
            "Max deviation was: 430.99285888671875\n",
            "Max difference in accs was: 0.005474090576171875\n",
            "Max difference in pub rec was: 0.11863107979297638\n",
            "Respects feature relations: False (g1: False, g2: False, g3: False)\n",
            "Results obtained with constraints set: (0.7057835556885588, 0.9196476165664149, False, False, False)\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "no_constraint_examples_results = is_constrained_adversarial(x_test_adv_projected, np.array([1 - y_test, y_test]).T, model, metadata)\n",
        "print(f\"Results obtained without constraints set: {no_constraint_examples_results}\\n\\n\")\n",
        "\n",
        "constraint_examples_results = is_constrained_adversarial(x_test_constraint, np.array([1 - y_test, y_test]).T, model, metadata)\n",
        "print(f\"Results obtained with constraints set: {constraint_examples_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZthtlIVjGHr"
      },
      "source": [
        "12. Comment your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owjHy9rwkEcl"
      },
      "source": [
        "PGD attack is passing only two elements per example to the input of the loss function, which is unexpected, as I could not find the error in my code, I was unfortunately unable to finish this exercise.\n",
        "However, I believe that the expected result would be adversarial examples that would indeed be considered a real example by our is_constrained_adversarial but it would be an adversarial example, bypassing our security function implemented"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "04-tabular-attacks",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
