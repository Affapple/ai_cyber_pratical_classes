{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a28e81e",
   "metadata": {},
   "source": [
    "# Explainability Lab\n",
    "**Date:** 2025-10-29\n",
    "\n",
    "**What this notebook covers**\n",
    "- Load & preprocess the UCI Adult dataset (via `fetch_openml`).\n",
    "- Train a Random Forest Classifier.\n",
    "- Generate and visualize feature attributions with **SHAP** and **Lime**.\n",
    "- Train a PyTorch MLP classifier.\n",
    "- Generate and visualize feature attributions using **Captum** gradient-based methods:\n",
    "  - Saliency\n",
    "  - SmoothGrad (NoiseTunnel)\n",
    "  - InputxGradients\n",
    "  - Integrated Gradients.\n",
    "- Generate and visualize feature attributions using **Zennit** LRP methods:\n",
    "  - LRP (Layer-wise Relevance Propagation).\n",
    "- Evaluate every XAI methods with **Quantus** metrics.\n",
    "- Implement a counterfactual generation for tabular data using **Dice**.\n",
    "\n",
    "> Notes:\n",
    "- This notebook expects an environment with internet to fetch the dataset and `captum` installed.\n",
    "- Install tips (if needed): `pip install captum lime shap zennit scikit-learn pandas matplotlib quantus`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bb501",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cf6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from captum.attr import (\n",
    "    Saliency,\n",
    "    IntegratedGradients,\n",
    "    NoiseTunnel,\n",
    "    InputXGradient,\n",
    ")\n",
    "\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "from zennit.attribution import Gradient\n",
    "\n",
    "import quantus\n",
    "\n",
    "from typing import List, Callable, Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Global seeds\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # safe even if no GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29ea21",
   "metadata": {},
   "source": [
    "## Load & Preprocess the Adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Adult from OpenML\n",
    "adult = fetch_openml(name='adult', version=2, as_frame=True)\n",
    "df = adult.frame.copy()\n",
    "\n",
    "# Replace '?' with NaN and drop rows with missing (simple approach)\n",
    "df = df.replace('?', np.nan).dropna()\n",
    "\n",
    "# Target is 'class': '>50K' or '<=50K' — convert to 0/1\n",
    "df['class'] = (df['class'] == '>50K').astype(int)\n",
    "\n",
    "# Identify categorical vs numeric columns\n",
    "target_col = 'class'\n",
    "X_df = df.drop(columns=[target_col])\n",
    "y = df[target_col].values\n",
    "\n",
    "cat_cols = X_df.select_dtypes(include=['category','object']).columns.tolist()\n",
    "num_cols = [c for c in X_df.columns if c not in cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c302f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing pipeline: OrdinalEncoder for categoricals, Standardize numerics\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        # ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),\n",
    "        ('cat', OrdinalEncoder(), cat_cols), # We use here OrdinalEncoder to limit the number of features\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_processed = preprocess.fit_transform(X_df)\n",
    "feature_names_num = num_cols\n",
    "feature_names_cat = list(preprocess.named_transformers_['cat'].get_feature_names_out(cat_cols))\n",
    "feature_names_all = feature_names_num + feature_names_cat\n",
    "\n",
    "# Split into train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ff634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the torch tensor\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1,1)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "if (y_train_t.ndim == 1) or (y_train_t.shape[1] == 1):\n",
    "    y_train_t = torch.column_stack((1 - y_train_t, y_train_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train/test dataloader\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec773ac",
   "metadata": {},
   "source": [
    "## Scikit Learn Random Forest Model & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e17cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "y_pred = rf.predict(X_test)\n",
    "#y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(\"ROC-AUC:\", round(roc_auc_score(y_test, y_prob), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58407f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rf.predict(X_test)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64588008",
   "metadata": {},
   "source": [
    "### SHAP Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16df35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\n",
    "explainer = shap.Explainer(rf, feature_names=feature_names_all)\n",
    "shap_values = explainer(X_test[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f92d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d86414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first prediction's explanation\n",
    "shap.plots.waterfall(shap_values[0, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first prediction's explanation with a force plot\n",
    "shap.plots.force(shap_values[0, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ac2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all the training set predictions\n",
    "shap.plots.force(shap_values[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ffbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "shap.plots.beeswarm(shap_values[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a946f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values[:, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d51b83",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbd30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "\n",
    "# def predict_proba_raw(X_raw_df):\n",
    "#     Xp = preprocess.transform(X_raw_df)\n",
    "#     return rf.predict_proba(Xp)\n",
    "\n",
    "\n",
    "explainer_lime = LimeTabularExplainer(\n",
    "    training_data=X_train,\n",
    "    feature_names=feature_names_all,\n",
    "    class_names=['<=50K','>50K'],\n",
    "    categorical_features=feature_names_cat,\n",
    "    discretize_continuous=True,\n",
    "    random_state=SEED)\n",
    "\n",
    "idx = 0\n",
    "x_raw = X_test[idx]\n",
    "exp = explainer_lime.explain_instance(\n",
    "    data_row=np.array(x_raw),\n",
    "    predict_fn=rf.predict_proba,\n",
    "    #predict_fn=lambda Xarr: predict_proba_raw(pd.DataFrame(Xarr, columns=X_train.columns)),\n",
    "    num_features=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e764558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility shim for LIME + modern IPython\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "html = exp.as_html()\n",
    "display(HTML(html))\n",
    "\n",
    "# # monkey-patch only if missing\n",
    "# if not hasattr(_icd, \"display\"):\n",
    "#     _icd.display = display\n",
    "# if not hasattr(_icd, \"HTML\"):\n",
    "#     _icd.HTML = HTML\n",
    "\n",
    "# exp.show_in_notebook(show_table=True, show_all=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079670f",
   "metadata": {},
   "source": [
    "## PyTorch MLP Model & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, n_layers, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()     \n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Input Layer (= first hidden layer)\n",
    "        layers += [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "\n",
    "        # Hidden Layers (number specified by n_layers)\n",
    "        for _ in range(n_layers -1):\n",
    "            layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU() ]\n",
    "\n",
    "        # Output Layer\n",
    "        layers += [nn.Linear(hidden_dim, output_dim)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.network(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the class attributes.\n",
    "        \"\"\"\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n_count=1):\n",
    "        \"\"\"Update the values.\n",
    "\n",
    "        Args:\n",
    "            val (float): Current value.\n",
    "            n_count (int, optional): Number of current value. Defaults to 1.\n",
    "        \"\"\"\n",
    "        self.val = val\n",
    "        self.sum += val * n_count\n",
    "        self.count += n_count\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, num_epochs, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    loss_meter = AverageMeter()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            logits = model(x_batch)\n",
    "            # print(logits.shape, y_batch.shape)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_meter.update(loss.item())\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | loss={loss_meter.avg:.3f}\")\n",
    "\n",
    "\n",
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # y_true = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in test_loader:\n",
    "            preds = model(batch_X.to(device))\n",
    "            # preds = torch.sigmoid(logits)\n",
    "            predictions.append(preds.detach().cpu().numpy())\n",
    "\n",
    "            # y_true.append(batch_y.cpu().numpy())\n",
    "\n",
    "    probas = np.concatenate(predictions)\n",
    "\n",
    "    # If binary task returns only probability for the true class, adapt it to return (N x 2)\n",
    "    if probas.shape[1] == 1:\n",
    "        probas = np.concatenate((1 - probas, probas), 1)\n",
    "\n",
    "    predictions = np.argmax(probas, axis=1)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_layers = 4\n",
    "input_dim = X_train_t.shape[1]\n",
    "hidden_dim = 47\n",
    "output_dim = 2 # number of classes\n",
    "num_epochs = 20\n",
    "\n",
    "model = MLPModel(n_layers=n_layers, input_dim=input_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=output_dim\n",
    "                ).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader=train_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        criterion=criterion, optimizer=optimizer,\n",
    "        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fef89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(model, test_loader, device)\n",
    "\n",
    "acc = (pred==y_test).mean()\n",
    "print(f\"Accuracy on the test set : {acc*100 :2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a399d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t = torch.from_numpy(pred).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da412d",
   "metadata": {},
   "source": [
    "## Captum Attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to print importances and visualize distribution\n",
    "def visualize_importances(feature_names,\n",
    "                        importances,\n",
    "                        title=\"Average Feature Importances\",\n",
    "                        plot=True,\n",
    "                        axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    for i in range(len(feature_names)):\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.bar(x_pos, importances, align='center')\n",
    "        plt.xticks(x_pos, feature_names, wrap=True, rotation=45)\n",
    "        plt.xlabel(axis_title)\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "def beeswarm_attributions(\n",
    "    attrs,                 # np.array [n_samples, n_features], signed attributions\n",
    "    X,                     # np.array [n_samples, n_features], original (preprocessed) feature values\n",
    "    feature_names,         # list[str] length n_features\n",
    "    max_display=20,        # how many features to show (top by mean |attr|)\n",
    "    color_by=\"feature\",    # \"feature\" (feature values) or \"attr\" (attribution values) or None\n",
    "    cmap=None,             # e.g., \"coolwarm\" or \"viridis\"; if None uses Matplotlib default\n",
    "    jitter=0.25,           # vertical jitter scale\n",
    "    dot_size=8,            # marker size\n",
    "    title=\"Beeswarm of Attributions\",\n",
    "    xlabel=\"Attribution (signed)\",\n",
    "):\n",
    "    attrs = np.asarray(attrs)\n",
    "    X = np.asarray(X)\n",
    "    assert attrs.shape == X.shape, \"attrs and X must have same shape [n_samples, n_features]\"\n",
    "    n_samples, n_features = attrs.shape\n",
    "    feature_names = list(feature_names)\n",
    "\n",
    "    # Rank features by mean absolute attribution\n",
    "    mean_abs = np.mean(np.abs(attrs), axis=0)\n",
    "    order = np.argsort(-mean_abs)[:max_display]\n",
    "    attrs_sub = attrs[:, order]\n",
    "    X_sub = X[:, order]\n",
    "    names_sub = [feature_names[i] for i in order]\n",
    "\n",
    "    # Prepare figure\n",
    "    plt.figure(figsize=(10, 0.4 * len(names_sub) + 2))\n",
    "    y_base = np.arange(len(names_sub))  # one row per feature (top is most important)\n",
    "    y_plot_positions = []\n",
    "\n",
    "    # Normalize color reference per-feature (like SHAP)\n",
    "    def normalize_col(v):\n",
    "        v = v.astype(float)\n",
    "        vmin, vmax = np.nanmin(v), np.nanmax(v)\n",
    "        if vmax == vmin:\n",
    "            return np.zeros_like(v)  # flat color if constant\n",
    "        return (v - vmin) / (vmax - vmin)\n",
    "\n",
    "    for j, (a_col, x_col) in enumerate(zip(attrs_sub.T, X_sub.T)):\n",
    "        # Jitter to avoid overplotting; more points near 0 should stack, not overlap\n",
    "        # Use rank-based spread to get a “swarm” feel\n",
    "        # We place points around y = (len(names_sub)-1 - j) so most important is at top\n",
    "        y0 = (len(names_sub) - 1 - j)\n",
    "        # Create a small symmetric jitter using ranks of attribution values\n",
    "        ranks = a_col.argsort().argsort()  # 0..n-1 ranks\n",
    "        # Center ranks around 0 and scale\n",
    "        jitter_offsets = (ranks - np.median(ranks)) / (np.max(ranks) + 1e-9)\n",
    "        y_vals = y0 + jitter * jitter_offsets\n",
    "        y_plot_positions.append(y0)\n",
    "\n",
    "        # Colors\n",
    "        if color_by == \"feature\":\n",
    "            cvals = normalize_col(x_col)\n",
    "            sc = plt.scatter(a_col, y_vals, s=dot_size, c=cvals, cmap=cmap, alpha=0.8, edgecolors='none')\n",
    "        elif color_by == \"attr\":\n",
    "            cvals = normalize_col(a_col)\n",
    "            sc = plt.scatter(a_col, y_vals, s=dot_size, c=cvals, cmap=cmap, alpha=0.8, edgecolors='none')\n",
    "        else:\n",
    "            sc = plt.scatter(a_col, y_vals, s=dot_size, alpha=0.8, edgecolors='none')\n",
    "\n",
    "    # Axes & labels\n",
    "    plt.yticks(np.arange(len(names_sub)), names_sub)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(axis='x', linestyle=':', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Optional colorbar\n",
    "    if color_by in (\"feature\", \"attr\") and cmap is not None:\n",
    "        cbar = plt.colorbar(sc, pad=0.01)\n",
    "        cbar.set_label(\"Feature value\" if color_by == \"feature\" else \"Attribution (normalized)\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0111afc",
   "metadata": {},
   "source": [
    "### Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42248c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_saliency = Saliency(model)\n",
    "\n",
    "attr = xai_saliency.attribute(X_test_t.requires_grad_(True).to(device),\n",
    "                              target=y_pred_t,\n",
    "                              abs=False\n",
    "                              )  # binary logit\n",
    "attr = attr.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b8afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the attributions (because Captum just return the gradients)\n",
    "\n",
    "eps = 1e-16\n",
    "denom = np.max(np.abs(attr), axis=0) + eps     # shape [n_features]\n",
    "attr_norm = attr / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d09b9a",
   "metadata": {},
   "source": [
    "#### Generate and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional, Sequence, Tuple\n",
    "\n",
    "def tabular_baseline_replacement_by_indices(\n",
    "    arr: np.ndarray,                  # shape (N, F)\n",
    "    indices: np.ndarray,              # shape (N, K) indices to replace per sample\n",
    "    *,\n",
    "    baselines: np.ndarray,            # shape (F,) per-feature baseline values\n",
    "    ordinal_idx: Optional[Sequence[int]] = None,  # indices of ordinal features\n",
    "    clip_bounds: Optional[Tuple[np.ndarray, np.ndarray]] = None,  # (mins, maxs), each shape (F,)\n",
    "    round_ordinals: bool = True,\n",
    "    perturb_baseline=None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Replace selected features with feature-wise baselines.\n",
    "    Optionally round ordinal features to integer codes and clip to valid ranges.\n",
    "    \"\"\"\n",
    "    out = arr.copy()\n",
    "    N, F = out.shape\n",
    "    if ordinal_idx is None:\n",
    "        ordinal_idx = []\n",
    "    ordinal_idx = np.asarray(ordinal_idx, dtype=int)\n",
    "\n",
    "    for i in range(N):\n",
    "        js = indices[i]  # features to replace for sample i\n",
    "        out[i, js] = baselines[js]\n",
    "\n",
    "        if len(ordinal_idx) and round_ordinals:\n",
    "            # round only the ordinal columns that were touched\n",
    "            touched_ord = np.intersect1d(js, ordinal_idx, assume_unique=False)\n",
    "            if touched_ord.size:\n",
    "                out[i, touched_ord] = np.rint(out[i, touched_ord])\n",
    "\n",
    "        if clip_bounds is not None:\n",
    "            mins, maxs = clip_bounds\n",
    "            out[i] = np.minimum(np.maximum(out[i], mins), maxs)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "cat_cols_idx = X_df.columns.get_indexer(cat_cols)\n",
    "\n",
    "# Continuous → mean; Ordinal → median (then round)\n",
    "means = X_train[:, ~cat_cols_idx].mean(axis=0)\n",
    "meds  = np.median(X_train[:, cat_cols_idx], axis=0)\n",
    "\n",
    "baselines = np.empty(X_train.shape[1], dtype=float)\n",
    "baselines[~cat_cols_idx] = means\n",
    "baselines[ cat_cols_idx] = np.rint(meds)  # integer code for ordinal\n",
    "\n",
    "ordinal_idx = np.where(cat_cols_idx)[0]\n",
    "\n",
    "# Optional (but helpful): per-feature clip bounds from train set\n",
    "mins = X_train.min(axis=0)\n",
    "maxs = X_train.max(axis=0)\n",
    "clip_bounds = (mins, maxs)\n",
    "\n",
    "metric = quantus.FaithfulnessEstimate(\n",
    "    features_in_step=1,           # ↑ → faster, ↓ → more resolution\n",
    "    abs=False, normalise=False,   # start simple for tabular\n",
    "    perturb_func=tabular_baseline_replacement_by_indices,\n",
    "    perturb_func_kwargs={\n",
    "        \"baselines\": baselines,\n",
    "        \"ordinal_idx\": ordinal_idx,\n",
    "        \"clip_bounds\": clip_bounds,\n",
    "        \"round_ordinals\": True,\n",
    "    },\n",
    "    similarity_func=None,         # default Pearson is fine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4fe9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import quantus\n",
    "from captum.attr import Saliency\n",
    "\n",
    "# --- 1. Define your model\n",
    "model.eval().to(device)\n",
    "\n",
    "# --- 2. Wrap Captum's Saliency into a callable\n",
    "xai_saliency = Saliency(model)\n",
    "\n",
    "def saliency_explainer(model, inputs, targets, **kwargs):\n",
    "    \"\"\"\n",
    "    Expected signature for custom explain functions in quantus.evaluate.\n",
    "    Must return np.ndarray of same shape as inputs.\n",
    "    \"\"\"\n",
    "    # Convert numpy -> torch\n",
    "    x_t = torch.tensor(inputs, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    y_t = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "    # Compute attributions (Captum expects tensor inputs)\n",
    "    attributions = xai_saliency.attribute(x_t, target=y_t, abs=False)\n",
    "    return attributions.detach().cpu().numpy()\n",
    "\n",
    "# --- 3. Metric and config\n",
    "metrics = {\n",
    "    \"RIS\": quantus.RelativeInputStability(nr_samples=5),\n",
    "    \"ROS\": quantus.RelativeOutputStability(nr_samples=5),\n",
    "    \"Consistency\": quantus.Consistency(discretise_func=quantus.functions.discretise_func.top_n_sign,\n",
    "                                       return_aggregate=False,),\n",
    "    \"Sufficiency\": quantus.Sufficiency(threshold=0.6,\n",
    "                                       return_aggregate=False,\n",
    "                                        ),\n",
    "    \"Faithfulness\": quantus.FaithfulnessEstimate(abs=False,\n",
    "                                                 normalise=False,\n",
    "                                                features_in_step=1,  \n",
    "                                                perturb_baseline=\"mean\",\n",
    "                                                ),\n",
    "}\n",
    "\n",
    "xai_methods = {\n",
    "    \"Saliency\": saliency_explainer,  \n",
    "}\n",
    "\n",
    "# explain_func_kwargs → required (even if empty)\n",
    "explain_func_kwargs = {}\n",
    "\n",
    "# call_kwargs for the metric\n",
    "call_kwargs = {\"run\": {\"device\": device}}\n",
    "\n",
    "# --- 4. Evaluate\n",
    "results = quantus.evaluate(\n",
    "    metrics=metrics,\n",
    "    xai_methods=xai_methods,\n",
    "    model=model,\n",
    "    x_batch=X_test[:100],  # numpy array\n",
    "    y_batch=y_test[:100],  # numpy array\n",
    "    #agg_func=np.mean,\n",
    "    explain_func_kwargs=explain_func_kwargs,\n",
    "    call_kwargs=call_kwargs,\n",
    "    #return_as_df=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importances(feature_names=feature_names_all,\n",
    "                      importances=np.mean(attr_norm, axis=0)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeswarm_attributions(attrs=attr_norm,\n",
    "                      X=X_test,\n",
    "                      feature_names=feature_names_all,\n",
    "                      cmap=\"coolwarm\",\n",
    "                      color_by='attr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148dfee",
   "metadata": {},
   "source": [
    "#### Compute explanations metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantus.AVAILABLE_METRICS['Faithfulness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_wrapper(model: nn.Module,\n",
    "                     inputs: np.ndarray,\n",
    "                     targets:np.ndarray, **kwargs\n",
    "                     ) -> np.ndarray:\n",
    "\n",
    "    # Convert numpy -> torch\n",
    "    x_t = torch.tensor(inputs, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    y_t = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "    # Compute attributions (Captum expects tensor inputs)\n",
    "    attributions = xai_saliency.attribute(x_t, target=y_t, abs=False)\n",
    "    return attributions.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7761b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantus_metrics_multi(model: nn.Module,\n",
    "                              xai_wrappers: List[Callable],\n",
    "                              xai_names: List[str],\n",
    "                              x_data: np.ndarray,\n",
    "                              y_data: np.ndarray,\n",
    "                              device: torch.device,\n",
    "                              verbose: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluates a model's XAI methods against a set of Quantus metrics.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model (nn.Module).\n",
    "        xai_wrappers: A list of Callable XAI explanation functions/wrappers.\n",
    "        xai_names: A list of strings corresponding to the names of the XAI methods.\n",
    "                   Must be the same length as xai_wrappers.\n",
    "        x_data: Input data batch (np.ndarray).\n",
    "        y_data: Target label batch (np.ndarray).\n",
    "        verbose: If True, prints evaluation details.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the evaluation results from quantus.evaluate.\n",
    "    \"\"\"\n",
    "    if len(xai_wrappers) != len(xai_names):\n",
    "        raise ValueError(\"The length of 'xai_wrappers' must match the length of 'xai_names'.\")\n",
    "\n",
    "    # Ensure model is in evaluation mode and on the correct device\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # Define Quantus metrics\n",
    "    metrics = {\n",
    "        \"RIS\": quantus.RelativeInputStability(nr_samples=5),\n",
    "        \"ROS\": quantus.RelativeOutputStability(nr_samples=5),\n",
    "        \"Consistency\": quantus.Consistency(discretise_func=quantus.functions.discretise_func.top_n_sign,\n",
    "                                           return_aggregate=False),\n",
    "        \"Sufficiency\": quantus.Sufficiency(threshold=0.6,\n",
    "                                           return_aggregate=False),\n",
    "        \"Faithfulness\": quantus.FaithfulnessEstimate(abs=False,\n",
    "                                                     normalise=False,\n",
    "                                                     features_in_step=1,  \n",
    "                                                     perturb_baseline=\"mean\"),\n",
    "    }\n",
    "    \n",
    "    # Construct the XAI methods dictionary\n",
    "    # This uses a dictionary comprehension to map names to functions\n",
    "    xai_methods = dict(zip(xai_names, xai_wrappers))\n",
    "\n",
    "    # Quantus config \n",
    "    # explain_func_kwargs → required (even if empty)\n",
    "    explain_func_kwargs = {}\n",
    "\n",
    "    # call_kwargs for the metric\n",
    "    call_kwargs = {\"run\": {\"device\": device}}\n",
    "\n",
    "    # Evaluate\n",
    "    results = quantus.evaluate(\n",
    "        metrics=metrics,\n",
    "        xai_methods=xai_methods,\n",
    "        model=model,\n",
    "        x_batch=x_data,\n",
    "        y_batch=y_data,\n",
    "        explain_func_kwargs=explain_func_kwargs,\n",
    "        call_kwargs=call_kwargs,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fcfe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_results(results: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts metric data from the first set of results, cleans it by\n",
    "    removing NaN values, and returns the cleaned data as a dictionary\n",
    "    of NumPy arrays.\n",
    "\n",
    "    Args:\n",
    "        results: A dictionary where the values are metric dictionaries\n",
    "                 (e.g., {'run_1': {'metric_a': [1, 2, np.nan], ...}}).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping metric names to their cleaned 1D NumPy arrays.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return {}\n",
    "\n",
    "    # Get the metric dictionary from the first result set\n",
    "    first_metrics_data = next(iter(results.values()))\n",
    "\n",
    "    # Process each metric in the first set\n",
    "    cleaned_metrics = {}\n",
    "    for metric_name, values_list in first_metrics_data.items():\n",
    "        metric_array = np.array(values_list, dtype=np.float64)\n",
    "\n",
    "        # Remove NaN values using Boolean indexing\n",
    "        #cleaned_array = metric_array[~np.isnan(metric_array)]\n",
    "        \n",
    "        #cleaned_metrics[metric_name] = cleaned_array\n",
    "        cleaned_metrics[metric_name] = metric_array\n",
    "    \n",
    "    return cleaned_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb631329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr._utils.attribution import GradientAttribution\n",
    "from captum.metrics import sensitivity_max, infidelity\n",
    "\n",
    "def get_captum_metrics(captum_attribution: GradientAttribution,\n",
    "                       inputs: torch.Tensor,\n",
    "                       target: torch.Tensor,\n",
    "                       attribution: torch.Tensor,\n",
    "                       device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Get Captum Sensitivity and Infidelity metrics.\n",
    "\n",
    "    Args:\n",
    "        captum_attribution (GradientAttribution): Captum attribution object.\n",
    "        inputs (torch.Tensor): Data features from which the attribution are computed.\n",
    "        target (torch.Tensor): Target predictions.\n",
    "        attribution (torch.Tensor): Captum attributions.\n",
    "        device (torch.device): Torch device.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: Sensitivity and Infidelity values.\n",
    "    \"\"\"\n",
    "\n",
    "    def perturb_fn(inputs):\n",
    "        noise = torch.tensor(np.random.normal(0, 0.003, inputs.shape)).to(inputs.device).float()\n",
    "        return noise, inputs - noise\n",
    "    \n",
    "    sens = sensitivity_max(captum_attribution.attribute,\n",
    "                           inputs.to(device),\n",
    "                           target=target.to(device)\n",
    "                           ).detach().cpu().numpy()\n",
    "\n",
    "    # Computes infidelity score for saliency maps\n",
    "    infid = infidelity(model, perturb_fn,\n",
    "                       inputs=inputs.to(device),\n",
    "                       attributions=attribution.to(device),\n",
    "                       target=target.to(device)\n",
    "                       ).detach().cpu().numpy()\n",
    "    return sens, infid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e64aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantus_metrics = get_quantus_metrics_multi(model=model,\n",
    "                                            xai_wrappers=[saliency_wrapper],\n",
    "                                            xai_names=[\"saliency\"],\n",
    "                                            x_data=X_test,\n",
    "                                            y_data=pred, # The target\n",
    "                                            device=device,\n",
    "                                            verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_metrics = get_captum_metrics(captum_attribution=xai_saliency,\n",
    "                                    inputs=X_test_t,\n",
    "                                    target=y_pred_t,\n",
    "                                    attribution=torch.from_numpy(attr),\n",
    "                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = get_metrics_results(quantus_metrics)\n",
    "all_metrics[\"Sensitivity\"] = captum_metrics[0]\n",
    "all_metrics[\"Infidelity\"] = captum_metrics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e81438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beware data may contains nan values as RIS/ROS metrics may not always give a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e3929",
   "metadata": {},
   "source": [
    "### SmoothGrad (NoiseTunnel over Saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cfef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_sg = NoiseTunnel(xai_saliency)\n",
    "attr = xai_sg.attribute(X_test_t.requires_grad_(True).to(device),\n",
    "                        target=y_pred_t,\n",
    "                        nt_type='smoothgrad',\n",
    "                        stdevs=0.1,\n",
    "                        nt_samples=10, # Lower this to reduce computational time\n",
    "                        abs=False)\n",
    "\n",
    "attr = attr.detach().cpu().numpy()\n",
    "\n",
    "# Normalize the attributions (because Captum just return the gradients)\n",
    "\n",
    "eps = 1e-16\n",
    "denom = np.max(np.abs(attr), axis=0) + eps     # shape [n_features]\n",
    "attr_norm = attr / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9638623",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importances(feature_names=feature_names_all,\n",
    "                      importances=np.mean(attr_norm, axis=0)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeswarm_attributions(attrs=attr_norm,\n",
    "                      X=X_test,\n",
    "                      feature_names=feature_names_all,\n",
    "                      cmap=\"coolwarm\",\n",
    "                      color_by='attr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852ae90",
   "metadata": {},
   "source": [
    "#### Explanation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68dc997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothgrad_wrapper(model: nn.Module,\n",
    "                     inputs: np.ndarray,\n",
    "                     targets:np.ndarray,\n",
    "                     **kwargs\n",
    "                     ) -> np.ndarray:\n",
    "\n",
    "    # Convert numpy -> torch\n",
    "    x_t = torch.tensor(inputs, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    y_t = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "    # Compute attributions (Captum expects tensor inputs)\n",
    "    attributions = xai_sg.attribute(x_t,\n",
    "                        target=y_t,\n",
    "                        nt_type='smoothgrad',\n",
    "                        stdevs=0.1,\n",
    "                        nt_samples=10, # Lower this to reduce computational time\n",
    "                        abs=False)\n",
    "\n",
    "    return attributions.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44495d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantus_metrics = get_quantus_metrics_multi(model=model,\n",
    "                                            xai_wrappers=[smoothgrad_wrapper],\n",
    "                                            xai_names=[\"smoothgrad\"],\n",
    "                                            x_data=X_test[:10],\n",
    "                                            y_data=pred[:10], # The target\n",
    "                                            device=device,\n",
    "                                            verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_metrics = get_captum_metrics(captum_attribution=xai_sg,\n",
    "                                    inputs=X_test_t,\n",
    "                                    target=y_pred_t,\n",
    "                                    attribution=torch.from_numpy(attr),\n",
    "                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa74cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = get_metrics_results(quantus_metrics)\n",
    "all_metrics[\"Sensitivity\"] = captum_metrics[0]\n",
    "all_metrics[\"Infidelity\"] = captum_metrics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55301824",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66743ce2",
   "metadata": {},
   "source": [
    "### Input x Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_ig = InputXGradient(model)\n",
    "\n",
    "attr = xai_ig.attribute(X_test_t.requires_grad_(True).to(device),\n",
    "                        target=y_pred_t,\n",
    "                        )  # binary logit\n",
    "attr = attr.detach().cpu().numpy()\n",
    "\n",
    "# Normalize the attributions (because Captum just return the gradients)\n",
    "\n",
    "eps = 1e-16\n",
    "denom = np.max(np.abs(attr), axis=0) + eps     # shape [n_features]\n",
    "attr_norm = attr / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97626a7",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importances(feature_names=feature_names_all,\n",
    "                      importances=np.mean(attr_norm, axis=0)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeswarm_attributions(attrs=attr_norm,\n",
    "                      X=X_test,\n",
    "                      feature_names=feature_names_all,\n",
    "                      cmap=\"coolwarm\",\n",
    "                      color_by='attr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb1539",
   "metadata": {},
   "source": [
    "#### Explanation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc76022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inp_grad_wrapper(model, inputs, targets, **kwargs):\n",
    "    # Convert numpy -> torch\n",
    "    x_t = torch.tensor(inputs, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    y_t = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "    attributions = xai_ig.attribute(x_t,\n",
    "                                    target=y_t,\n",
    "                                    )\n",
    "    return attributions.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02853e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantus_metrics = get_quantus_metrics_multi(model=model,\n",
    "                                            xai_wrappers=[inp_grad_wrapper],\n",
    "                                            xai_names=[\"inputgrad\"],\n",
    "                                            x_data=X_test[:10],\n",
    "                                            y_data=pred[:10], # The target\n",
    "                                            device=device,\n",
    "                                            verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_metrics = get_captum_metrics(captum_attribution=xai_ig,\n",
    "                                    inputs=X_test_t[:10],\n",
    "                                    target=y_pred_t[:10],\n",
    "                                    attribution=torch.from_numpy(attr)[:10],\n",
    "                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b26291",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = get_metrics_results(quantus_metrics)\n",
    "all_metrics[\"Sensitivity\"] = captum_metrics[0]\n",
    "all_metrics[\"Infidelity\"] = captum_metrics[1]\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123c7c4",
   "metadata": {},
   "source": [
    "### Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_int_grad = IntegratedGradients(model)\n",
    "baseline = X_train_t.mean(dim=0) # Take on input (as the mean of the training set)\n",
    "baselines = baseline.repeat(X_test_t.shape[0], 1).to(device) # Cast to the shape of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = xai_int_grad.attribute(X_test_t.requires_grad_(True).to(device),\n",
    "                        target=y_pred_t,\n",
    "                        baselines=baselines,\n",
    "                        n_steps=50\n",
    "                        )\n",
    "attr = attr.detach().cpu().numpy()\n",
    "\n",
    "# Normalize the attributions (because Captum just return the gradients)\n",
    "eps = 1e-16\n",
    "denom = np.max(np.abs(attr), axis=0) + eps     # shape [n_features]\n",
    "attr_norm = attr / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38257b1",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importances(feature_names=feature_names_all,\n",
    "                      importances=np.mean(attr_norm, axis=0)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeswarm_attributions(attrs=attr_norm,\n",
    "                      X=X_test,\n",
    "                      feature_names=feature_names_all,\n",
    "                      cmap=\"coolwarm\",\n",
    "                      color_by='attr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8f345",
   "metadata": {},
   "source": [
    "#### Explanation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intgrad_wrapper(model, inputs, targets, **kwargs):\n",
    "\n",
    "    # Convert numpy -> torch\n",
    "    x_t = torch.tensor(inputs, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    y_t = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "    baseline = X_train_t.mean(dim=0)\n",
    "    baselines = baseline.repeat(x_t.shape[0], 1).to(device) # Cast to the shape of the test set\n",
    "\n",
    "    attributions = xai_int_grad.attribute(x_t,\n",
    "                        target=y_t,\n",
    "                        baselines=baselines,\n",
    "                        n_steps=50\n",
    "                        )\n",
    "    return attributions.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaef2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantus_metrics = get_quantus_metrics_multi(model=model,\n",
    "                                            xai_wrappers=[intgrad_wrapper],\n",
    "                                            xai_names=[\"integrated_gradients\"],\n",
    "                                            x_data=X_test[:10],\n",
    "                                            y_data=pred[:10], # The target\n",
    "                                            device=device,\n",
    "                                            verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31056723",
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_metrics = get_captum_metrics(captum_attribution=xai_int_grad,\n",
    "                                    inputs=X_test_t[:10],\n",
    "                                    target=y_pred_t[:10],\n",
    "                                    attribution=torch.from_numpy(attr)[:10],\n",
    "                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17555a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = get_metrics_results(quantus_metrics)\n",
    "all_metrics[\"Sensitivity\"] = captum_metrics[0]\n",
    "all_metrics[\"Infidelity\"] = captum_metrics[1]\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada00c5",
   "metadata": {},
   "source": [
    "### LRP (Layer-wise Relevance Propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.composites import EpsilonPlus\n",
    "\n",
    "\n",
    "# create a composite instance\n",
    "composite = EpsilonPlus()\n",
    "\n",
    "# use the following instead to ignore bias for the relevance\n",
    "# composite = EpsilonPlusFlat(zero_params='bias')\n",
    "\n",
    "# make sure the input requires a gradien\n",
    "\n",
    "xai_lrp_grad = Gradient(model, composite)\n",
    "\n",
    "targets = torch.nn.functional.one_hot(y_pred_t, num_classes=2).float()\n",
    "\n",
    "with xai_lrp_grad:\n",
    "     # gradient/ relevance wrt. output/class 1\n",
    "     output, attr = xai_lrp_grad(X_test_t.requires_grad_(True).to(device),\n",
    "                                 targets\n",
    "                                 )\n",
    "\n",
    "attr = attr.detach().cpu().numpy()\n",
    "\n",
    "# Normalize the attributions (because Captum just return the gradients)\n",
    "\n",
    "eps = 1e-16\n",
    "denom = np.max(np.abs(attr), axis=0) + eps     # shape [n_features]\n",
    "attr_norm = attr / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3bd31",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ac9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_importances(feature_names=feature_names_all,\n",
    "                      importances=np.mean(attr_norm, axis=0)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45838797",
   "metadata": {},
   "outputs": [],
   "source": [
    "beeswarm_attributions(attrs=attr_norm,\n",
    "                      X=X_test,\n",
    "                      feature_names=feature_names_all,\n",
    "                      cmap=\"coolwarm\",\n",
    "                      color_by='attr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e6588",
   "metadata": {},
   "source": [
    "#### Explanation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp_wrapper(model, inputs, targets, **kwargs):\n",
    "    # Convert numpy -> torch\n",
    "    x_t = torch.tensor(inputs, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    y_t = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "    y_t = torch.nn.functional.one_hot(y_t, num_classes=2).float()\n",
    "\n",
    "\n",
    "    with xai_lrp_grad:\n",
    "     # gradient/ relevance wrt. output/class 1\n",
    "     _, attributions = xai_lrp_grad(x_t,\n",
    "                            y_t\n",
    "                            )\n",
    "     return attributions.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantus_metrics = get_quantus_metrics_multi(model=model,\n",
    "                                            xai_wrappers=[lrp_wrapper],\n",
    "                                            xai_names=[\"lrp\"],\n",
    "                                            x_data=X_test[:10],\n",
    "                                            y_data=pred[:10], # The target\n",
    "                                            device=device,\n",
    "                                            verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cdd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "captum_metrics = get_captum_metrics(captum_attribution=xai_lrp_grad,\n",
    "                                    inputs=X_test_t,\n",
    "                                    target=y_pred_t,\n",
    "                                    attribution=torch.from_numpy(attr),\n",
    "                                    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa9a0a0",
   "metadata": {},
   "source": [
    "## Counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062bd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will attempt to minimally change continuous features to flip the prediction.\n",
    "# Categorical one-hots will be kept fixed for simplicity.\n",
    "\n",
    "# Identify indices of continuous (original numeric) features in processed vector\n",
    "num_indices = []\n",
    "for i, name in enumerate(feature_names_all):\n",
    "    if name in num_cols:\n",
    "        num_indices.append(i)\n",
    "num_indices = np.array(num_indices, dtype=int)\n",
    "\n",
    "def predict_proba(model, x):\n",
    "    with torch.no_grad():\n",
    "        p = torch.sigmoid(model(x))\n",
    "    return p\n",
    "\n",
    "def counterfactual_search(x_init, target_label=1, steps=300, lr=0.05, l2=0.01):\n",
    "    x = x_init.clone().detach().to(device)\n",
    "    x_cf = x.clone().detach().requires_grad_(True)\n",
    "    opt = torch.optim.Adam([x_cf], lr=lr)\n",
    "    target = torch.tensor([[float(target_label)]], device=device)\n",
    "    for t in range(steps):\n",
    "        opt.zero_grad()\n",
    "        logit = model(x_cf)\n",
    "        prob = torch.sigmoid(logit)\n",
    "        # BCE encouraging target\n",
    "        bce = nn.functional.binary_cross_entropy(prob, target)\n",
    "        # Proximity on numeric dims only\n",
    "        diff = (x_cf - x)\n",
    "        diff_masked = diff[:, num_indices]\n",
    "        prox = l2 * torch.sum(diff_masked**2)\n",
    "        loss = bce + prox\n",
    "        loss.backward()\n",
    "        # Project: keep categorical one-hots unchanged\n",
    "        with torch.no_grad():\n",
    "            # freeze categorical parts by copying from original\n",
    "            for i in range(x_cf.shape[1]):\n",
    "                if i not in num_indices:\n",
    "                    x_cf[:, i] = x[:, i]\n",
    "        opt.step()\n",
    "        # Early stop if flipped\n",
    "        if (target_label == 1 and prob.item() >= 0.5) or (target_label == 0 and prob.item() < 0.5):\n",
    "            break\n",
    "    return x_cf.detach(), prob.item()\n",
    "\n",
    "# Choose an instance predicted as 0 => try to flip to 1 (or vice versa)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    probs_test = torch.sigmoid(model(X_test_t.to(device))).cpu().numpy().ravel()\n",
    "preds_test = (probs_test >= 0.5).astype(int)\n",
    "idx0 = int(np.where(preds_test == 0)[0][0]) if (preds_test==0).any() else 0\n",
    "x_orig = X_test_t[idx0:idx0+1]\n",
    "y_pred_orig = preds_test[idx0]\n",
    "target_label = 1 - y_pred_orig\n",
    "x_cf, prob_after = counterfactual_search(x_orig, target_label=target_label, steps=400, lr=0.05, l2=0.01)\n",
    "\n",
    "print('Original pred:', y_pred_orig, '→ Target:', target_label, '| New prob:', round(prob_after, 3))\n",
    "\n",
    "# Show changed continuous features in original feature space (standardized units)\n",
    "delta = (x_cf - x_orig).cpu().numpy().ravel()\n",
    "changes = {name: delta[i] for i, name in enumerate(feature_names_all) if i in num_indices and abs(delta[i])>1e-6}\n",
    "changes_sorted = dict(sorted(changes.items(), key=lambda kv: abs(kv[1]), reverse=True)[:12])\n",
    "pd.DataFrame({'feature': list(changes_sorted.keys()), 'delta_std_units': list(changes_sorted.values())})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47300674",
   "metadata": {},
   "source": [
    "## Attacks on LIME and SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9aadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Adult from OpenML\n",
    "adult = fetch_openml(name='adult', version=2, as_frame=True)\n",
    "df = adult.frame.copy()\n",
    "\n",
    "# Replace '?' with NaN and drop rows with missing (simple approach)\n",
    "df = df.replace('?', np.nan).dropna()\n",
    "\n",
    "# Target is 'class': '>50K' or '<=50K' — convert to 0/1\n",
    "df['class'] = (df['class'] == '>50K').astype(int)\n",
    "\n",
    "# Add a random column -- this is what we'll have LIME/SHAP explain.\n",
    "df['unrelated_column'] = np.random.choice([0,1],size=df.shape[0])\n",
    "\n",
    "# Identify categorical vs numeric columns\n",
    "target_col = 'class'\n",
    "X_df = df.drop(columns=[target_col])\n",
    "y = df[target_col].values\n",
    "\n",
    "categorical_feature_name = X_df.select_dtypes(include=['category','object']).columns.tolist()\n",
    "categorical_feature_name += [\"unrelated_column\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93699148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing pipeline: OrdinalEncoder for categoricals, Standardize numerics\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        # ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),\n",
    "        ('cat', OrdinalEncoder(), categorical_feature_name), # We use here OrdinalEncoder to limit the number of features\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_processed = preprocess.fit_transform(X_df)\n",
    "feature_names_num = num_cols\n",
    "feature_names_cat = list(preprocess.named_transformers_['cat'].get_feature_names_out(categorical_feature_name))\n",
    "feature_names_all = feature_names_num + feature_names_cat\n",
    "\n",
    "categorical_feature_indcs = [feature_names_all.index(cat) for cat in feature_names_cat if cat in feature_names_all]\n",
    "\n",
    "\n",
    "# Split into train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_indc = feature_names_all.index('sex')\n",
    "unrelated_indcs = feature_names_all.index('unrelated_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_outcome = 0\n",
    "positive_outcome = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    \"\"\" One hot encode y for binary features.  We use this to get from 1 dim ys to predict proba's.\n",
    "    This is taken from this s.o. post: https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    A np.ndarray of the one hot encoded data.\n",
    "    \"\"\"\n",
    "    y_hat_one_hot = np.zeros((len(y), 2))\n",
    "    y_hat_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_hat_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7841f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "enc = preprocess.named_transformers_['cat']\n",
    "for j, col in enumerate(categorical_feature_name):\n",
    "    for i, cat in enumerate(enc.categories_[j]):\n",
    "        rows.append({'column': col, 'category': cat, 'code': i})\n",
    "    if enc.handle_unknown == 'use_encoded_value':\n",
    "        rows.append({'column': col, 'category': '<UNKNOWN>', 'code': enc.unknown_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f63685",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa686eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sexist_model_f:\n",
    "    # Decision rule: classify negatively if race is black\n",
    "    def predict(self, X):\n",
    "        return np.array([negative_outcome if x[sex_indc] == 0 else positive_outcome for x in X])\n",
    "\n",
    "    def predict_proba(self, X): \n",
    "        return one_hot_encode(self.predict(X))\n",
    "\n",
    "    def score(self, X,y):\n",
    "        return np.sum(self.predict(X)==y) / len(X)\n",
    "    \n",
    "class innocuous_model_psi:\n",
    "    # Decision rule: classify according to randomly drawn column 'unrelated column'\n",
    "    def predict(self,X):\n",
    "        return np.array([negative_outcome if x[unrelated_indcs] > 0 else positive_outcome for x in X])\n",
    "\n",
    "    def predict_proba(self, X): \n",
    "        return one_hot_encode(self.predict(X))\n",
    "\n",
    "    def score(self, X,y):\n",
    "        return np.sum(self.predict(X)==y) / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb28ab",
   "metadata": {},
   "source": [
    "### Fooling LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe4887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fooling_lime_shap import Adversarial_Lime_Model\n",
    "\n",
    "# Train the adversarial model for LIME with f and psi \n",
    "adv_lime = Adversarial_Lime_Model(sexist_model_f(), innocuous_model_psi()).\\\n",
    "            train(X_train[:100], y_train[:100], feature_names=feature_names_all, categorical_features=categorical_feature_indcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_indc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "\n",
    "# Let's just look at a the first example in the test set\n",
    "ex_indc = np.random.choice(X_test.shape[0])\n",
    "\n",
    "# To get a baseline, we'll look at LIME applied to the biased model f\n",
    "normal_explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=adv_lime.get_column_names(),\n",
    "                                                          discretize_continuous=False,\n",
    "                                                          categorical_features=categorical_feature_indcs)\n",
    "\n",
    "normal_exp = normal_explainer.explain_instance(X_test[ex_indc], sexist_model_f().predict_proba)\n",
    "\n",
    "# print (\"Explanation on biased f:\\n\",normal_exp[:3],\"\\n\\n\")\n",
    "\n",
    "# Now, lets look at the explanations on the adversarial model \n",
    "adv_explainer = lime.lime_tabular.LimeTabularExplainer(X_train,feature_names=adv_lime.get_column_names(), \n",
    "                                                       discretize_continuous=False,\n",
    "                                                       categorical_features=categorical_feature_indcs)\n",
    "\n",
    "adv_exp = adv_explainer.explain_instance(X_test[ex_indc], adv_lime.predict_proba)\n",
    "\n",
    "# print (\"Explanation on adversarial model:\\n\",adv_exp[:3],\"\\n\")\n",
    "\n",
    "# print(\"Prediction fidelity: {0:3.2}\".format(adv_lime.fidelity(X_test[ex_indc:ex_indc+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89afcfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "html = normal_exp.as_html()\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = adv_exp.as_html()\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3f326",
   "metadata": {},
   "source": [
    "### Fooling SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f07cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fooling_lime_shap import Adversarial_Kernel_SHAP_Model\n",
    "\n",
    "\n",
    "# Train the adversarial model\n",
    "adv_shap = Adversarial_Kernel_SHAP_Model(sexist_model_f(), innocuous_model_psi()).\\\n",
    "            train(X_train[:100], y_train[:100], feature_names=feature_names_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Set the background distribution for the shap explainer using kmeans\n",
    "background_distribution = shap.kmeans(X_train, 100)\n",
    "\n",
    "# Let's use the shap kernel explainer and grab a point to explain\n",
    "to_examine = np.random.choice(X_test.shape[0])\n",
    "\n",
    "# Explain the biased model\n",
    "biased_kernel_explainer = shap.KernelExplainer(sexist_model_f().predict, background_distribution)\n",
    "biased_shap_values = biased_kernel_explainer.shap_values(X_test[to_examine:to_examine+1])\n",
    "\n",
    "# Explain the adversarial model\n",
    "adv_kerenel_explainer = shap.KernelExplainer(adv_shap.predict, background_distribution)\n",
    "adv_shap_values = adv_kerenel_explainer.shap_values(X_test[to_examine:to_examine+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea0bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it using SHAP's plotting features.\n",
    "shap.summary_plot(biased_shap_values, feature_names=feature_names_all, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(adv_shap_values, feature_names=feature_names_all, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a8b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0x_xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
