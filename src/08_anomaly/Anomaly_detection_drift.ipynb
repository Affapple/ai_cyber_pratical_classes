{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b4c7c136ff9480",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "id": "16b4c7c136ff9480"
   },
   "source": [
    "# Practical Session: Anomaly Detection and Distribution Drifts\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter notebook provides a comprehensive, hands-on practical session on anomaly detection and distribution drifts. The content is structured into two main sections: **Anomaly Detection** and **Distribution Drift**. Using real-world datasets and popular Python libraries (e.g., scikit-learn, TensorFlow), we'll implement, evaluate, and mitigate core concepts in these areas.\n",
    "\n",
    "### Key Learning Objectives\n",
    "- Implement and compare anomaly detection methods (statistical, distance-based, ML-based).\n",
    "- Evaluate models using imbalanced metrics (PR-AUC, ROC-AUC, F1) and visualize performance curves.\n",
    "- Detect and mitigate distribution drifts with incremental learning and statistical tests.\n",
    "- Address advanced challenges, including adversarial simulations and adaptation strategies.\n",
    "\n",
    "\n",
    "### Datasets Used\n",
    "- **Anomaly Detection**: Credit Card Fraud Detection (~284k transactions, 0.17% fraud rate—highly imbalanced).\n",
    "- **Distribution Drift**: Electricity (Elec2): Real-world stream with abrupt/gradual drifts in electricity price prediction (~45k instances)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2269473aee593",
   "metadata": {
    "id": "c9f2269473aee593"
   },
   "source": [
    "## Setup: Install and Import Libraries\n",
    "\n",
    "This cell installs required packages. Restart the kernel after installation if needed."
   ]
  },
  {
   "cell_type": "code",
   "id": "cabcb384609d9ce8",
   "metadata": {
    "id": "cabcb384609d9ce8"
   },
   "source": [
    "! pip install scikit-learn scipy pandas matplotlib seaborn tensorflow shap openml tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a85dfc4e5151be2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a85dfc4e5151be2b",
    "outputId": "b0696941-5255-4b3e-8ea2-328255c038a7"
   },
   "source": [
    "# Imports & utilities\n",
    "import os, time, math, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture # Using GMM for probabilistic anomaly detection\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import ks_2samp, wasserstein_distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Keras for autoencoder\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, models\n",
    "    TF_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "print('Tensorflow available:', TF_AVAILABLE)\n",
    "\n",
    "# Evaluation helper\n",
    "def pr_auc(y_true, scores):\n",
    "    prec, rec, _ = precision_recall_curve(y_true, scores)\n",
    "    return auc(rec, prec)\n",
    "\n",
    "def evaluate_continuous(y_true, scores, name='score'):\n",
    "    results = {}\n",
    "    results['roc_auc'] = roc_auc_score(y_true, scores)\n",
    "    results['pr_auc'] = pr_auc(y_true, scores)\n",
    "    return results\n",
    "\n",
    "def plot_score_vs_label(scores, y, title='Scores'):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(scores[y==0], bins=50, label='normal', stat='density', alpha=0.6, kde=True)\n",
    "    sns.histplot(scores[y==1], bins=50, label='anomaly', stat='density', alpha=0.6, kde=True)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Anomaly Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "\n",
    "print('Setup complete.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc9b747029ee6a52",
   "metadata": {
    "id": "dc9b747029ee6a52"
   },
   "source": [
    "## 2. Loading the Datasets\n",
    "\n",
    "This cell attempts to download datasets from OpenML. If your environment is offline or OpenML fails, place `creditcard.csv` and `electricity.csv` in the working directory and the code will load them."
   ]
  },
  {
   "cell_type": "code",
   "id": "6bfc7e155e4b4971",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bfc7e155e4b4971",
    "outputId": "2448297f-77ff-4dec-9cc0-04a8db5c8620"
   },
   "source": [
    "# Try to fetch datasets from OpenML (non-blocking: will raise if offline)\n",
    "def load_creditcard_openml():\n",
    "    try:\n",
    "        from sklearn.datasets import fetch_openml\n",
    "        print('Attempting to download Credit Card dataset from OpenML...')\n",
    "        cc = fetch_openml(data_id=1597, as_frame=True)\n",
    "        df = cc.frame  # includes 'class' or 'Class' column\n",
    "        print('Downloaded via fetch_openml (data_id=1597). Columns:', df.columns[:10].tolist())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print('OpenML download failed or unavailable:', e)\n",
    "        return None\n",
    "\n",
    "def load_electricity_openml():\n",
    "    try:\n",
    "        from sklearn.datasets import fetch_openml\n",
    "        print('Attempting to download Electricity dataset from OpenML...')\n",
    "        el = fetch_openml(name='electricity', version=1, as_frame=True)\n",
    "        df = el.frame\n",
    "        print('Downloaded electricity dataset. Columns:', df.columns[:10].tolist())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print('OpenML electricity fetch failed or unavailable:', e)\n",
    "        return None\n",
    "\n",
    "credit_df = load_creditcard_openml()\n",
    "electric_df = load_electricity_openml()\n",
    "\n",
    "# Fallback to local files if openml failed\n",
    "if credit_df is None:\n",
    "    if os.path.exists('creditcard.csv'):\n",
    "        print('Loading creditcard.csv from local directory.')\n",
    "        credit_df = pd.read_csv('creditcard.csv')\n",
    "    else:\n",
    "        print('creditcard.csv not found locally. Please download the Credit Card Fraud dataset (creditcard.csv) and place it here.')\n",
    "if electric_df is None:\n",
    "    if os.path.exists('electricity.csv'):\n",
    "        print('Loading electricity.csv from local directory.')\n",
    "        electric_df = pd.read_csv('electricity.csv')\n",
    "    else:\n",
    "        print('electricity.csv not found locally. Please provide a suitable electricity timeseries CSV (e.g., date,value columns).')\n",
    "\n",
    "# Quick checks\n",
    "print('\\nCredit df:', None if credit_df is None else credit_df.shape)\n",
    "print('Electric df:', None if electric_df is None else electric_df.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6196906a965be97f",
   "metadata": {
    "id": "6196906a965be97f"
   },
   "source": [
    "## 3. Anomaly Detection with Credit Card Data (Part A)\n",
    "\n",
    "### 3.1 Exploratory Data Analysis (EDA)\n",
    "\n",
    "If `credit_df` loaded successfully, run the next cell to perform EDA. Otherwise, provide `creditcard.csv` in the notebook folder."
   ]
  },
  {
   "cell_type": "code",
   "id": "c3fe0a15bfc7ba06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c3fe0a15bfc7ba06",
    "outputId": "941e5e3e-818a-4357-b0b3-6dcc152f384f"
   },
   "source": [
    "# Basic EDA (runs only if data loaded)\n",
    "if credit_df is None:\n",
    "    print('Credit dataset not loaded. Skipping EDA.')\n",
    "else:\n",
    "    df = credit_df.copy()\n",
    "    # Standardize common column names if needed\n",
    "    if 'Class' in df.columns:\n",
    "        df.rename(columns={'Class':'class'}, inplace=True)\n",
    "    # Inspect\n",
    "    df['class'] = pd.to_numeric(df['class'], errors='coerce')\n",
    "    display(df.head())\n",
    "    display(df.describe().T)\n",
    "    print('\\nValue counts for class (1=fraud):')\n",
    "    print(df['class'].value_counts(dropna=False))\n",
    "    # Percent fraud\n",
    "    fraud_ratio = df['class'].mean()\n",
    "    print(f'Fraud ratio: {fraud_ratio:.6f} (fraction of rows labelled 1)')\n",
    "    # Correlation heatmap (sampled for speed)\n",
    "    sample = df.sample(min(5000, len(df)), random_state=42)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(sample.select_dtypes(include=[np.number]).corr(), cmap='coolwarm', center=0, vmax=0.8)\n",
    "    plt.title('Feature correlation (sample)')\n",
    "    plt.show()\n",
    "    # PCA visualization colored by class (for numeric features)\n",
    "    numeric = sample.select_dtypes(include=[np.number]).fillna(0)\n",
    "    cols = numeric.columns.tolist()\n",
    "    Xnum = numeric[cols].values\n",
    "    Xnum = (Xnum - Xnum.mean(axis=0)) / (Xnum.std(axis=0) + 1e-9)\n",
    "    pca = PCA(n_components=2)\n",
    "    proj = pca.fit_transform(Xnum)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(proj[:,0], proj[:,1], c=sample['class'], s=10, cmap='coolwarm', alpha=0.7)\n",
    "    plt.title('PCA projection (sample) — colored by class')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4edf59fd0574be43",
   "metadata": {
    "id": "4edf59fd0574be43"
   },
   "source": [
    "### 3.2 Preprocessing & Splits\n",
    "\n",
    "We’ll prepare features for unsupervised anomaly detection. We treat the minority fraud class as ‘anomalies’ for evaluation. We scale numeric features and create train/test splits. Training for unsupervised methods will use mostly normal samples to simulate realistic scenarios."
   ]
  },
  {
   "cell_type": "code",
   "id": "7f910301bee93562",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f910301bee93562",
    "outputId": "9fe68006-ded5-4790-8fbe-becbda4912c6"
   },
   "source": [
    "if credit_df is None:\n",
    "    print('Credit dataset not loaded — cannot continue preprocessing.')\n",
    "else:\n",
    "    df = credit_df.copy()\n",
    "    if 'Class' in df.columns:\n",
    "        df.rename(columns={'Class':'class'}, inplace=True)\n",
    "    # Identify numeric columns (drop identifiers if present)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'class' in numeric_cols:\n",
    "        numeric_cols.remove('class')\n",
    "    X = df[numeric_cols].fillna(0).values\n",
    "    y = df['class'].astype(int).values  # 1 indicates fraud in the usual creditcard dataset\n",
    "    print('Numeric features:', len(numeric_cols))\n",
    "    print('Total samples:', X.shape[0], 'Positive (fraud):', y.sum())\n",
    "    # scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.4, random_state=42, stratify=y)\n",
    "    # For unsupervised training (e.g., autoencoder), keep mainly normal in train\n",
    "    X_train_norm = X_train[y_train==0]\n",
    "    print('Train normal samples:', X_train_norm.shape[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a574d09f30a9540c",
   "metadata": {
    "id": "a574d09f30a9540c"
   },
   "source": [
    "### 3.3 Comparing Anomaly Detection Methods\n",
    "\n",
    "We’ll implement and compare:\n",
    "- IsolationForest (tree-based)\n",
    "- Gaussian Mixture Model (GMM, statistical/probabilistic)\n",
    "- One-Class SVM (kernel)\n",
    "- Keras Autoencoder (deep reconstruction, if TF available)\n",
    "\n",
    "Each method will produce a continuous anomaly score and we will evaluate using ROC-AUC and PR-AUC."
   ]
  },
  {
   "cell_type": "code",
   "id": "4643110540cca5c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4643110540cca5c9",
    "outputId": "ef1f2c22-b4f5-4742-e821-84ce40e4e1f5"
   },
   "source": [
    "if credit_df is None:\n",
    "    print('Credit dataset not loaded — skipping model runs.')\n",
    "else:\n",
    "    results = {}\n",
    "    # Isolation Forest\n",
    "    print('Training IsolationForest...')\n",
    "    iso = IsolationForest(n_estimators=200, contamination=y.mean(), random_state=42, n_jobs=-1)\n",
    "    iso.fit(X_train)  # unsupervised\n",
    "    iso_scores = -iso.decision_function(X_test)  # higher -> more anomalous\n",
    "    results['IsolationForest'] = evaluate_continuous(y_test, iso_scores)\n",
    "    print('IsolationForest done.')\n",
    "\n",
    "    # GMM (Probabilistic Anomaly Detection)\n",
    "    print('Training GMM on train data...')\n",
    "    # We use 5 components, assuming the normal data distribution can be modeled by a mixture of 5 Gaussians\n",
    "    gmm = GaussianMixture(n_components=5, random_state=42)\n",
    "    gmm.fit(X_train_norm)\n",
    "    # Anomaly score is the negative log-likelihood (lower log-prob -> higher anomaly score)\n",
    "    gmm_scores = -gmm.score_samples(X_test)\n",
    "    results['GMM'] = evaluate_continuous(y_test, gmm_scores)\n",
    "    print('GMM done.')\n",
    "\n",
    "    # One-Class SVM\n",
    "    print('Training One-Class SVM...')\n",
    "    oc = OneClassSVM(kernel='rbf', gamma='scale', nu=0.0015)  # nu should be small for rare anomalies\n",
    "    oc.fit(X_train_norm)\n",
    "    oc_scores = -oc.decision_function(X_test)\n",
    "    results['OneClassSVM'] = evaluate_continuous(y_test, oc_scores)\n",
    "    print('One-Class SVM done.')\n",
    "\n",
    "    # Keras Autoencoder (if available)\n",
    "    if TF_AVAILABLE:\n",
    "        print('Training Keras autoencoder (deep reconstruction)...')\n",
    "        input_dim = X_train_norm.shape[1]\n",
    "        latent_dim = min(16, max(8, input_dim//8))\n",
    "        inp = keras.Input(shape=(input_dim,))\n",
    "        x = layers.Dense(64, activation='relu')(inp)\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        lat = layers.Dense(latent_dim, activation='relu')(x)\n",
    "        x = layers.Dense(32, activation='relu')(lat)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        out = layers.Dense(input_dim, activation='linear')(x)\n",
    "        auto = keras.Model(inp, out)\n",
    "        auto.compile(optimizer='adam', loss='mse')\n",
    "        # Train on normal data only\n",
    "        history = auto.fit(X_train_norm, X_train_norm, epochs=30, batch_size=256, validation_split=0.1, verbose=0)\n",
    "        # reconstruction errors on test\n",
    "        X_test_pred = auto.predict(X_test)\n",
    "        recon_err = np.mean(np.square(X_test - X_test_pred), axis=1)\n",
    "        results['KerasAutoencoder'] = evaluate_continuous(y_test, recon_err)\n",
    "        print('Keras autoencoder done.')\n",
    "\n",
    "    # Summarize results\n",
    "    print('\\nMethod performance summary (ROC-AUC / PR-AUC):')\n",
    "    for k,v in results.items():\n",
    "        print(f'{k}: ROC-AUC={v[\"roc_auc\"]:.4f}, PR-AUC={v[\"pr_auc\"]:.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2abe4f8bf6f3535",
   "metadata": {
    "id": "c2abe4f8bf6f3535"
   },
   "source": [
    "### 3.4 Visualize Scores & Precision-Recall Curves\n",
    "\n",
    "Plot score distributions for top methods and PR curves to compare."
   ]
  },
  {
   "cell_type": "code",
   "id": "234f600975e7bf89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "234f600975e7bf89",
    "outputId": "cf2b0654-5587-4862-ca9b-a3ca1c1e5151"
   },
   "source": [
    "if credit_df is None:\n",
    "    print('Credit dataset not loaded — skipping visualization.')\n",
    "else:\n",
    "    # Plot score distributions with improved handling for GMM\n",
    "    if 'GMM' in results:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        normal_scores = gmm_scores[y_test==0]\n",
    "        anomaly_scores = gmm_scores[y_test==1]\n",
    "\n",
    "        # Use percentile-based bins to handle outliers\n",
    "        bins = np.linspace(\n",
    "            np.percentile(gmm_scores, 1),\n",
    "            np.percentile(gmm_scores, 99),\n",
    "            50\n",
    "        )\n",
    "\n",
    "        sns.histplot(normal_scores, bins=bins, label='normal', stat='density', alpha=0.6, kde=True)\n",
    "        sns.histplot(anomaly_scores, bins=bins, label='anomaly', stat='density', alpha=0.6, kde=True)\n",
    "        plt.legend()\n",
    "        plt.title('GMM Anomaly Scores Distribution')\n",
    "        plt.xlabel('Anomaly Score (negative log-likelihood)')\n",
    "        plt.ylabel('Density')\n",
    "        plt.xlim(np.percentile(gmm_scores, 1), np.percentile(gmm_scores, 99))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if 'IsolationForest' in results:\n",
    "        plot_score_vs_label(iso_scores, y_test, title='IsolationForest Anomaly Scores Distribution')\n",
    "\n",
    "    # Plot PR curves\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # Define colors for consistency\n",
    "    colors = {'IsolationForest': '#2E86AB', 'GMM': '#A23B72',\n",
    "              'OneClassSVM': '#F18F01', 'KerasAutoencoder': '#C73E1D'}\n",
    "\n",
    "    for name, score_dict in results.items():\n",
    "        if name == 'KerasAutoencoder' and not TF_AVAILABLE:\n",
    "            continue\n",
    "        # Get scores for plotting PR curve\n",
    "        if name == 'IsolationForest':\n",
    "            scores = iso_scores\n",
    "        elif name == 'GMM':\n",
    "            scores = gmm_scores\n",
    "        elif name == 'OneClassSVM':\n",
    "            scores = oc_scores\n",
    "        elif name == 'KerasAutoencoder':\n",
    "            scores = recon_err\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_test, scores)\n",
    "        plt.plot(recall, precision,\n",
    "                label=f'{name} (AUC={score_dict[\"pr_auc\"]:.3f})',\n",
    "                linewidth=2.5,\n",
    "                color=colors.get(name, None),\n",
    "                alpha=0.8)\n",
    "\n",
    "    # Print summary table\n",
    "    print('\\n' + '='*60)\n",
    "    print('ANOMALY DETECTION PERFORMANCE SUMMARY')\n",
    "    print('='*60)\n",
    "    print(f'{\"Method\":<20} {\"ROC-AUC\":>10} {\"PR-AUC\":>10}')\n",
    "    print('-'*60)\n",
    "    for name, score_dict in sorted(results.items(), key=lambda x: x[1]['pr_auc'], reverse=True):\n",
    "        print(f'{name:<20} {score_dict[\"roc_auc\"]:>10.4f} {score_dict[\"pr_auc\"]:>10.4f}')\n",
    "    print('='*60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "19a6d2f2a0e090a8",
   "metadata": {
    "id": "19a6d2f2a0e090a8"
   },
   "source": [
    "## 4. Distribution Drift Analysis (Part B: Electricity Data)\n",
    "\n",
    "### 4.1 Data Preparation for Drift\n",
    "\n",
    "We use the electricity dataset to simulate a time-series environment where the underlying data distribution may change over time. We will focus on the `nswprice` column as the primary variable for drift detection."
   ]
  },
  {
   "cell_type": "code",
   "id": "c2578cc813a23f0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "c2578cc813a23f0b",
    "outputId": "cf2cd19a-409b-43a0-e6fb-9b23efd2c0fa"
   },
   "source": [
    "if electric_df is None:\n",
    "    print('Electricity dataset not loaded — cannot continue with drift analysis.')\n",
    "else:\n",
    "    # Clean up and prepare the electricity data\n",
    "    edf = electric_df.copy()\n",
    "    # Standardize column names\n",
    "    if 'class' in edf.columns:\n",
    "        edf.drop(columns=['class'], inplace=True) # Drop the target variable if it exists\n",
    "\n",
    "    # Identify time and value columns\n",
    "    time_col = 'date'\n",
    "    value_col = 'nswprice'\n",
    "    if time_col in edf.columns:\n",
    "        try:\n",
    "            edf[time_col] = pd.to_datetime(edf[time_col], errors='coerce')\n",
    "            edf = edf.dropna(subset=[time_col])  # Drop rows with invalid dates\n",
    "            edf.sort_values(by=time_col, inplace=True)\n",
    "            edf.reset_index(drop=True, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert '{time_col}' to datetime: {e}\")\n",
    "            # Use index as time\n",
    "            edf['time_index'] = np.arange(len(edf))\n",
    "            time_col = 'time_index'\n",
    "\n",
    "    # Plot the time series\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    if time_col == 'date':\n",
    "        plt.plot(edf.index, edf[value_col], label=value_col, linewidth=0.5)\n",
    "    else:\n",
    "        plt.plot(edf[time_col], edf[value_col], label=value_col, linewidth=0.5)\n",
    "    plt.title(f'Time Series of {value_col}')\n",
    "    plt.xlabel('Time Index' if time_col != 'date' else 'Date')\n",
    "    plt.ylabel(value_col)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Data shape: {edf.shape}')\n",
    "    print(f'{value_col} statistics: mean={edf[value_col].mean():.2f}, std={edf[value_col].std():.2f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41bec7bd220108ba",
   "metadata": {
    "id": "41bec7bd220108ba"
   },
   "source": [
    "### 4.2 Drift Detection using Wasserstein Distance\n",
    "\n",
    "We will implement a sliding-window approach to detect drift in the distribution of the `nswprice` feature. The Wasserstein distance (Earth Mover's Distance) is a robust metric for comparing probability distributions.\n",
    "\n",
    "We compare the distribution of a **reference window** (the first 1000 samples) against a **test window** (a sliding window of 500 samples)."
   ]
  },
  {
   "cell_type": "code",
   "id": "ee9cde07f8841f2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "ee9cde07f8841f2d",
    "outputId": "afa6b1bf-6e49-4cee-9909-c72f4dcb933f"
   },
   "source": [
    "if electric_df is None:\n",
    "    print('Electricity dataset not loaded — skipping drift detection.')\n",
    "else:\n",
    "    # Parameters for drift detection\n",
    "    REF_WINDOW_SIZE = 1000\n",
    "    TEST_WINDOW_SIZE = 500\n",
    "    STEP_SIZE = 100\n",
    "\n",
    "    # Extract the feature values\n",
    "    data = edf[value_col].values.astype(float)\n",
    "\n",
    "    # Define the reference distribution (first 1000 samples)\n",
    "    ref_data = data[:REF_WINDOW_SIZE]\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    drift_scores = []\n",
    "    window_indices = []\n",
    "\n",
    "    # Sliding window analysis\n",
    "    print('Starting sliding window drift analysis...')\n",
    "    for i in tqdm(range(REF_WINDOW_SIZE, len(data) - TEST_WINDOW_SIZE, STEP_SIZE)):\n",
    "        test_data = data[i:i + TEST_WINDOW_SIZE]\n",
    "\n",
    "        # Calculate Wasserstein distance\n",
    "        # Note: The data must be 1D arrays for scipy.stats.wasserstein_distance\n",
    "        distance = wasserstein_distance(ref_data, test_data)\n",
    "\n",
    "        drift_scores.append(distance)\n",
    "        window_indices.append(i + TEST_WINDOW_SIZE // 2) # Center of the test window\n",
    "\n",
    "    # Convert to numpy arrays for easier analysis\n",
    "    drift_scores = np.array(drift_scores)\n",
    "    window_indices = np.array(window_indices)\n",
    "\n",
    "    # Define a simple threshold for drift alert\n",
    "    mean_score = np.mean(drift_scores)\n",
    "    std_score = np.std(drift_scores)\n",
    "    drift_threshold = mean_score + 2 * std_score\n",
    "\n",
    "    # Plotting the drift scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(window_indices, drift_scores, label='Wasserstein Distance Score')\n",
    "    plt.axhline(drift_threshold, color='r', linestyle='--', label=f'Drift Threshold (Mean + 2*Std)')\n",
    "\n",
    "    # Mark drift points\n",
    "    drift_points = window_indices[drift_scores > drift_threshold]\n",
    "    plt.scatter(drift_points, drift_scores[drift_scores > drift_threshold], color='red', marker='o', label='Drift Detected')\n",
    "\n",
    "    plt.title(f'Distribution Drift Detection on {value_col} using Wasserstein Distance')\n",
    "    plt.xlabel('Time Index (Center of Test Window)')\n",
    "    plt.ylabel('Wasserstein Distance to Reference')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'\\nDrift detected at {len(drift_points)} points.')\n",
    "    if len(drift_points) > 0:\n",
    "        print('First drift point index:', drift_points[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "782e5165-d000-4dd9-8364-f4ea136dd11e",
   "metadata": {
    "id": "782e5165-d000-4dd9-8364-f4ea136dd11e"
   },
   "source": [
    "### 4.3 Performance-Based Drift Detection with DDM and Incremental Learning\n",
    "\n",
    "In this section, we implement a **performance-based drift detection** strategy using the Drift Detection Method (DDM) in an incremental learning approach. We train an initial model on a reference window and continuously monitor error rates on incoming batches. DDM tracks the error rate and its standard deviation, detecting warnings and drifts based on statistical thresholds, triggering retraining when confirmed drift occurs.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Prequential Evaluation**: Test-then-train paradigm where each sample is first used for testing, then for training.\n",
    "- **DDM Monitoring**: Tracks minimum error rate (p_min) and std (s_min); detects warning at p_t + s_t >= p_min + 2*s_min, drift at >= 3*s_min.\n",
    "- **Adaptive Retraining**: Retrain model on recent window upon drift detection and reset DDM statistics."
   ]
  },
  {
   "cell_type": "code",
   "id": "b21dff03-c96d-48ba-8b1f-65eceebcccbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "b21dff03-c96d-48ba-8b1f-65eceebcccbd",
    "outputId": "7fe3a8e4-5fc4-40b0-b654-9adfc273ab40"
   },
   "source": [
    "if electric_df is None:\n",
    "    print('Electricity dataset not loaded — skipping performance-based drift detection.')\n",
    "else:\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import math  # For sqrt\n",
    "\n",
    "    # Prepare data: use 'class' as target (UP/DOWN electricity price movement)\n",
    "    # If 'class' column exists in original dataset\n",
    "    edf_full = electric_df.copy()\n",
    "\n",
    "    # Map string labels to integers for consistency\n",
    "    if 'class' in edf_full.columns and edf_full['class'].dtype == object:\n",
    "        edf_full['class'] = edf_full['class'].map({'DOWN': 0, 'UP': 1}).astype(int)\n",
    "\n",
    "    # Create feature matrix and target\n",
    "    feature_cols = ['nswdemand', 'vicprice', 'vicdemand', 'transfer']\n",
    "    available_features = [col for col in feature_cols if col in edf_full.columns]\n",
    "\n",
    "    if len(available_features) < 2 or 'class' not in edf_full.columns:\n",
    "        print(\"Warning: Not enough features or no 'class' column. Creating synthetic target.\")\n",
    "        # Create synthetic binary target based on price change\n",
    "        edf_full = edf_full.sort_values('date').reset_index(drop=True)\n",
    "        edf_full['price_change'] = edf_full['nswprice'].diff()\n",
    "        edf_full['class'] = (edf_full['price_change'] > 0).astype(int)\n",
    "        edf_full = edf_full.dropna()\n",
    "\n",
    "    X_stream = edf_full[available_features].fillna(0).values\n",
    "    y_stream = edf_full['class'].values\n",
    "\n",
    "    # Parameters\n",
    "    INITIAL_TRAIN_SIZE = 500\n",
    "    BATCH_SIZE = 100\n",
    "    MIN_INSTANCES = 100  # Min samples before DDM stats stabilize\n",
    "\n",
    "    # Initialize model and scaler\n",
    "    scaler_online = StandardScaler()\n",
    "    X_train_init = scaler_online.fit_transform(X_stream[:INITIAL_TRAIN_SIZE])\n",
    "    y_train_init = y_stream[:INITIAL_TRAIN_SIZE]\n",
    "    model = SGDClassifier(loss='log_loss', random_state=42, max_iter=1)\n",
    "    model.fit(X_train_init, y_train_init)\n",
    "\n",
    "    # DDM variables\n",
    "    p_min = float('inf')  # Min error rate\n",
    "    s_min = float('inf')  # Min std\n",
    "    p_current = 0  # Current error rate\n",
    "    s_current = 0  # Current std\n",
    "    t = 0  # Instance count for DDM\n",
    "\n",
    "    # Track performance\n",
    "    accuracies = []\n",
    "    batch_indices = []\n",
    "    drift_detected_at = []\n",
    "    warning_at = []  # For warnings\n",
    "\n",
    "    print('Starting prequential evaluation with DDM drift detection...')\n",
    "\n",
    "    for i in tqdm(range(INITIAL_TRAIN_SIZE, len(X_stream) - BATCH_SIZE, BATCH_SIZE)):\n",
    "        X_batch = X_stream[i:i+BATCH_SIZE]\n",
    "        y_batch = y_stream[i:i+BATCH_SIZE]\n",
    "        X_batch_scaled = scaler_online.transform(X_batch)\n",
    "\n",
    "        # Predict and get error\n",
    "        y_pred = model.predict(X_batch_scaled)\n",
    "        batch_acc = accuracy_score(y_batch, y_pred)\n",
    "        batch_error = 1 - batch_acc\n",
    "        accuracies.append(batch_acc)\n",
    "        batch_indices.append(i + BATCH_SIZE // 2)\n",
    "\n",
    "        # Update DDM per instance (simulate stream)\n",
    "        for error in (y_pred != y_batch):  # Per-sample error (1 or 0)\n",
    "            t += 1\n",
    "            p_current = p_current + (error - p_current) / t\n",
    "            s_current = math.sqrt(p_current * (1 - p_current) / t)\n",
    "\n",
    "            if t > MIN_INSTANCES:\n",
    "                if p_current + s_current < p_min + s_min:\n",
    "                    p_min = p_current\n",
    "                    s_min = s_current\n",
    "                if p_current + s_current >= p_min + 2 * s_min:\n",
    "                    warning_at.append(i + BATCH_SIZE // 2)\n",
    "                if p_current + s_current >= p_min + 3 * s_min:\n",
    "                    drift_detected_at.append(i + BATCH_SIZE // 2)\n",
    "\n",
    "                    # Retrain on recent window\n",
    "                    retrain_window = 1000\n",
    "                    retrain_start = max(0, i - retrain_window)\n",
    "                    X_retrain = scaler_online.fit_transform(X_stream[retrain_start:i+BATCH_SIZE])\n",
    "                    y_retrain = y_stream[retrain_start:i+BATCH_SIZE]\n",
    "                    model = SGDClassifier(loss='log_loss', random_state=42, max_iter=10)\n",
    "                    model.fit(X_retrain, y_retrain)\n",
    "\n",
    "                    # Reset DDM\n",
    "                    p_min = float('inf')\n",
    "                    s_min = float('inf')\n",
    "                    p_current = 0\n",
    "                    s_current = 0\n",
    "                    t = 0\n",
    "\n",
    "        # Incremental train\n",
    "        model.partial_fit(X_batch_scaled, y_batch, classes=np.unique(y_stream))\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(batch_indices, accuracies, label='Batch Accuracy', alpha=0.6, linewidth=0.8)\n",
    "\n",
    "    # Smooth line\n",
    "    window_size = 10\n",
    "    if len(accuracies) >= window_size:\n",
    "        smoothed = np.convolve(accuracies, np.ones(window_size)/window_size, mode='valid')\n",
    "        smooth_indices = batch_indices[window_size-1:]\n",
    "        plt.plot(smooth_indices, smoothed, label='Smoothed Accuracy', linewidth=2, color='blue')\n",
    "\n",
    "    if len(warning_at) > 0:\n",
    "        warning_accs = [accuracies[batch_indices.index(w)] for w in warning_at if w in batch_indices]\n",
    "        plt.scatter(warning_at, warning_accs, color='orange', s=80, marker='o', label=f'Warnings ({len(warning_at)})', zorder=4)\n",
    "    if len(drift_detected_at) > 0:\n",
    "        drift_accs = [accuracies[batch_indices.index(d)] for d in drift_detected_at if d in batch_indices]\n",
    "        plt.scatter(drift_detected_at, drift_accs, color='red', s=100, marker='X', label=f'Drifts Detected ({len(drift_detected_at)})', zorder=5)\n",
    "\n",
    "    plt.title('DDM-Based Drift Detection with Online Learning')\n",
    "    plt.xlabel('Time Index (Batch Center)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'\\nDrifts detected at {len(drift_detected_at)} points.')\n",
    "    print(f'Mean accuracy: {np.mean(accuracies):.4f}')\n",
    "    print(f'Std accuracy: {np.std(accuracies):.4f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78d2d368c311d65e",
   "metadata": {
    "id": "78d2d368c311d65e"
   },
   "source": [
    "## 5. Model Robustness: Adversarial Simulation (Label-Flip Poisoning)\n",
    "\n",
    "Simulate flipping labels on a small fraction of the training data and measure degradation on a clean test set for a supervised classifier (LogisticRegression). This section uses the Credit Card data."
   ]
  },
  {
   "cell_type": "code",
   "id": "4b35cdc1a216f7ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b35cdc1a216f7ee",
    "outputId": "c504535f-8e3f-4fc5-d1d2-58014c63202b"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "if credit_df is None:\n",
    "    print('Credit dataset not loaded — skipping poisoning simulation.')\n",
    "else:\n",
    "    # We need to re-run the preprocessing section to get X_train, y_train, X_test, y_test\n",
    "    # (Assuming the preprocessing cell 3.2 has been run)\n",
    "\n",
    "    # Supervised baseline\n",
    "    clf = LogisticRegression(max_iter=500, solver='liblinear', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    base_f1 = f1_score(y_test, y_pred)\n",
    "    print('Baseline supervised F1:', base_f1)\n",
    "\n",
    "    # Poisoning: flip labels of small random subset of training data\n",
    "    rng = np.random.RandomState(42)\n",
    "    poison_frac = 0.05  # 5%\n",
    "    n_poison = int(poison_frac * len(y_train))\n",
    "    idx = rng.choice(np.arange(len(y_train)), size=n_poison, replace=False)\n",
    "    y_train_poison = y_train.copy()\n",
    "    y_train_poison[idx] = 1 - y_train_poison[idx]  # flip\n",
    "\n",
    "    clf2 = LogisticRegression(max_iter=500, solver='liblinear', random_state=42)\n",
    "    clf2.fit(X_train, y_train_poison)\n",
    "    y_pred2 = clf2.predict(X_test)\n",
    "    poisoned_f1 = f1_score(y_test, y_pred2)\n",
    "    print(f'After flipping {poison_frac*100:.1f}% of training labels, F1 ->', poisoned_f1)\n",
    "    print('Relative drop in F1:', (base_f1 - poisoned_f1) / (base_f1 + 1e-9))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Student Tasks\n",
    "These tasks are designed for deeper exploration of the concepts covered in this lab, focusing on hyperparameter tuning, model comparison, and practical application of anomaly detection and drift adaptation.\n"
   ],
   "metadata": {
    "id": "SmQtgkW-IvBm"
   },
   "id": "SmQtgkW-IvBm"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 1: Hyperparameter Tuning\n",
    "Optimize the hyperparameters of IsolationForest and GaussianMixture for anomaly detection on the Credit Card Fraud dataset. Report improved metrics and discuss trade-offs (e.g., computation time vs. accuracy)."
   ],
   "metadata": {
    "id": "07C1BXjtJHjD"
   },
   "id": "07C1BXjtJHjD"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2: Drift Mitigation Comparison\n",
    "\n",
    "Implement and compare two mitigation methods for handling drifts in the Elec2 dataset:  \n",
    "(1) **Passive approach**: Periodic retraining every N batches (e.g., N=10).  \n",
    "(2) **Active approach**: Triggered retraining on detected drifts using ADWIN (extend the DDM section by replacing it with an ADWIN implementation); reference: https://riverml.xyz/dev/api/drift/ADWIN/.\n",
    "\n",
    "Track overall accuracy and F1-score over the stream, plot the results for both approaches."
   ],
   "metadata": {
    "id": "Kd3VtskqJRn3"
   },
   "id": "Kd3VtskqJRn3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 3: Implement Poisoning Defense\n",
    "Extend the label-flip poisoning simulation by implementing a defense: Use IsolationForest on X_train to detect and remove potential poisoned samples from the poisoned training data (y_train_poison). Retrain the model on the cleaned data and compare performance to the undefended poisoned case using multiple metrics (F1, MCC, Precision, Recall, and PR-AUC). Plot relative drops in these metrics and discuss which metric best captures the degradation and recovery."
   ],
   "metadata": {
    "id": "gtL0nvHkJRwO"
   },
   "id": "gtL0nvHkJRwO"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
